Natural language  0.6923076923076923
language  1.0
language processing  0.22297297297297297
processing  1.0
processing -LRB-  0.07407407407407407
-LRB-  0.948509485094851
-LRB- NLP  0.008130081300813009
NLP  0.9787234042553191
NLP -RRB-  0.0851063829787234
-RRB-  1.0394366197183098
-RRB- is  0.030985915492957747
is  1.0
is a  0.10975609756097561
a  1.0
a field  0.0036809815950920245
field  1.0
field of  0.4444444444444444
of  1.0
of computer  0.0035650623885918
computer  1.0
computer science  0.09090909090909091
science  1.0
science ,  0.4
,  0.9994385176866929
, artificial  0.0011229646266142617
artificial  1.0
artificial intelligence  0.6363636363636364
intelligence  1.0
intelligence -LRB-  0.125
-LRB- also  0.01084010840108401
also  1.0
also called  0.043478260869565216
called  1.0
called machine  0.05555555555555555
machine  1.0
machine learning  0.24050632911392406
learning  1.0
learning -RRB-  0.023255813953488372
-RRB- ,  0.21971830985915494
, and  0.10612015721504772
and  0.9985549132947977
and linguistics  0.002890173410404624
linguistics  1.0
linguistics concerned  0.05
concerned  1.0
concerned with  0.8
with  1.0
with the  0.15300546448087432
the  0.9993079584775086
the interactions  0.0006920415224913495
interactions  1.0
interactions between  1.0
between  1.0
between computers  0.02564102564102564
computers  1.0
computers and  0.1111111111111111
and human  0.001445086705202312
human  0.9782608695652174
human -LRB-  0.043478260869565216
-LRB- natural  0.0027100271002710027
natural  1.0
natural -RRB-  0.013333333333333334
-RRB- languages  0.0028169014084507044
languages  1.0
languages .  0.16
.  80.125
Specifically ,  1.0
, it  0.01347557551937114
it  1.0
it is  0.20512820512820512
is the  0.09146341463414634
the process  0.007612456747404845
process  1.0
process of  0.3333333333333333
of a  0.08199643493761141
a computer  0.018404907975460124
computer extracting  0.022727272727272728
extracting  1.0
extracting meaningful  0.2
meaningful  1.0
meaningful information  0.125
information  1.0
information from  0.06521739130434782
from  1.0
from natural  0.009615384615384616
natural language  0.76
language input  0.02027027027027027
input  1.0
input and\/or  0.024390243902439025
and\/or  1.0
and\/or producing  0.3333333333333333
producing  1.0
producing natural  0.3333333333333333
language output  0.006756756756756757
output  1.0
output .  0.15384615384615385
In theory  0.01904761904761905
theory  1.0
theory ,  0.3076923076923077
, natural  0.0005614823133071309
processing is  0.037037037037037035
a very  0.013496932515337423
very  1.0
very attractive  0.024390243902439025
attractive  1.0
attractive method  0.3333333333333333
method  1.0
method of  0.125
of human  0.004456327985739751
human --  0.021739130434782608
--  1.0
-- computer  0.04
computer interaction  0.022727272727272728
interaction  1.0
interaction .  0.25
language understanding  0.0945945945945946
understanding  1.0
understanding is  0.06060606060606061
is sometimes  0.006097560975609756
sometimes  1.0
sometimes referred  0.23076923076923078
referred  1.0
referred to  1.0
to  1.0
to as  0.005312084993359893
as  1.0
as an  0.04529616724738676
an  1.0
an AI-complete  0.007575757575757576
AI-complete  1.0
AI-complete problem  0.3333333333333333
problem  1.0
problem because  0.022727272727272728
because  0.9666666666666667
because it  0.1
it seems  0.008547008547008548
seems  1.0
seems to  1.0
to require  0.0013280212483399733
require  0.9545454545454546
require extensive  0.09090909090909091
extensive  1.0
extensive knowledge  0.3333333333333333
knowledge  1.0
knowledge about  0.1111111111111111
about  1.0
about the  0.2
the outside  0.0006920415224913495
outside  1.0
outside world  0.5
world  1.0
world and  0.06666666666666667
and the  0.059248554913294796
the ability  0.001384083044982699
ability  1.0
ability to  0.75
to manipulate  0.0026560424966799467
manipulate  1.0
manipulate it  0.3333333333333333
it .  0.042735042735042736
Whether NLP  0.5
NLP is  0.02127659574468085
is distinct  0.0020325203252032522
distinct  1.0
distinct from  0.42857142857142855
from ,  0.009615384615384616
, or  0.018528916339135316
or  1.0
or identical  0.0045045045045045045
identical  1.0
identical to  1.0
to ,  0.0026560424966799467
, the  0.058394160583941604
the field  0.011764705882352941
of computational  0.00267379679144385
computational  1.0
computational linguistics  0.6
linguistics is  0.15
a matter  0.001226993865030675
matter  1.0
matter of  0.3333333333333333
of perspective  0.00089126559714795
perspective  1.0
perspective .  0.25
The Association  0.005208333333333333
Association  1.0
Association for  1.0
for  1.0
for Computational  0.0036101083032490976
Computational  1.0
Computational Linguistics  1.0
Linguistics  1.0
Linguistics defines  0.3333333333333333
defines  1.0
defines the  0.5
the latter  0.0006920415224913495
latter  1.0
latter as  1.0
as focusing  0.003484320557491289
focusing  1.0
focusing on  1.0
on  1.0
on the  0.30660377358490565
the theoretical  0.0006920415224913495
theoretical  1.0
theoretical aspects  0.3333333333333333
aspects  1.0
aspects of  0.8571428571428571
of NLP  0.004456327985739751
NLP .  0.10638297872340426
On the  0.3333333333333333
the other  0.005536332179930796
other  1.0
other hand  0.07142857142857142
hand  1.0
hand ,  0.5
the open-access  0.0006920415224913495
open-access  1.0
open-access journal  1.0
journal  1.0
journal ``  0.3333333333333333
``  0.9735449735449735
`` Computational  0.005291005291005291
Linguistics ''  0.3333333333333333
''  1.043010752688172
'' ,  0.16129032258064516
, styles  0.0005614823133071309
styles  1.0
styles itself  1.0
itself  1.0
itself as  0.2
as ``  0.04878048780487805
`` the  0.026455026455026454
the longest  0.0006920415224913495
longest  1.0
longest running  1.0
running  1.0
running publication  0.3333333333333333
publication  1.0
publication devoted  0.3333333333333333
devoted  1.0
devoted exclusively  0.2
exclusively  1.0
exclusively to  1.0
to the  0.10225763612217796
the design  0.001384083044982699
design  1.0
design and  0.25
and analysis  0.001445086705202312
analysis  1.0
analysis of  0.18461538461538463
of natural  0.0196078431372549
processing systems  0.05555555555555555
systems  1.0
systems ''  0.008928571428571428
'' -LRB-  0.04838709677419355
-LRB- Computational  0.0027100271002710027
Linguistics -LRB-  0.3333333333333333
-LRB- Journal  0.0027100271002710027
Journal  1.0
Journal -RRB-  0.3333333333333333
-RRB- -RRB-  0.005633802816901409
-RRB- Modern  0.0028169014084507044
Modern  0.6666666666666666
Modern NLP  0.3333333333333333
NLP algorithms  0.0425531914893617
algorithms  1.0
algorithms are  0.05714285714285714
are  1.0
are grounded  0.008298755186721992
grounded  1.0
grounded in  1.0
in  1.0
in machine  0.009363295880149813
learning ,  0.09302325581395349
, especially  0.0050533408197641775
especially  1.0
especially statistical  0.06666666666666667
statistical  1.0
statistical machine  0.09090909090909091
learning .  0.09302325581395349
Research into  0.125
into  1.0
into modern  0.01282051282051282
modern  1.0
modern statistical  0.2
statistical NLP  0.06060606060606061
algorithms requires  0.02857142857142857
requires  1.0
requires an  0.0625
an understanding  0.015151515151515152
understanding of  0.15151515151515152
a number  0.024539877300613498
number  1.0
number of  0.8372093023255814
of disparate  0.00089126559714795
disparate  1.0
disparate fields  1.0
fields  1.0
fields ,  0.3333333333333333
, including  0.004491858506457047
including  1.0
including linguistics  0.14285714285714285
linguistics ,  0.4
, computer  0.0011229646266142617
and statistics  0.002890173410404624
statistics  1.0
statistics .  0.375
For a  0.03278688524590164
a discussion  0.001226993865030675
discussion  1.0
discussion of  0.5
of the  0.17379679144385027
the types  0.001384083044982699
types  1.0
types of  0.8571428571428571
of algorithms  0.00089126559714795
algorithms currently  0.02857142857142857
currently  1.0
currently used  0.14285714285714285
used  1.0
used in  0.20353982300884957
in NLP  0.0149812734082397
NLP ,  0.02127659574468085
, see  0.0011229646266142617
see  1.0
see the  0.1
the article  0.0006920415224913495
article  1.0
article on  0.034482758620689655
on pattern  0.0047169811320754715
pattern  1.0
pattern recognition  0.6666666666666666
recognition  1.0
recognition .  0.05785123966942149
An automated  0.0625
automated  1.0
automated online  0.14285714285714285
online  1.0
online assistant  0.125
assistant  1.0
assistant providing  1.0
providing  1.0
providing customer  0.5
customer  1.0
customer service  1.0
service  1.0
service on  0.2
on a  0.10849056603773585
a web  0.001226993865030675
web  1.0
web page  0.125
page  1.0
page ,  0.42857142857142855
, an  0.005614823133071308
an example  0.03787878787878788
example  1.0
example of  0.08641975308641975
of an  0.011586452762923352
an application  0.015151515151515152
application  1.0
application where  0.07142857142857142
where  1.0
where natural  0.02857142857142857
a major  0.006134969325153374
major  1.0
major component  0.08333333333333333
component  1.0
component .  0.2
In 1950  0.01904761904761905
1950  1.0
1950 ,  1.0
, Alan  0.0005614823133071309
Alan  1.0
Alan Turing  1.0
Turing  1.0
Turing published  0.5
published  1.0
published his  0.14285714285714285
his  1.0
his famous  0.08333333333333333
famous  1.0
famous article  0.3333333333333333
article ``  0.034482758620689655
`` Computing  0.005291005291005291
Computing  1.0
Computing Machinery  0.5
Machinery  1.0
Machinery and  1.0
and Intelligence  0.001445086705202312
Intelligence  1.0
Intelligence ''  0.3333333333333333
'' which  0.005376344086021506
which  1.0
which proposed  0.007246376811594203
proposed  1.0
proposed what  0.1111111111111111
what  1.0
what is  0.09375
is now  0.006097560975609756
now  1.0
now called  0.07692307692307693
called the  0.1111111111111111
the Turing  0.0006920415224913495
Turing test  0.5
test  1.0
test as  0.1
as a  0.11846689895470383
a criterion  0.001226993865030675
criterion  1.0
criterion of  0.5
of intelligence  0.00089126559714795
intelligence .  0.125
This criterion  0.015873015873015872
criterion depends  0.5
depends  1.0
depends on  0.875
ability of  0.25
computer program  0.11363636363636363
program  1.0
program to  0.09090909090909091
to impersonate  0.0013280212483399733
impersonate  1.0
impersonate a  1.0
a human  0.013496932515337423
human in  0.021739130434782608
in a  0.09363295880149813
a real-time  0.001226993865030675
real-time  1.0
real-time written  0.5
written  1.0
written conversation  0.038461538461538464
conversation  1.0
conversation with  0.5
with a  0.1092896174863388
human judge  0.021739130434782608
judge  1.0
judge ,  0.25
, sufficiently  0.0005614823133071309
sufficiently  1.0
sufficiently well  1.0
well  1.0
well that  0.03571428571428571
that  1.0
that the  0.08156028368794327
the judge  0.0006920415224913495
judge is  0.25
is unable  0.0020325203252032522
unable  1.0
unable to  1.0
to distinguish  0.006640106241699867
distinguish  1.0
distinguish reliably  0.2
reliably  1.0
reliably --  1.0
-- on  0.04
the basis  0.002768166089965398
basis  1.0
basis of  0.6666666666666666
the conversational  0.0006920415224913495
conversational  1.0
conversational content  1.0
content  1.0
content alone  0.08333333333333333
alone  1.0
alone --  0.25
-- between  0.04
between the  0.1794871794871795
the program  0.0020761245674740486
program and  0.045454545454545456
and a  0.023121387283236993
a real  0.00245398773006135
real  1.0
real human  0.1111111111111111
human .  0.06521739130434782
The Georgetown  0.015625
Georgetown  1.0
Georgetown experiment  1.0
experiment  1.0
experiment in  0.2
in 1954  0.003745318352059925
1954  1.0
1954 involved  0.3333333333333333
involved  1.0
involved fully  0.3333333333333333
fully  1.0
fully automatic  0.5
automatic  1.0
automatic translation  0.08695652173913043
translation  1.0
translation of  0.14864864864864866
of more  0.0035650623885918
more  1.0
more than  0.042105263157894736
than  1.0
than sixty  0.022222222222222223
sixty  1.0
sixty Russian  1.0
Russian  1.0
Russian sentences  1.0
sentences  1.0
sentences into  0.039473684210526314
into English  0.02564102564102564
English  0.972972972972973
English .  0.13513513513513514
The authors  0.015625
authors  1.0
authors claimed  0.4
claimed  1.0
claimed that  1.0
that within  0.0070921985815602835
within  1.0
within three  0.1111111111111111
three  1.0
three or  0.3333333333333333
or five  0.0045045045045045045
five  1.0
five years  0.4
years  1.0
years ,  0.23809523809523808
, machine  0.0016844469399213925
machine translation  0.4936708860759494
translation would  0.02702702702702703
would  1.0
would be  0.16981132075471697
be  1.0
be a  0.05485232067510549
a solved  0.00245398773006135
solved  1.0
solved problem  0.4
problem .  0.22727272727272727
However ,  0.8648648648648649
, real  0.0005614823133071309
real progress  0.1111111111111111
progress  1.0
progress was  0.2857142857142857
was  1.0
was much  0.025974025974025976
much  1.0
much slower  0.09090909090909091
slower  1.0
slower ,  1.0
and after  0.004335260115606936
after  1.0
after the  0.16666666666666666
the ALPAC  0.001384083044982699
ALPAC  1.0
ALPAC report  1.0
report  1.0
report in  0.25
in 1966  0.0018726591760299626
1966  1.0
1966 ,  0.3333333333333333
, which  0.031443009545199324
which found  0.014492753623188406
found  1.0
found that  0.35714285714285715
that ten  0.0035460992907801418
ten  1.0
ten years  1.0
years long  0.047619047619047616
long  1.0
long research  0.5
research  1.0
research had  0.047619047619047616
had  1.0
had failed  0.14285714285714285
failed  1.0
failed to  1.0
to fulfill  0.0026560424966799467
fulfill  1.0
fulfill the  0.5
the expectations  0.0006920415224913495
expectations  1.0
expectations ,  1.0
, funding  0.0016844469399213925
funding  1.0
funding for  0.25
for machine  0.010830324909747292
translation was  0.02702702702702703
was dramatically  0.012987012987012988
dramatically  1.0
dramatically reduced  1.0
reduced  1.0
reduced .  0.5
Little further  1.0
further  1.0
further research  0.125
research in  0.14285714285714285
was conducted  0.025974025974025976
conducted  1.0
conducted until  0.2
until  1.0
until the  0.5
the late  0.005536332179930796
late  1.0
late 1980s  0.4444444444444444
1980s  1.0
1980s ,  0.5555555555555556
, when  0.003368893879842785
when  1.0
when the  0.11428571428571428
the first  0.010380622837370242
first  1.0
first statistical  0.06060606060606061
translation systems  0.02702702702702703
systems were  0.05357142857142857
were  1.0
were developed  0.12195121951219512
developed  1.0
developed .  0.07692307692307693
Some notably  0.047619047619047616
notably  1.0
notably successful  0.3333333333333333
successful  1.0
successful NLP  0.1111111111111111
NLP systems  0.06382978723404255
systems developed  0.017857142857142856
developed in  0.23076923076923078
in the  0.2602996254681648
the 1960s  0.001384083044982699
1960s  1.0
1960s were  0.3333333333333333
were SHRDLU  0.024390243902439025
SHRDLU  0.6666666666666666
SHRDLU ,  0.16666666666666666
, a  0.02695115103874228
a natural  0.006134969325153374
language system  0.006756756756756757
system  1.0
system working  0.010752688172043012
working  1.0
working in  0.2857142857142857
in restricted  0.0018726591760299626
restricted  1.0
restricted ``  0.25
`` blocks  0.010582010582010581
blocks  1.0
blocks worlds  0.25
worlds  1.0
worlds ''  1.0
'' with  0.021505376344086023
with restricted  0.00546448087431694
restricted vocabularies  0.25
vocabularies  1.0
vocabularies ,  1.0
and ELIZA  0.002890173410404624
ELIZA  0.6666666666666666
ELIZA ,  0.3333333333333333
a simulation  0.001226993865030675
simulation  1.0
simulation of  0.3333333333333333
a Rogerian  0.001226993865030675
Rogerian  1.0
Rogerian psychotherapist  1.0
psychotherapist  1.0
psychotherapist ,  1.0
, written  0.0005614823133071309
written by  0.23076923076923078
by  1.0
by Joseph  0.005714285714285714
Joseph  1.0
Joseph Weizenbaum  1.0
Weizenbaum  1.0
Weizenbaum between  0.3333333333333333
between 1964  0.02564102564102564
1964  1.0
1964 to  1.0
to 1966  0.0013280212483399733
1966 .  0.3333333333333333
Using almost  0.5
almost  1.0
almost no  1.0
no  1.0
no information  0.07692307692307693
information about  0.043478260869565216
about human  0.025
human thought  0.021739130434782608
thought  1.0
thought or  0.3333333333333333
or emotion  0.0045045045045045045
emotion  1.0
emotion ,  1.0
, ELIZA  0.0011229646266142617
ELIZA sometimes  0.1111111111111111
sometimes provided  0.07692307692307693
provided  1.0
provided a  0.2
a startlingly  0.001226993865030675
startlingly  1.0
startlingly human-like  1.0
human-like  1.0
human-like interaction  1.0
When the  0.14285714285714285
the ``  0.0034602076124567475
`` patient  0.005291005291005291
patient  1.0
patient ''  1.0
'' exceeded  0.005376344086021506
exceeded  1.0
exceeded the  1.0
the very  0.0006920415224913495
very small  0.04878048780487805
small  1.0
small knowledge  0.1111111111111111
knowledge base  0.14814814814814814
base  1.0
base ,  0.5
ELIZA might  0.1111111111111111
might  0.9615384615384616
might provide  0.038461538461538464
provide  1.0
provide a  0.3333333333333333
a generic  0.001226993865030675
generic  1.0
generic response  0.3333333333333333
response  1.0
response ,  0.5
, for  0.012352610892756879
for example  0.06498194945848375
example ,  0.6666666666666666
, responding  0.0005614823133071309
responding  1.0
responding to  1.0
to ``  0.005312084993359893
`` My  0.005291005291005291
My  1.0
My head  1.0
head  1.0
head hurts  1.0
hurts  1.0
hurts ''  0.5
with ``  0.01639344262295082
`` Why  0.015873015873015872
Why  0.7142857142857143
Why do  0.14285714285714285
do  1.0
do you  0.038461538461538464
you  1.0
you say  0.07692307692307693
say  1.0
say your  0.14285714285714285
your  1.0
your head  0.5
hurts ?  0.5
?  2.0
? ''  0.75
'' .  0.06989247311827956
During the  0.5
the 70  0.0006920415224913495
70  1.0
70 's  0.25
's  1.0
's many  0.0196078431372549
many  1.0
many programmers  0.019230769230769232
programmers  1.0
programmers began  1.0
began  1.0
began to  0.5714285714285714
to write  0.0013280212483399733
write  1.0
write `  1.0
`  1.0
` conceptual  0.0625
conceptual  1.0
conceptual ontologies  0.5
ontologies  1.0
ontologies '  0.16666666666666666
'  1.0
' ,  0.3157894736842105
which structured  0.007246376811594203
structured  1.0
structured real-world  0.16666666666666666
real-world  1.0
real-world information  0.16666666666666666
information into  0.043478260869565216
into computer-understandable  0.01282051282051282
computer-understandable  1.0
computer-understandable data  1.0
data  1.0
data .  0.22077922077922077
Examples are  0.6666666666666666
are MARGIE  0.004149377593360996
MARGIE  1.0
MARGIE -LRB-  1.0
-LRB- Schank  0.0027100271002710027
Schank  1.0
Schank ,  0.2
, 1975  0.0005614823133071309
1975  1.0
1975 -RRB-  1.0
, SAM  0.0005614823133071309
SAM  1.0
SAM -LRB-  1.0
-LRB- Cullingford  0.0027100271002710027
Cullingford  1.0
Cullingford ,  1.0
, 1978  0.0011229646266142617
1978  1.0
1978 -RRB-  0.6666666666666666
, PAM  0.0005614823133071309
PAM  1.0
PAM -LRB-  1.0
-LRB- Wilensky  0.0027100271002710027
Wilensky  1.0
Wilensky ,  1.0
, TaleSpin  0.0005614823133071309
TaleSpin  1.0
TaleSpin -LRB-  1.0
-LRB- Meehan  0.0027100271002710027
Meehan  1.0
Meehan ,  1.0
, 1976  0.0011229646266142617
1976  1.0
1976 -RRB-  0.5
, QUALM  0.0005614823133071309
QUALM  1.0
QUALM -LRB-  1.0
-LRB- Lehnert  0.005420054200542005
Lehnert  1.0
Lehnert ,  0.6666666666666666
, 1977  0.0005614823133071309
1977  1.0
1977 -RRB-  1.0
, Politics  0.0005614823133071309
Politics  1.0
Politics -LRB-  1.0
-LRB- Carbonell  0.0027100271002710027
Carbonell  1.0
Carbonell ,  1.0
, 1979  0.0005614823133071309
1979  1.0
1979 -RRB-  1.0
and Plot  0.001445086705202312
Plot  1.0
Plot Units  1.0
Units  1.0
Units -LRB-  1.0
Lehnert 1981  0.3333333333333333
1981  1.0
1981 -RRB-  1.0
-RRB- .  0.28450704225352114
During this  0.5
this  1.0
this time  0.03296703296703297
time  1.0
time ,  0.3333333333333333
, many  0.0039303761931499155
many chatterbots  0.019230769230769232
chatterbots  1.0
chatterbots were  0.5
were written  0.024390243902439025
written including  0.038461538461538464
including PARRY  0.07142857142857142
PARRY  1.0
PARRY ,  1.0
, Racter  0.0005614823133071309
Racter  1.0
Racter ,  1.0
and Jabberwacky  0.001445086705202312
Jabberwacky  1.0
Jabberwacky .  1.0
Up to  1.0
the 1980s  0.001384083044982699
, most  0.004491858506457047
most  1.0
most NLP  0.017241379310344827
were based  0.024390243902439025
based  1.0
based on  0.8333333333333334
on complex  0.0047169811320754715
complex  1.0
complex sets  0.041666666666666664
sets  1.0
sets of  0.36363636363636365
of hand-written  0.00267379679144385
hand-written  1.0
hand-written rules  0.8571428571428571
rules  1.0
rules .  0.13953488372093023
Starting in  1.0
, however  0.006176305446378439
however  1.0
however ,  0.9230769230769231
, there  0.006176305446378439
there  1.0
there was  0.075
was a  0.03896103896103896
a revolution  0.001226993865030675
revolution  1.0
revolution in  1.0
NLP with  0.02127659574468085
the introduction  0.0006920415224913495
introduction  1.0
introduction of  1.0
of machine  0.0071301247771836
learning algorithms  0.11627906976744186
algorithms for  0.11428571428571428
for language  0.0036101083032490976
processing .  0.12962962962962962
This was  0.015873015873015872
was due  0.012987012987012988
due  1.0
due both  0.4
both  1.0
both to  0.12903225806451613
the steady  0.001384083044982699
steady  1.0
steady increase  0.5
increase  1.0
increase in  0.75
In computational  0.003745318352059925
computational power  0.2
power  1.0
power resulting  0.25
resulting  1.0
resulting from  0.25
from Moore  0.009615384615384616
Moore  1.0
Moore 's  1.0
's Law  0.0196078431372549
Law  1.0
Law and  1.0
the gradual  0.0006920415224913495
gradual  1.0
gradual lessening  1.0
lessening  1.0
lessening of  1.0
the dominance  0.0006920415224913495
dominance  1.0
dominance of  1.0
of Chomskyan  0.00089126559714795
Chomskyan  1.0
Chomskyan theories  1.0
theories  1.0
theories of  0.6
of linguistics  0.00089126559714795
linguistics -LRB-  0.05
-LRB- e.g.  0.10298102981029811
e.g.  0.9642857142857143
e.g. transformational  0.017857142857142856
transformational  1.0
transformational grammar  1.0
grammar  1.0
grammar -RRB-  0.08108108108108109
, whose  0.0011229646266142617
whose  1.0
whose theoretical  0.3333333333333333
theoretical underpinnings  0.3333333333333333
underpinnings  1.0
underpinnings discouraged  1.0
discouraged  1.0
discouraged the  1.0
the sort  0.0006920415224913495
sort  1.0
sort of  0.6666666666666666
of corpus  0.00089126559714795
corpus  1.0
corpus linguistics  0.0967741935483871
linguistics that  0.1
that underlies  0.0035460992907801418
underlies  1.0
underlies the  1.0
the machine-learning  0.0006920415224913495
machine-learning  1.0
machine-learning approach  0.25
approach  1.0
approach to  0.17142857142857143
to language  0.0013280212483399733
Some of  0.19047619047619047
the earliest-used  0.001384083044982699
earliest-used  1.0
earliest-used machine  0.5
algorithms ,  0.14285714285714285
, such  0.019651880965749578
such  1.0
such as  0.7317073170731707
as decision  0.010452961672473868
decision  1.0
decision trees  1.0
trees  1.0
trees ,  0.5
, produced  0.0016844469399213925
produced  1.0
produced systems  0.2222222222222222
systems of  0.05357142857142857
of hard  0.0017825311942959
hard  1.0
hard if-then  0.3333333333333333
if-then  1.0
if-then rules  1.0
rules similar  0.046511627906976744
similar  1.0
similar to  0.5555555555555556
to existing  0.0013280212483399733
existing  1.0
existing hand-written  0.2
Increasingly ,  1.0
, research  0.0011229646266142617
research has  0.14285714285714285
has  1.0
has focused  0.047619047619047616
focused  1.0
focused on  0.9090909090909091
on statistical  0.009433962264150943
statistical models  0.21212121212121213
models  1.0
models ,  0.07692307692307693
which make  0.014492753623188406
make  1.0
make soft  0.2
soft  1.0
soft ,  0.5
, probabilistic  0.0016844469399213925
probabilistic  1.0
probabilistic decisions  0.2857142857142857
decisions  1.0
decisions based  0.2
on attaching  0.009433962264150943
attaching  1.0
attaching real-valued  1.0
real-valued  1.0
real-valued weights  0.6666666666666666
weights  1.0
weights to  0.4
the features  0.0034602076124567475
features  1.0
features making  0.038461538461538464
making  1.0
making up  0.14285714285714285
up  1.0
up the  0.045454545454545456
the input  0.005536332179930796
input data  0.14634146341463414
The cache  0.005208333333333333
cache  1.0
cache language  1.0
language models  0.013513513513513514
models upon  0.038461538461538464
upon  1.0
upon which  1.0
which many  0.007246376811594203
many speech  0.019230769230769232
speech  1.0
speech recognition  0.42105263157894735
recognition systems  0.08264462809917356
systems now  0.008928571428571428
now rely  0.07692307692307693
rely  1.0
rely are  0.14285714285714285
are examples  0.012448132780082987
examples  1.0
examples of  0.20833333333333334
of such  0.004456327985739751
such statistical  0.008130081300813009
models .  0.11538461538461539
Such models  0.25
models are  0.038461538461538464
are generally  0.016597510373443983
generally  1.0
generally more  0.18181818181818182
more robust  0.021052631578947368
robust  1.0
robust when  0.5
when given  0.05714285714285714
given  1.0
given unfamiliar  0.08333333333333333
unfamiliar  1.0
unfamiliar input  1.0
input ,  0.07317073170731707
especially input  0.13333333333333333
input that  0.04878048780487805
that contains  0.010638297872340425
contains  1.0
contains errors  0.2
errors  1.0
errors -LRB-  0.4
-LRB- as  0.018970189701897018
as is  0.013937282229965157
is very  0.012195121951219513
very common  0.04878048780487805
common  1.0
common for  0.08
for real-world  0.007220216606498195
real-world data  0.3333333333333333
data -RRB-  0.03896103896103896
and produce  0.002890173410404624
produce  1.0
produce more  0.09090909090909091
more reliable  0.031578947368421054
reliable  1.0
reliable results  0.5
results  1.0
results when  0.09523809523809523
when integrated  0.02857142857142857
integrated  1.0
integrated into  0.3333333333333333
into a  0.21794871794871795
a larger  0.0049079754601227
larger  1.0
larger system  0.125
system comprising  0.010752688172043012
comprising  1.0
comprising multiple  0.5
multiple  1.0
multiple subtasks  0.07692307692307693
subtasks  1.0
subtasks .  0.5
Many of  0.16666666666666666
the notable  0.0006920415224913495
notable  1.0
notable early  1.0
early  1.0
early successes  0.1
successes  1.0
successes occurred  1.0
occurred  1.0
occurred in  1.0
translation ,  0.10810810810810811
, due  0.0005614823133071309
due especially  0.2
especially to  0.06666666666666667
to work  0.0026560424966799467
work  1.0
work at  0.041666666666666664
at  1.0
at IBM  0.014705882352941176
IBM  0.6666666666666666
IBM Research  0.3333333333333333
Research  0.75
Research ,  0.125
, where  0.008422234699606962
where successively  0.02857142857142857
successively  1.0
successively more  1.0
more complicated  0.010526315789473684
complicated  1.0
complicated statistical  0.3333333333333333
models were  0.038461538461538464
These systems  0.23529411764705882
were able  0.024390243902439025
able  1.0
able to  1.0
to take  0.006640106241699867
take  1.0
take advantage  0.4
advantage  1.0
advantage of  0.8
of existing  0.0017825311942959
existing multilingual  0.2
multilingual  1.0
multilingual textual  0.3333333333333333
textual  1.0
textual corpora  0.2
corpora  1.0
corpora that  0.09090909090909091
that had  0.0035460992907801418
had been  0.07142857142857142
been  1.0
been produced  0.014705882352941176
produced by  0.3333333333333333
by the  0.15428571428571428
the Parliament  0.0006920415224913495
Parliament  1.0
Parliament of  0.5
of Canada  0.0017825311942959
Canada  0.8333333333333334
Canada and  0.16666666666666666
the European  0.001384083044982699
European  1.0
European Union  0.3333333333333333
Union  1.0
Union as  1.0
a result  0.0036809815950920245
result  1.0
result of  0.2727272727272727
of laws  0.00089126559714795
laws  1.0
laws calling  1.0
calling  1.0
calling for  1.0
for the  0.11191335740072202
the translation  0.004152249134948097
of all  0.0035650623885918
all  1.0
all governmental  0.023255813953488372
governmental  1.0
governmental proceedings  1.0
proceedings  1.0
proceedings into  1.0
into all  0.01282051282051282
all official  0.023255813953488372
official  1.0
official languages  1.0
languages of  0.02
the corresponding  0.001384083044982699
corresponding  1.0
corresponding systems  0.16666666666666666
of government  0.0017825311942959
government  1.0
government .  0.3333333333333333
most other  0.017241379310344827
other systems  0.02857142857142857
systems depended  0.008928571428571428
depended  1.0
depended on  1.0
on corpora  0.0047169811320754715
corpora specifically  0.09090909090909091
specifically  1.0
specifically developed  0.5
developed for  0.038461538461538464
the tasks  0.0006920415224913495
tasks  1.0
tasks implemented  0.03125
implemented  1.0
implemented by  0.2
by these  0.005714285714285714
these  1.0
these systems  0.11904761904761904
systems ,  0.05357142857142857
which was  0.036231884057971016
was -LRB-  0.012987012987012988
-LRB- and  0.013550135501355014
and often  0.004335260115606936
often  1.0
often continues  0.022727272727272728
continues  1.0
continues to  1.0
to be  0.057104913678618856
be -RRB-  0.004219409282700422
-RRB- a  0.005633802816901409
major limitation  0.08333333333333333
limitation  1.0
limitation in  1.0
the success  0.001384083044982699
success  1.0
success of  0.6
of these  0.00980392156862745
systems .  0.08928571428571429
As a  0.1111111111111111
result ,  0.2727272727272727
a great  0.00245398773006135
great  1.0
great deal  0.3333333333333333
deal  1.0
deal of  0.25
of research  0.0071301247771836
has gone  0.011904761904761904
gone  1.0
gone into  1.0
into methods  0.01282051282051282
methods  1.0
methods of  0.045454545454545456
more effectively  0.010526315789473684
effectively  1.0
effectively learning  0.3333333333333333
learning from  0.046511627906976744
from limited  0.009615384615384616
limited  1.0
limited amounts  0.1
amounts  1.0
amounts of  1.0
of data  0.006238859180035651
Recent research  0.6666666666666666
has increasingly  0.011904761904761904
increasingly  1.0
increasingly focused  0.3333333333333333
on unsupervised  0.0047169811320754715
unsupervised  1.0
unsupervised and  0.125
and semi-supervised  0.001445086705202312
semi-supervised  1.0
semi-supervised learning  0.5
algorithms .  0.11428571428571428
Such algorithms  0.125
are able  0.012448132780082987
to learn  0.00796812749003984
learn  1.0
learn from  0.07692307692307693
from data  0.019230769230769232
data that  0.025974025974025976
that has  0.02127659574468085
has not  0.023809523809523808
not  1.0
not been  0.017857142857142856
been hand-annotated  0.029411764705882353
hand-annotated  1.0
hand-annotated with  1.0
the desired  0.002768166089965398
desired  1.0
desired answers  0.2
answers  1.0
answers ,  0.08333333333333333
or using  0.009009009009009009
using  1.0
using a  0.1694915254237288
a combination  0.00245398773006135
combination  1.0
combination of  0.2
of annotated  0.00089126559714795
annotated  1.0
annotated and  0.5
and non-annotated  0.001445086705202312
non-annotated  1.0
non-annotated data  1.0
Generally ,  0.6
, this  0.003368893879842785
this task  0.04395604395604396
task  1.0
task is  0.14285714285714285
is much  0.0040650406504065045
much more  0.18181818181818182
more difficult  0.07368421052631578
difficult  1.0
difficult than  0.10714285714285714
than supervised  0.022222222222222223
supervised  1.0
supervised learning  0.3125
and typically  0.002890173410404624
typically  1.0
typically produces  0.05555555555555555
produces  1.0
produces less  0.25
less  1.0
less accurate  0.08333333333333333
accurate  1.0
accurate results  0.14285714285714285
results for  0.047619047619047616
for a  0.10469314079422383
a given  0.014723926380368098
given amount  0.041666666666666664
amount  1.0
amount of  1.0
of input  0.00267379679144385
there is  0.35
is an  0.02032520325203252
an enormous  0.007575757575757576
enormous  1.0
enormous amount  1.0
of non-annotated  0.00089126559714795
data available  0.03896103896103896
available  1.0
available -LRB-  0.058823529411764705
-LRB- including  0.0027100271002710027
including ,  0.07142857142857142
, among  0.0005614823133071309
among  1.0
among other  0.375
other things  0.04285714285714286
things  1.0
things ,  0.3333333333333333
the entire  0.0006920415224913495
entire  1.0
entire content  0.3333333333333333
content of  0.08333333333333333
the World  0.0034602076124567475
World  1.0
World Wide  0.5714285714285714
Wide  1.0
Wide Web  1.0
Web  1.0
Web -RRB-  0.1111111111111111
which can  0.036231884057971016
can  1.0
can often  0.0055248618784530384
often make  0.022727272727272728
make up  0.1
up for  0.09090909090909091
the inferior  0.0006920415224913495
inferior  1.0
inferior results  1.0
results .  0.09523809523809523
NLP using  0.02127659574468085
using machine  0.01694915254237288
learning As  0.023255813953488372
As  0.2222222222222222
As described  0.05555555555555555
described  1.0
described above  0.5
above  1.0
above ,  0.3076923076923077
, modern  0.0005614823133071309
modern approaches  0.2
approaches  1.0
approaches to  0.17857142857142858
to natural  0.0013280212483399733
-RRB- are  0.008450704225352112
The paradigm  0.005208333333333333
paradigm  1.0
paradigm of  0.3333333333333333
learning is  0.023255813953488372
is different  0.0020325203252032522
different  1.0
different from  0.12244897959183673
from that  0.009615384615384616
that of  0.028368794326241134
of most  0.00089126559714795
most prior  0.017241379310344827
prior  1.0
prior attempts  0.3333333333333333
attempts  1.0
attempts at  0.3333333333333333
at language  0.014705882352941176
Prior implementations  1.0
implementations  1.0
implementations of  1.0
of language-processing  0.00089126559714795
language-processing  1.0
language-processing tasks  1.0
tasks typically  0.03125
typically involved  0.05555555555555555
involved the  0.16666666666666666
the direct  0.0006920415224913495
direct  1.0
direct hand  0.16666666666666666
hand coding  0.07142857142857142
coding  1.0
coding of  1.0
of large  0.0035650623885918
large  1.0
large sets  0.08695652173913043
of rules  0.00267379679144385
The machine-learning  0.005208333333333333
machine-learning paradigm  0.25
paradigm calls  0.3333333333333333
calls  1.0
calls instead  1.0
instead  1.0
instead for  0.14285714285714285
for using  0.0036101083032490976
using general  0.01694915254237288
general  1.0
general learning  0.045454545454545456
algorithms --  0.02857142857142857
-- often  0.04
often ,  0.022727272727272728
, although  0.0022459292532285235
although  1.0
although not  0.16666666666666666
not always  0.008928571428571428
always  1.0
always ,  0.3333333333333333
, grounded  0.0005614823133071309
in statistical  0.003745318352059925
statistical inference  0.06060606060606061
inference  1.0
inference --  0.25
-- to  0.04
to automatically  0.00796812749003984
automatically  1.0
automatically learn  0.09523809523809523
learn such  0.07692307692307693
such rules  0.008130081300813009
rules through  0.023255813953488372
through  1.0
through the  0.5
the analysis  0.002768166089965398
large corpora  0.043478260869565216
corpora of  0.09090909090909091
of typical  0.00089126559714795
typical  1.0
typical real-world  0.1111111111111111
real-world examples  0.16666666666666666
examples .  0.16666666666666666
A corpus  0.02
corpus -LRB-  0.03225806451612903
-LRB- plural  0.0027100271002710027
plural  1.0
plural ,  0.4
, ``  0.01403705783267827
`` corpora  0.005291005291005291
corpora ''  0.09090909090909091
'' -RRB-  0.0967741935483871
a set  0.01717791411042945
set  1.0
set of  0.717948717948718
of documents  0.004456327985739751
documents  1.0
documents -LRB-  0.13157894736842105
-LRB- or  0.02710027100271003
or sometimes  0.0045045045045045045
sometimes ,  0.07692307692307693
, individual  0.0005614823133071309
individual  1.0
individual sentences  0.08333333333333333
sentences -RRB-  0.02631578947368421
-RRB- that  0.005633802816901409
that have  0.02127659574468085
have  1.0
have been  0.25
the correct  0.004152249134948097
correct  1.0
correct values  0.06666666666666667
values  1.0
values to  0.125
be learned  0.004219409282700422
learned  1.0
learned .  0.2
Consider the  1.0
the task  0.004844290657439446
task of  0.21428571428571427
of part  0.00089126559714795
part  1.0
part of  0.8148148148148148
of speech  0.040998217468805706
speech tagging  0.013157894736842105
tagging  1.0
tagging ,  0.08
, i.e.  0.0039303761931499155
i.e.  1.0
i.e. determining  0.05263157894736842
determining  1.0
determining the  0.6666666666666666
correct part  0.2
speech of  0.006578947368421052
of each  0.006238859180035651
each  1.0
each word  0.1111111111111111
word  1.0
word in  0.06666666666666667
given sentence  0.08333333333333333
sentence  1.0
sentence ,  0.125
, typically  0.0016844469399213925
typically one  0.05555555555555555
one  1.0
one that  0.015384615384615385
has never  0.023809523809523808
never  1.0
never been  0.4
been seen  0.04411764705882353
seen  1.0
seen before  0.2
before  1.0
before .  0.16666666666666666
A typical  0.04
typical machine-learning-based  0.1111111111111111
machine-learning-based  1.0
machine-learning-based implementation  1.0
implementation  1.0
implementation of  1.0
a part  0.00245398773006135
speech tagger  0.006578947368421052
tagger  1.0
tagger proceeds  0.1111111111111111
proceeds  1.0
proceeds in  1.0
in two  0.0018726591760299626
two  1.0
two steps  0.034482758620689655
steps  1.0
steps ,  0.5
a training  0.001226993865030675
training  1.0
training step  0.07142857142857142
step  1.0
step and  0.06666666666666667
and an  0.004335260115606936
an evaluation  0.007575757575757576
evaluation  1.0
evaluation step  0.037037037037037035
step .  0.13333333333333333
The first  0.036458333333333336
first step  0.06060606060606061
step --  0.13333333333333333
-- the  0.12
the training  0.002768166089965398
-- makes  0.04
makes  1.0
makes use  0.125
use  1.0
use of  0.2916666666666667
a corpus  0.0036809815950920245
corpus of  0.22580645161290322
of training  0.0035650623885918
training data  0.35714285714285715
data ,  0.12987012987012986
which consists  0.007246376811594203
consists  1.0
consists of  1.0
a large  0.0098159509202454
large number  0.08695652173913043
of sentences  0.006238859180035651
sentences ,  0.10526315789473684
, each  0.003368893879842785
each of  0.1111111111111111
of which  0.008912655971479501
which has  0.050724637681159424
has the  0.023809523809523808
speech attached  0.006578947368421052
attached  1.0
attached to  0.5
to each  0.006640106241699867
word .  0.13333333333333333
-LRB- An  0.005420054200542005
An  0.3125
An example  0.1875
such a  0.04878048780487805
corpus in  0.06451612903225806
in common  0.003745318352059925
common use  0.08
use is  0.013888888888888888
the Penn  0.005536332179930796
Penn  1.0
Penn Treebank  0.6666666666666666
Treebank  1.0
Treebank .  0.3333333333333333
This includes  0.015873015873015872
includes  1.0
includes -LRB-  0.14285714285714285
-LRB- among  0.005420054200542005
things -RRB-  0.3333333333333333
of 500  0.0017825311942959
500  1.0
500 texts  0.5
texts  1.0
texts from  0.058823529411764705
from the  0.21153846153846154
the Brown  0.005536332179930796
Brown  1.0
Brown Corpus  0.8571428571428571
Corpus  1.0
Corpus ,  0.0625
, containing  0.0011229646266142617
containing  1.0
containing examples  0.125
of various  0.00089126559714795
various  1.0
various genres  0.05555555555555555
genres  1.0
genres of  1.0
of text  0.0213903743315508
text  1.0
text ,  0.18867924528301888
and 2500  0.001445086705202312
2500  1.0
2500 articles  1.0
articles  1.0
articles from  0.125
the Wall  0.001384083044982699
Wall  1.0
Wall Street  1.0
Street  1.0
Street Journal  0.6666666666666666
Journal .  0.3333333333333333
. -RRB-  0.5
This corpus  0.031746031746031744
corpus is  0.03225806451612903
is analyzed  0.006097560975609756
analyzed  1.0
analyzed and  0.2
a learning  0.00245398773006135
learning model  0.023255813953488372
model  1.0
model is  0.1
is generated  0.006097560975609756
generated  1.0
generated from  0.13333333333333333
from it  0.009615384615384616
it ,  0.017094017094017096
, consisting  0.0005614823133071309
consisting  1.0
consisting of  1.0
of automatically  0.0017825311942959
automatically created  0.047619047619047616
created  1.0
created rules  0.14285714285714285
rules for  0.046511627906976744
for determining  0.007220216606498195
the part  0.001384083044982699
speech for  0.02631578947368421
a word  0.013496932515337423
a sentence  0.01717791411042945
typically based  0.05555555555555555
the nature  0.0034602076124567475
nature  1.0
nature of  1.0
the word  0.005536332179930796
in question  0.003745318352059925
question  1.0
question ,  0.2619047619047619
of surrounding  0.00089126559714795
surrounding  1.0
surrounding words  0.4
words  1.0
words ,  0.13761467889908258
the most  0.01314878892733564
most likely  0.05172413793103448
likely  1.0
likely part  0.0625
for those  0.007220216606498195
those  1.0
those surrounding  0.045454545454545456
words .  0.14678899082568808
The model  0.005208333333333333
model that  0.13333333333333333
that is  0.05673758865248227
generated is  0.06666666666666667
is typically  0.006097560975609756
typically the  0.05555555555555555
the best  0.008996539792387544
best  1.0
best model  0.05555555555555555
that can  0.04609929078014184
can be  0.5027624309392266
be found  0.012658227848101266
that simultaneously  0.0035460992907801418
simultaneously  1.0
simultaneously meets  0.5
meets  1.0
meets two  0.5
two conflicting  0.034482758620689655
conflicting  1.0
conflicting objectives  1.0
objectives  1.0
objectives :  0.5
:  1.0
: To  0.00980392156862745
To  0.1111111111111111
To perform  0.1111111111111111
perform  1.0
perform as  0.09090909090909091
as well  0.04878048780487805
well as  0.4642857142857143
as possible  0.017421602787456445
possible  1.0
possible on  0.041666666666666664
and to  0.015895953757225433
be as  0.012658227848101266
as simple  0.006968641114982578
simple  1.0
simple as  0.07692307692307693
possible -LRB-  0.041666666666666664
-LRB- so  0.005420054200542005
so  1.0
so that  0.2
the model  0.0020761245674740486
model avoids  0.03333333333333333
avoids  1.0
avoids overfitting  1.0
overfitting  1.0
overfitting the  0.5
i.e. so  0.05263157894736842
that it  0.010638297872340425
it generalizes  0.008547008547008548
generalizes  1.0
generalizes as  1.0
possible to  0.125
to new  0.005312084993359893
new  1.0
new data  0.041666666666666664
data rather  0.012987012987012988
rather  1.0
rather than  0.875
than only  0.044444444444444446
only  1.0
only succeeding  0.02631578947368421
succeeding  1.0
succeeding on  1.0
on sentences  0.0047169811320754715
sentences that  0.06578947368421052
have already  0.009615384615384616
already  1.0
already been  0.4
seen -RRB-  0.1
In the  0.13333333333333333
the second  0.001384083044982699
second  1.0
second step  0.2
step -LRB-  0.06666666666666667
-LRB- the  0.02168021680216802
the evaluation  0.0034602076124567475
step -RRB-  0.06666666666666667
has been  0.3333333333333333
been learned  0.029411764705882353
learned is  0.2
is used  0.026422764227642278
used to  0.19469026548672566
to process  0.00398406374501992
process new  0.027777777777777776
new sentences  0.041666666666666664
sentences .  0.10526315789473684
An important  0.125
important  1.0
important part  0.0625
the development  0.0034602076124567475
development  1.0
development of  0.5833333333333334
of any  0.00267379679144385
any  1.0
any learning  0.03225806451612903
learning algorithm  0.11627906976744186
algorithm  1.0
algorithm is  0.17857142857142858
is testing  0.0020325203252032522
testing  1.0
testing the  0.2
learned on  0.2
on new  0.0047169811320754715
new ,  0.041666666666666664
, previously  0.0005614823133071309
previously  1.0
previously unseen  0.5
unseen  1.0
unseen data  1.0
It is  0.6052631578947368
is critical  0.0020325203252032522
critical  1.0
critical that  0.25
the data  0.002768166089965398
data used  0.025974025974025976
used for  0.13274336283185842
for testing  0.0036101083032490976
testing is  0.2
is not  0.03861788617886179
not the  0.044642857142857144
the same  0.01522491349480969
same  1.0
same as  0.08
as the  0.09407665505226481
for training  0.010830324909747292
training ;  0.03571428571428571
;  1.0
; otherwise  0.02127659574468085
otherwise  1.0
otherwise ,  0.5
the testing  0.0006920415224913495
testing accuracy  0.2
accuracy  1.0
accuracy will  0.03225806451612903
will  1.0
will be  0.2571428571428571
be unrealistically  0.004219409282700422
unrealistically  1.0
unrealistically high  1.0
high  1.0
high .  0.05555555555555555
Many different  0.16666666666666666
different classes  0.02040816326530612
classes  1.0
classes of  0.4
algorithms have  0.08571428571428572
been applied  0.08823529411764706
applied  1.0
applied to  0.7333333333333333
to NLP  0.0013280212483399733
NLP tasks  0.0425531914893617
tasks .  0.125
In common  0.009523809523809525
common to  0.04
to all  0.00398406374501992
all of  0.09302325581395349
these algorithms  0.023809523809523808
algorithms is  0.02857142857142857
is that  0.024390243902439025
that they  0.024822695035460994
they  1.0
they take  0.025
take as  0.1
as input  0.006968641114982578
input a  0.024390243902439025
large set  0.043478260869565216
of ``  0.0071301247771836
`` features  0.005291005291005291
features ''  0.038461538461538464
'' that  0.021505376344086023
that are  0.05319148936170213
are generated  0.004149377593360996
As an  0.1111111111111111
a part-of-speech  0.001226993865030675
part-of-speech  1.0
part-of-speech tagger  0.06666666666666667
tagger ,  0.4444444444444444
, typical  0.0005614823133071309
typical features  0.1111111111111111
features might  0.038461538461538464
might be  0.23076923076923078
be the  0.012658227848101266
the identity  0.002768166089965398
identity  1.0
identity of  1.0
word being  0.03333333333333333
being  1.0
being processed  0.05555555555555555
processed  1.0
processed ,  0.16666666666666666
the words  0.004152249134948097
words immediately  0.009174311926605505
immediately  1.0
immediately to  1.0
the left  0.001384083044982699
left  1.0
left and  0.3333333333333333
and right  0.002890173410404624
right  1.0
right ,  0.1
the part-of-speech  0.0006920415224913495
part-of-speech tag  0.06666666666666667
tag  1.0
tag of  0.0625
word to  0.016666666666666666
left ,  0.16666666666666666
and whether  0.001445086705202312
whether  1.0
whether the  0.15384615384615385
being considered  0.1111111111111111
considered  1.0
considered or  0.1111111111111111
or its  0.0045045045045045045
its  1.0
its immediate  0.02857142857142857
immediate  1.0
immediate neighbors  1.0
neighbors  1.0
neighbors are  0.3333333333333333
are content  0.004149377593360996
content words  0.08333333333333333
words or  0.06422018348623854
or function  0.0045045045045045045
function  1.0
function words  0.125
The algorithms  0.010416666666666666
algorithms differ  0.02857142857142857
differ  1.0
differ ,  0.3333333333333333
, in  0.01909039865244245
the rules  0.0034602076124567475
rules generated  0.023255813953488372
generated .  0.2
earliest-used algorithms  0.5
the systems  0.0020761245674740486
rules that  0.09302325581395349
that were  0.014184397163120567
were then  0.024390243902439025
then  1.0
then common  0.02857142857142857
common .  0.08
each input  0.022222222222222223
input feature  0.024390243902439025
feature  1.0
feature .  0.15384615384615385
models have  0.038461538461538464
have the  0.009615384615384616
the advantage  0.0006920415224913495
advantage that  0.2
they can  0.15
can express  0.022099447513812154
express  1.0
express the  0.4
the relative  0.0006920415224913495
relative  1.0
relative certainty  0.3333333333333333
certainty  1.0
certainty of  1.0
of many  0.0017825311942959
many different  0.07692307692307693
different possible  0.02040816326530612
possible answers  0.041666666666666664
answers rather  0.08333333333333333
only one  0.02631578947368421
one ,  0.06153846153846154
, producing  0.0005614823133071309
producing more  0.3333333333333333
when such  0.05714285714285714
a model  0.001226993865030675
is included  0.0020325203252032522
included  1.0
included as  0.125
a component  0.00245398773006135
component of  0.6
system .  0.11827956989247312
In addition  0.02857142857142857
addition  1.0
addition ,  0.3333333333333333
, models  0.0005614823133071309
models that  0.11538461538461539
that make  0.010638297872340425
soft decisions  0.5
decisions are  0.1
Systems based  0.25
on machine-learning  0.0047169811320754715
machine-learning algorithms  0.25
have many  0.04807692307692308
many advantages  0.019230769230769232
advantages  1.0
advantages over  1.0
over  1.0
over hand-produced  0.08333333333333333
hand-produced  1.0
hand-produced rules  1.0
rules :  0.046511627906976744
: The  0.0392156862745098
The  0.23958333333333334
The learning  0.005208333333333333
learning procedures  0.046511627906976744
procedures  1.0
procedures used  0.25
used during  0.008849557522123894
during  1.0
during machine  0.1
learning automatically  0.023255813953488372
automatically focus  0.047619047619047616
focus  1.0
focus on  0.5714285714285714
most common  0.10344827586206896
common cases  0.04
cases  1.0
cases ,  0.3888888888888889
, whereas  0.0011229646266142617
whereas  1.0
whereas when  0.3333333333333333
when writing  0.05714285714285714
writing  1.0
writing rules  0.1111111111111111
rules by  0.023255813953488372
by hand  0.03428571428571429
hand it  0.07142857142857142
is often  0.022357723577235773
often not  0.045454545454545456
not obvious  0.008928571428571428
obvious  1.0
obvious at  1.0
at all  0.07352941176470588
all where  0.023255813953488372
where the  0.37142857142857144
the effort  0.0006920415224913495
effort  1.0
effort should  0.25
should  1.0
should be  0.47368421052631576
be directed  0.004219409282700422
directed  1.0
directed .  1.0
Automatic learning  0.1111111111111111
procedures can  0.5
can make  0.016574585635359115
make use  0.05
of statistical  0.0017825311942959
inference algorithms  0.25
algorithms to  0.08571428571428572
to produce  0.013280212483399735
produce models  0.045454545454545456
are robust  0.004149377593360996
robust to  0.25
to unfamiliar  0.0013280212483399733
input -LRB-  0.04878048780487805
e.g. containing  0.017857142857142856
containing words  0.125
or structures  0.0045045045045045045
structures  1.0
structures that  0.4
have not  0.019230769230769232
before -RRB-  0.3333333333333333
-RRB- and  0.056338028169014086
to erroneous  0.0013280212483399733
erroneous  1.0
erroneous input  1.0
e.g. with  0.017857142857142856
with misspelled  0.00546448087431694
misspelled  1.0
misspelled words  1.0
or words  0.0045045045045045045
words accidentally  0.009174311926605505
accidentally  1.0
accidentally omitted  1.0
omitted  1.0
omitted -RRB-  1.0
, handling  0.0011229646266142617
handling  1.0
handling such  0.5
such input  0.008130081300813009
input gracefully  0.024390243902439025
gracefully  1.0
gracefully with  1.0
with hand-written  0.00546448087431694
rules --  0.023255813953488372
-- or  0.04
or more  0.018018018018018018
more generally  0.010526315789473684
generally ,  0.09090909090909091
, creating  0.0011229646266142617
creating  1.0
creating systems  0.14285714285714285
decisions --  0.1
-- is  0.04
is extremely  0.0040650406504065045
extremely  1.0
extremely difficult  0.75
difficult ,  0.03571428571428571
, error-prone  0.0005614823133071309
error-prone  1.0
error-prone and  1.0
and time-consuming  0.001445086705202312
time-consuming  1.0
time-consuming .  0.3333333333333333
on automatically  0.0047169811320754715
automatically learning  0.047619047619047616
learning the  0.023255813953488372
rules can  0.06976744186046512
be made  0.016877637130801686
made  1.0
made more  0.125
more accurate  0.031578947368421054
accurate simply  0.14285714285714285
simply  1.0
simply by  0.08333333333333333
by supplying  0.005714285714285714
supplying  1.0
supplying more  1.0
more input  0.010526315789473684
, systems  0.0011229646266142617
systems based  0.017857142857142856
on hand-written  0.0047169811320754715
can only  0.011049723756906077
only be  0.02631578947368421
accurate by  0.14285714285714285
by increasing  0.005714285714285714
increasing  1.0
increasing the  0.3333333333333333
the complexity  0.005536332179930796
complexity  1.0
complexity of  0.6666666666666666
rules ,  0.11627906976744186
which is  0.09420289855072464
a much  0.0036809815950920245
difficult task  0.03571428571428571
task .  0.23809523809523808
In particular  0.02857142857142857
particular  1.0
particular ,  0.23076923076923078
a limit  0.001226993865030675
limit  1.0
limit to  0.25
of systems  0.0017825311942959
on hand-crafted  0.0047169811320754715
hand-crafted  1.0
hand-crafted rules  0.5
, beyond  0.0005614823133071309
beyond  1.0
beyond which  0.16666666666666666
which the  0.057971014492753624
systems become  0.008928571428571428
become  1.0
become more  0.25
more and  0.021052631578947368
and more  0.0072254335260115606
more unmanageable  0.010526315789473684
unmanageable  1.0
unmanageable .  1.0
creating more  0.14285714285714285
more data  0.021052631578947368
data to  0.012987012987012988
to input  0.0026560424966799467
input to  0.07317073170731707
to machine-learning  0.0013280212483399733
machine-learning systems  0.25
systems simply  0.008928571428571428
simply requires  0.08333333333333333
requires a  0.0625
a corresponding  0.001226993865030675
corresponding increase  0.16666666666666666
the number  0.004844290657439446
of man-hours  0.00089126559714795
man-hours  1.0
man-hours worked  1.0
worked  1.0
worked ,  0.2
, generally  0.0005614823133071309
generally without  0.09090909090909091
without  1.0
without significant  0.07692307692307693
significant  1.0
significant increases  0.1111111111111111
increases  0.5
increases in  1.0
the annotation  0.0006920415224913495
annotation  1.0
annotation process  0.25
process .  0.1388888888888889
Major tasks  0.5
tasks in  0.09375
NLP The  0.0425531914893617
The following  0.020833333333333332
following  1.0
following is  0.06666666666666667
a list  0.008588957055214725
list  1.0
list of  0.7272727272727273
of some  0.004456327985739751
some  1.0
some of  0.1566265060240964
most commonly  0.017241379310344827
commonly  1.0
commonly researched  0.125
researched  1.0
researched tasks  1.0
Note that  0.7777777777777778
that some  0.014184397163120567
these tasks  0.047619047619047616
tasks have  0.03125
have direct  0.009615384615384616
direct real-world  0.16666666666666666
real-world applications  0.16666666666666666
applications  1.0
applications ,  0.16
, while  0.007860752386299831
while  1.0
while others  0.2
others  1.0
others more  0.08333333333333333
more commonly  0.021052631578947368
commonly serve  0.125
serve  1.0
serve as  0.8
as subtasks  0.003484320557491289
subtasks that  0.5
are used  0.03319502074688797
to aid  0.0013280212483399733
aid  1.0
aid in  0.75
in solving  0.0018726591760299626
solving  1.0
solving larger  1.0
larger tasks  0.0625
What distinguishes  0.09090909090909091
distinguishes  1.0
distinguishes these  0.5
tasks from  0.03125
from other  0.009615384615384616
other potential  0.014285714285714285
potential  1.0
potential and  0.14285714285714285
and actual  0.001445086705202312
actual  1.0
actual NLP  0.2
tasks is  0.03125
not only  0.0625
only the  0.10526315789473684
the volume  0.0006920415224913495
volume  1.0
volume of  0.5
research devoted  0.023809523809523808
devoted to  0.6
to them  0.0026560424966799467
them  1.0
them but  0.05263157894736842
but  1.0
but the  0.04411764705882353
the fact  0.002768166089965398
fact  1.0
fact that  0.45454545454545453
that for  0.0035460992907801418
for each  0.02527075812274368
each one  0.044444444444444446
one there  0.015384615384615385
typically a  0.05555555555555555
a well-defined  0.001226993865030675
well-defined  1.0
well-defined problem  1.0
problem setting  0.022727272727272728
setting  1.0
setting ,  0.4
a standard  0.00245398773006135
standard  1.0
standard metric  0.07142857142857142
metric  1.0
metric for  0.3333333333333333
for evaluating  0.010830324909747292
evaluating  1.0
evaluating the  0.2
task ,  0.09523809523809523
, standard  0.0011229646266142617
standard corpora  0.07142857142857142
corpora on  0.09090909090909091
on which  0.014150943396226415
task can  0.023809523809523808
be evaluated  0.008438818565400843
evaluated  1.0
evaluated ,  0.14285714285714285
and competitions  0.001445086705202312
competitions  1.0
competitions devoted  1.0
the specific  0.0020761245674740486
specific  1.0
specific task  0.047619047619047616
Automatic summarization  0.2222222222222222
summarization  1.0
summarization :  0.02
: Produce  0.00980392156862745
Produce  1.0
Produce a  1.0
a readable  0.001226993865030675
readable  1.0
readable summary  0.3333333333333333
summary  1.0
summary of  0.07142857142857142
a chunk  0.007361963190184049
chunk  1.0
chunk of  1.0
text .  0.16981132075471697
Often used  0.3333333333333333
to provide  0.005312084993359893
provide summaries  0.16666666666666666
summaries  1.0
summaries of  0.09302325581395349
text of  0.006289308176100629
a known  0.00245398773006135
known  1.0
known type  0.038461538461538464
type  1.0
type ,  0.07142857142857142
as articles  0.003484320557491289
articles in  0.25
the financial  0.0006920415224913495
financial  1.0
financial section  0.25
section  1.0
section of  0.16666666666666666
a newspaper  0.001226993865030675
newspaper  1.0
newspaper .  0.3333333333333333
Coreference resolution  1.0
resolution  1.0
resolution :  0.25
: Given  0.09803921568627451
Given  0.7857142857142857
Given a  0.7142857142857143
sentence or  0.041666666666666664
or larger  0.0045045045045045045
larger chunk  0.0625
, determine  0.003368893879842785
determine  1.0
determine which  0.08695652173913043
which words  0.014492753623188406
words -LRB-  0.027522935779816515
-LRB- ``  0.02168021680216802
`` mentions  0.005291005291005291
mentions  1.0
mentions ''  0.3333333333333333
-RRB- refer  0.0028169014084507044
refer  1.0
refer to  1.0
same objects  0.04
objects  1.0
objects -LRB-  0.2
`` entities  0.005291005291005291
entities  1.0
entities ''  0.14285714285714285
Anaphora resolution  1.0
resolution is  0.25
a specific  0.006134969325153374
specific example  0.047619047619047616
of this  0.00980392156862745
and is  0.008670520231213872
is specifically  0.0020325203252032522
specifically concerned  0.5
with matching  0.00546448087431694
matching  1.0
matching up  0.2
up pronouns  0.045454545454545456
pronouns  1.0
pronouns with  0.5
the nouns  0.0006920415224913495
nouns  1.0
nouns or  0.1111111111111111
or names  0.0045045045045045045
names  1.0
names that  0.2857142857142857
they refer  0.05
to .  0.00398406374501992
For example  0.6229508196721312
sentence such  0.020833333333333332
`` He  0.005291005291005291
He  0.125
He entered  0.125
entered  1.0
entered John  0.5
John  1.0
John 's  0.25
's house  0.0392156862745098
house  1.0
house through  0.5
the front  0.0020761245674740486
front  1.0
front door  1.0
door  1.0
door ''  0.5
'' is  0.04838709677419355
a referring  0.001226993865030675
referring  1.0
referring expression  0.5
expression  1.0
expression and  0.2
the bridging  0.0006920415224913495
bridging  1.0
bridging relationship  1.0
relationship  1.0
relationship to  0.16666666666666666
be identified  0.008438818565400843
identified  1.0
identified is  0.2
the door  0.0006920415224913495
door being  0.25
being referred  0.05555555555555555
to is  0.0013280212483399733
door of  0.25
of John  0.00089126559714795
house -LRB-  0.5
-LRB- rather  0.0027100271002710027
than of  0.022222222222222223
some other  0.08433734939759036
other structure  0.014285714285714285
structure  1.0
structure that  0.08333333333333333
that might  0.0070921985815602835
might also  0.038461538461538464
also be  0.10144927536231885
be referred  0.004219409282700422
to -RRB-  0.0013280212483399733
Discourse analysis  1.0
analysis :  0.06153846153846154
: This  0.0392156862745098
This  0.1746031746031746
This rubric  0.015873015873015872
rubric  1.0
rubric includes  1.0
includes a  0.14285714285714285
of related  0.00267379679144385
related  1.0
related tasks  0.2
One task  0.07692307692307693
is identifying  0.0020325203252032522
identifying  1.0
identifying the  0.6666666666666666
the discourse  0.0020761245674740486
discourse  1.0
discourse structure  0.027777777777777776
structure of  0.3333333333333333
of connected  0.00089126559714795
connected  1.0
connected text  0.2
i.e. the  0.2631578947368421
discourse relationships  0.027777777777777776
relationships  1.0
relationships between  0.16666666666666666
between sentences  0.05128205128205128
sentences -LRB-  0.02631578947368421
e.g. elaboration  0.017857142857142856
elaboration  1.0
elaboration ,  1.0
, explanation  0.0005614823133071309
explanation  1.0
explanation ,  1.0
, contrast  0.0005614823133071309
contrast  1.0
contrast -RRB-  0.125
Another possible  0.07692307692307693
possible task  0.041666666666666664
is recognizing  0.0020325203252032522
recognizing  1.0
recognizing and  0.2
and classifying  0.001445086705202312
classifying  1.0
classifying the  0.2
the speech  0.006920415224913495
speech acts  0.019736842105263157
acts  1.0
acts in  0.3333333333333333
text -LRB-  0.03773584905660377
e.g. yes-no  0.017857142857142856
yes-no  1.0
yes-no question  1.0
, content  0.0005614823133071309
content question  0.08333333333333333
, statement  0.0005614823133071309
statement  1.0
statement ,  1.0
, assertion  0.0005614823133071309
assertion  1.0
assertion ,  1.0
, etc.  0.011229646266142616
etc.  1.0
etc. -RRB-  0.4090909090909091
Machine translation  0.5555555555555556
translation :  0.02702702702702703
: Automatically  0.00980392156862745
Automatically  1.0
Automatically translate  1.0
translate  1.0
translate text  0.3333333333333333
text from  0.012578616352201259
from one  0.028846153846153848
one human  0.015384615384615385
human language  0.06521739130434782
language to  0.02702702702702703
to another  0.00398406374501992
another  1.0
another .  0.23076923076923078
This is  0.2698412698412698
is one  0.012195121951219513
one of  0.2153846153846154
most difficult  0.017241379310344827
difficult problems  0.10714285714285714
problems  1.0
problems ,  0.35294117647058826
a member  0.001226993865030675
member  1.0
member of  1.0
a class  0.001226993865030675
class  1.0
class of  0.75
of problems  0.0017825311942959
problems colloquially  0.11764705882352941
colloquially  1.0
colloquially termed  1.0
termed  1.0
termed ``  0.5
`` AI-complete  0.010582010582010581
AI-complete ''  0.6666666666666666
i.e. requiring  0.05263157894736842
requiring  1.0
requiring all  0.5
the different  0.0006920415224913495
different types  0.04081632653061224
of knowledge  0.0017825311942959
knowledge that  0.037037037037037035
that humans  0.0070921985815602835
humans  1.0
humans possess  0.08333333333333333
possess  1.0
possess -LRB-  1.0
-LRB- grammar  0.0027100271002710027
grammar ,  0.10810810810810811
, semantics  0.0016844469399213925
semantics  1.0
semantics ,  0.2857142857142857
, facts  0.0005614823133071309
facts  1.0
facts about  1.0
the real  0.0020761245674740486
real world  0.3333333333333333
world ,  0.06666666666666667
-RRB- in  0.011267605633802818
in order  0.013108614232209739
order  1.0
order to  0.5714285714285714
to solve  0.005312084993359893
solve  1.0
solve properly  0.25
properly  1.0
properly .  0.5
Morphological segmentation  1.0
segmentation  1.0
segmentation :  0.09090909090909091
: Separate  0.0196078431372549
Separate  1.0
Separate words  0.5
words into  0.03669724770642202
into individual  0.01282051282051282
individual morphemes  0.08333333333333333
morphemes  1.0
morphemes and  0.3333333333333333
and identify  0.002890173410404624
identify  1.0
identify the  0.5
the class  0.0006920415224913495
the morphemes  0.0006920415224913495
morphemes .  0.3333333333333333
The difficulty  0.015625
difficulty  1.0
difficulty of  0.42857142857142855
task depends  0.023809523809523808
depends greatly  0.125
greatly  1.0
greatly on  0.14285714285714285
the morphology  0.0006920415224913495
morphology  1.0
morphology -LRB-  0.14285714285714285
-LRB- i.e.  0.02981029810298103
the structure  0.0020761245674740486
of words  0.013368983957219251
words -RRB-  0.027522935779816515
-RRB- of  0.01971830985915493
the language  0.005536332179930796
language being  0.013513513513513514
considered .  0.1111111111111111
English has  0.05405405405405406
has fairly  0.011904761904761904
fairly  1.0
fairly simple  0.25
simple morphology  0.038461538461538464
morphology ,  0.7142857142857143
especially inflectional  0.06666666666666667
inflectional  1.0
inflectional morphology  1.0
and thus  0.004335260115606936
thus  1.0
thus it  0.1
often possible  0.022727272727272728
to ignore  0.0013280212483399733
ignore  1.0
ignore this  1.0
task entirely  0.023809523809523808
entirely  1.0
entirely and  0.5
and simply  0.001445086705202312
simply model  0.08333333333333333
model all  0.03333333333333333
all possible  0.06976744186046512
possible forms  0.041666666666666664
forms  1.0
forms of  0.3333333333333333
word -LRB-  0.016666666666666666
e.g. ``  0.017857142857142856
`` open  0.005291005291005291
open  1.0
open ,  0.25
, opens  0.0005614823133071309
opens  1.0
opens ,  1.0
, opened  0.0005614823133071309
opened  1.0
opened ,  1.0
, opening  0.0005614823133071309
opening  1.0
opening ''  1.0
-RRB- as  0.008450704225352112
as separate  0.003484320557491289
separate  1.0
separate words  0.3
In languages  0.009523809523809525
languages such  0.1
as Turkish  0.003484320557491289
Turkish  1.0
Turkish ,  1.0
such an  0.016260162601626018
an approach  0.030303030303030304
approach is  0.14285714285714285
not possible  0.008928571428571428
possible ,  0.125
, as  0.01291409320606401
as each  0.003484320557491289
each dictionary  0.022222222222222223
dictionary  1.0
dictionary entry  0.14285714285714285
entry  1.0
entry has  0.25
has thousands  0.011904761904761904
thousands  1.0
thousands of  0.6666666666666666
of possible  0.00267379679144385
possible word  0.041666666666666664
word forms  0.016666666666666666
forms .  0.16666666666666666
Named entity  1.0
entity  1.0
entity recognition  0.4
recognition -LRB-  0.049586776859504134
-LRB- NER  0.0027100271002710027
NER  1.0
NER -RRB-  1.0
-RRB- :  0.02535211267605634
a stream  0.001226993865030675
stream  1.0
stream of  0.5
which items  0.007246376811594203
items  1.0
items in  0.5
the text  0.017993079584775088
text map  0.006289308176100629
map  1.0
map to  0.5
to proper  0.0013280212483399733
proper  1.0
proper names  0.14285714285714285
names ,  0.2857142857142857
as people  0.003484320557491289
people  1.0
people or  0.0625
or places  0.0045045045045045045
places  1.0
places ,  0.5
and what  0.001445086705202312
what the  0.125
the type  0.002768166089965398
type of  0.5714285714285714
each such  0.022222222222222223
such name  0.008130081300813009
name  1.0
name is  0.2
is -LRB-  0.0040650406504065045
e.g. person  0.017857142857142856
person  1.0
person ,  0.21052631578947367
, location  0.0005614823133071309
location  1.0
location ,  1.0
, organization  0.0011229646266142617
organization  1.0
organization -RRB-  0.2
that ,  0.0035460992907801418
although capitalization  0.16666666666666666
capitalization  1.0
capitalization can  0.3333333333333333
can aid  0.0055248618784530384
in recognizing  0.003745318352059925
recognizing named  0.2
named  1.0
named entities  0.42857142857142855
entities in  0.14285714285714285
in languages  0.0018726591760299626
as English  0.010452961672473868
English ,  0.16216216216216217
this information  0.01098901098901099
information can  0.021739130434782608
can not  0.08287292817679558
not aid  0.008928571428571428
in determining  0.0018726591760299626
of named  0.00089126559714795
named entity  0.2857142857142857
entity ,  0.4
and in  0.010115606936416185
in any  0.0018726591760299626
any case  0.0967741935483871
case  1.0
case is  0.058823529411764705
often inaccurate  0.022727272727272728
inaccurate  1.0
inaccurate or  1.0
or insufficient  0.0045045045045045045
insufficient  1.0
insufficient .  1.0
first word  0.030303030303030304
word of  0.016666666666666666
sentence is  0.041666666666666664
is also  0.02032520325203252
also capitalized  0.014492753623188406
capitalized  1.0
capitalized ,  0.6666666666666666
and named  0.001445086705202312
entities often  0.14285714285714285
often span  0.022727272727272728
span  1.0
span several  1.0
several  1.0
several words  0.045454545454545456
, only  0.0011229646266142617
only some  0.05263157894736842
which are  0.08695652173913043
are capitalized  0.004149377593360996
capitalized .  0.3333333333333333
Furthermore ,  1.0
many other  0.09615384615384616
other languages  0.07142857142857142
languages in  0.02
in non-Western  0.0018726591760299626
non-Western  1.0
non-Western scripts  1.0
scripts  1.0
scripts -LRB-  0.6666666666666666
e.g. Chinese  0.017857142857142856
Chinese  0.8571428571428571
Chinese or  0.14285714285714285
or Arabic  0.0045045045045045045
Arabic  1.0
Arabic -RRB-  0.25
-RRB- do  0.0028169014084507044
do not  0.5
not have  0.017857142857142856
have any  0.009615384615384616
any capitalization  0.03225806451612903
capitalization at  0.3333333333333333
all ,  0.06976744186046512
and even  0.008670520231213872
even  1.0
even languages  0.037037037037037035
languages with  0.02
with capitalization  0.00546448087431694
capitalization may  0.3333333333333333
may  1.0
may not  0.09615384615384616
not consistently  0.017857142857142856
consistently  1.0
consistently use  0.3333333333333333
use it  0.027777777777777776
it to  0.042735042735042736
distinguish names  0.2
names .  0.2857142857142857
, German  0.0011229646266142617
German  1.0
German capitalizes  0.25
capitalizes  1.0
capitalizes all  1.0
all nouns  0.023255813953488372
nouns ,  0.6666666666666666
, regardless  0.0016844469399213925
regardless  1.0
regardless of  1.0
of whether  0.00089126559714795
whether they  0.07692307692307693
to names  0.0013280212483399733
and French  0.001445086705202312
French  1.0
French and  0.25
and Spanish  0.001445086705202312
Spanish  1.0
Spanish do  0.5
not capitalize  0.008928571428571428
capitalize  1.0
capitalize names  1.0
that serve  0.0035460992907801418
as adjectives  0.003484320557491289
adjectives  1.0
adjectives .  0.3333333333333333
language generation  0.033783783783783786
generation  1.0
generation :  0.2222222222222222
: Convert  0.0196078431372549
Convert  1.0
Convert information  0.5
from computer  0.009615384615384616
computer databases  0.022727272727272728
databases  1.0
databases into  0.125
into readable  0.01282051282051282
readable human  0.3333333333333333
language .  0.07432432432432433
understanding :  0.030303030303030304
Convert chunks  0.5
chunks  1.0
chunks of  1.0
text into  0.0440251572327044
into more  0.02564102564102564
more formal  0.010526315789473684
formal  1.0
formal representations  0.2222222222222222
representations  1.0
representations such  0.25
as first-order  0.003484320557491289
first-order  1.0
first-order logic  1.0
logic  1.0
logic structures  0.25
are easier  0.004149377593360996
easier  1.0
easier for  0.125
for computer  0.010830324909747292
computer programs  0.045454545454545456
programs  1.0
programs to  0.09090909090909091
manipulate .  0.3333333333333333
understanding involves  0.030303030303030304
involves  1.0
involves the  0.2
the identification  0.001384083044982699
identification  1.0
identification of  0.4
the intended  0.001384083044982699
intended  1.0
intended semantic  0.2
semantic  1.0
semantic from  0.047619047619047616
the multiple  0.0006920415224913495
multiple possible  0.15384615384615385
possible semantics  0.041666666666666664
semantics which  0.14285714285714285
be derived  0.008438818565400843
derived  1.0
derived from  0.5
from a  0.11538461538461539
language expression  0.006756756756756757
expression which  0.1
which usually  0.007246376811594203
usually  1.0
usually takes  0.03125
takes  1.0
takes the  0.3333333333333333
the form  0.0006920415224913495
form  1.0
form of  0.35
of organized  0.00089126559714795
organized  1.0
organized notations  1.0
notations  1.0
notations of  0.5
natural languages  0.12
languages concepts  0.02
concepts  1.0
concepts .  0.4
Introduction and  1.0
and creation  0.001445086705202312
creation  1.0
creation of  1.0
of language  0.004456327985739751
language metamodel  0.006756756756756757
metamodel  1.0
metamodel and  1.0
and ontology  0.001445086705202312
ontology  1.0
ontology are  0.5
are efficient  0.004149377593360996
efficient  1.0
efficient however  0.3333333333333333
however empirical  0.07692307692307693
empirical  1.0
empirical solutions  1.0
solutions  1.0
solutions .  0.5
An explicit  0.0625
explicit  1.0
explicit formalization  0.2
formalization  1.0
formalization of  0.5
languages semantics  0.02
semantics without  0.07142857142857142
without confusions  0.07692307692307693
confusions  1.0
confusions with  1.0
with implicit  0.00546448087431694
implicit  1.0
implicit assumptions  1.0
assumptions  1.0
assumptions such  0.2
as closed  0.003484320557491289
closed  1.0
closed world  1.0
world assumption  0.13333333333333333
assumption  1.0
assumption -LRB-  0.5
-LRB- CWA  0.0027100271002710027
CWA  1.0
CWA -RRB-  1.0
-RRB- vs.  0.0028169014084507044
vs.  0.8333333333333334
vs. open  0.08333333333333333
open world  0.25
assumption ,  0.5
or subjective  0.009009009009009009
subjective  1.0
subjective Yes\/No  0.16666666666666666
Yes\/No  1.0
Yes\/No vs.  1.0
vs. objective  0.08333333333333333
objective  1.0
objective True\/False  0.2
True\/False  1.0
True\/False is  1.0
is expected  0.0020325203252032522
expected  1.0
expected for  0.14285714285714285
the construction  0.0006920415224913495
construction  1.0
construction of  0.6666666666666666
a basis  0.00245398773006135
of semantics  0.00089126559714795
semantics formalization  0.07142857142857142
formalization .  0.5
Optical character  0.6666666666666666
character  1.0
character recognition  0.5
-LRB- OCR  0.0027100271002710027
OCR  0.9183673469387755
OCR -RRB-  0.02040816326530612
Given an  0.07142857142857142
an image  0.007575757575757576
image  1.0
image representing  0.3333333333333333
representing  1.0
representing printed  0.5
printed  1.0
printed text  0.25
determine the  0.391304347826087
corresponding text  0.16666666666666666
Part-of-speech tagging  0.5
tagging :  0.04
Many words  0.16666666666666666
especially common  0.13333333333333333
common ones  0.04
ones  1.0
ones ,  0.3
, can  0.003368893879842785
can serve  0.011049723756906077
as multiple  0.003484320557491289
multiple parts  0.07692307692307693
parts  1.0
parts of  1.0
speech .  0.06578947368421052
`` book  0.005291005291005291
book  1.0
book ''  0.125
'' can  0.026881720430107527
a noun  0.007361963190184049
noun  1.0
noun -LRB-  0.07142857142857142
the book  0.001384083044982699
book on  0.125
the table  0.001384083044982699
table  1.0
table ''  0.14285714285714285
-RRB- or  0.011267605633802818
or verb  0.0045045045045045045
verb  1.0
verb -LRB-  0.15384615384615385
`` to  0.010582010582010581
to book  0.0013280212483399733
book a  0.125
a flight  0.001226993865030675
flight  1.0
flight ''  0.5
-RRB- ;  0.022535211267605635
; ``  0.02127659574468085
`` set  0.005291005291005291
set ''  0.05128205128205128
noun ,  0.42857142857142855
, verb  0.0011229646266142617
verb or  0.23076923076923078
or adjective  0.0045045045045045045
adjective  1.0
adjective ;  0.14285714285714285
; and  0.0851063829787234
and ``  0.028901734104046242
`` out  0.005291005291005291
out  1.0
out ''  0.07142857142857142
be any  0.004219409282700422
any of  0.06451612903225806
of at  0.00089126559714795
at least  0.07352941176470588
least  1.0
least five  0.2
five different  0.2
different parts  0.04081632653061224
some languages  0.024096385542168676
languages have  0.04
have more  0.028846153846153848
more such  0.010526315789473684
such ambiguity  0.024390243902439025
ambiguity  1.0
ambiguity than  0.125
than others  0.044444444444444446
others .  0.25
Languages with  0.3333333333333333
with little  0.00546448087431694
little  1.0
little inflectional  0.3333333333333333
English are  0.02702702702702703
are particularly  0.004149377593360996
particularly  1.0
particularly prone  0.2
prone  1.0
prone to  1.0
to such  0.0026560424966799467
ambiguity .  0.125
Chinese is  0.14285714285714285
is prone  0.0020325203252032522
ambiguity because  0.125
a tonal  0.001226993865030675
tonal  1.0
tonal language  1.0
language during  0.006756756756756757
during verbalization  0.1
verbalization  1.0
verbalization .  1.0
Such inflection  0.125
inflection  1.0
inflection is  1.0
not readily  0.008928571428571428
readily  1.0
readily conveyed  0.3333333333333333
conveyed  1.0
conveyed via  1.0
via  1.0
via the  1.0
the entities  0.0006920415224913495
entities employed  0.14285714285714285
employed  1.0
employed within  1.0
within the  0.16666666666666666
the orthography  0.0006920415224913495
orthography  1.0
orthography to  0.5
to convey  0.00398406374501992
convey  1.0
convey intended  0.3333333333333333
intended meaning  0.2
meaning  1.0
meaning .  0.08695652173913043
Parsing :  0.2
: Determine  0.00980392156862745
Determine  1.0
Determine the  1.0
the parse  0.0006920415224913495
parse  1.0
parse tree  0.1111111111111111
tree  1.0
tree -LRB-  1.0
-LRB- grammatical  0.0027100271002710027
grammatical  1.0
grammatical analysis  0.09090909090909091
analysis -RRB-  0.03076923076923077
sentence .  0.14583333333333334
The grammar  0.010416666666666666
grammar for  0.02702702702702703
for natural  0.01444043321299639
languages is  0.02
is ambiguous  0.0020325203252032522
ambiguous  1.0
ambiguous and  0.16666666666666666
and typical  0.001445086705202312
typical sentences  0.1111111111111111
sentences have  0.02631578947368421
have multiple  0.009615384615384616
possible analyses  0.08333333333333333
analyses  1.0
analyses .  0.4
In fact  0.0380952380952381
fact ,  0.45454545454545453
, perhaps  0.0016844469399213925
perhaps  1.0
perhaps surprisingly  0.16666666666666666
surprisingly  1.0
surprisingly ,  0.3333333333333333
a typical  0.00245398773006135
typical sentence  0.1111111111111111
sentence there  0.020833333333333332
there may  0.025
may be  0.40384615384615385
be thousands  0.004219409282700422
of potential  0.0017825311942959
potential parses  0.14285714285714285
parses  1.0
parses -LRB-  0.5
-LRB- most  0.01084010840108401
most of  0.08620689655172414
which will  0.021739130434782608
will seem  0.02857142857142857
seem  1.0
seem completely  0.5
completely  1.0
completely nonsensical  1.0
nonsensical  1.0
nonsensical to  1.0
to a  0.03718459495351926
human -RRB-  0.043478260869565216
Question answering  0.2857142857142857
answering  1.0
answering :  0.08333333333333333
a human-language  0.001226993865030675
human-language  1.0
human-language question  1.0
determine its  0.08695652173913043
its answer  0.02857142857142857
answer  1.0
answer .  0.23333333333333334
Typical questions  0.5
questions  1.0
questions have  0.038461538461538464
have a  0.125
specific right  0.047619047619047616
right answer  0.1
answer -LRB-  0.03333333333333333
-LRB- such  0.02168021680216802
`` What  0.015873015873015872
What  0.6363636363636364
What is  0.2727272727272727
the capital  0.001384083044982699
capital  1.0
capital of  0.6666666666666666
Canada ?  0.16666666666666666
, but  0.02695115103874228
but sometimes  0.014705882352941176
sometimes open-ended  0.07692307692307693
open-ended  1.0
open-ended questions  1.0
questions are  0.07692307692307693
are also  0.03319502074688797
also considered  0.014492753623188406
considered -LRB-  0.1111111111111111
the meaning  0.006920415224913495
meaning of  0.30434782608695654
of life  0.00089126559714795
life  1.0
life ?  0.25
Relationship extraction  1.0
extraction  1.0
extraction :  0.06451612903225806
, identify  0.0011229646266142617
the relationships  0.0006920415224913495
relationships among  0.16666666666666666
among named  0.125
entities -LRB-  0.14285714285714285
e.g. who  0.017857142857142856
who  1.0
who is  0.2
the wife  0.0006920415224913495
wife  1.0
wife of  1.0
of whom  0.00089126559714795
whom  1.0
whom -RRB-  0.5
Sentence breaking  0.2
breaking  1.0
breaking -LRB-  0.5
also known  0.08695652173913043
known as  0.38461538461538464
as sentence  0.006968641114982578
sentence boundary  0.0625
boundary  1.0
boundary disambiguation  0.3333333333333333
disambiguation  1.0
disambiguation -RRB-  0.2
, find  0.0011229646266142617
find  1.0
find the  0.3076923076923077
the sentence  0.004152249134948097
sentence boundaries  0.08333333333333333
boundaries  1.0
boundaries .  0.36363636363636365
Sentence boundaries  0.2
boundaries are  0.09090909090909091
are often  0.016597510373443983
often marked  0.022727272727272728
marked  1.0
marked by  0.3333333333333333
by periods  0.005714285714285714
periods  1.0
periods or  0.3333333333333333
or other  0.009009009009009009
other punctuation  0.014285714285714285
punctuation  1.0
punctuation marks  0.2857142857142857
marks  1.0
marks ,  0.25
but these  0.014705882352941176
these same  0.023809523809523808
same characters  0.04
characters  1.0
characters can  0.1875
serve other  0.2
other purposes  0.014285714285714285
purposes  1.0
purposes -LRB-  0.25
e.g. marking  0.017857142857142856
marking  1.0
marking abbreviations  0.5
abbreviations  1.0
abbreviations -RRB-  0.2
Sentiment analysis  0.8333333333333334
: Extract  0.00980392156862745
Extract  1.0
Extract subjective  1.0
subjective information  0.3333333333333333
information usually  0.021739130434782608
usually from  0.03125
documents ,  0.23684210526315788
, often  0.0016844469399213925
often using  0.022727272727272728
using online  0.01694915254237288
online reviews  0.25
reviews  1.0
reviews to  0.16666666666666666
to determine  0.014608233731739707
determine ``  0.043478260869565216
`` polarity  0.005291005291005291
polarity  1.0
polarity ''  0.25
'' about  0.005376344086021506
about specific  0.025
specific objects  0.047619047619047616
objects .  0.2
is especially  0.0040650406504065045
especially useful  0.06666666666666667
useful  1.0
useful for  0.21428571428571427
for identifying  0.0036101083032490976
identifying trends  0.16666666666666666
trends  1.0
trends of  1.0
of public  0.00089126559714795
public  1.0
public opinion  1.0
opinion  1.0
opinion in  0.4
the social  0.0020761245674740486
social  1.0
social media  0.2857142857142857
media  1.0
media ,  0.5
the purpose  0.001384083044982699
purpose  1.0
purpose of  0.2
of marketing  0.00089126559714795
marketing  1.0
marketing .  1.0
Speech recognition  0.2903225806451613
recognition :  0.01652892561983471
a sound  0.008588957055214725
sound  1.0
sound clip  0.1
clip  1.0
clip of  1.0
a person  0.013496932515337423
person or  0.10526315789473684
or people  0.009009009009009009
people speaking  0.125
speaking  1.0
speaking ,  0.625
the textual  0.0006920415224913495
textual representation  0.2
representation  1.0
representation of  0.10526315789473684
the opposite  0.001384083044982699
opposite  1.0
opposite of  1.0
text to  0.0440251572327044
to speech  0.00398406374501992
speech and  0.013157894736842105
the extremely  0.0006920415224913495
-LRB- see  0.032520325203252036
see above  0.05
above -RRB-  0.07692307692307693
In natural  0.009523809523809525
natural speech  0.02666666666666667
speech there  0.006578947368421052
there are  0.375
are hardly  0.004149377593360996
hardly  1.0
hardly any  1.0
any pauses  0.03225806451612903
pauses  1.0
pauses between  0.5
between successive  0.02564102564102564
successive  1.0
successive words  0.5
thus speech  0.1
speech segmentation  0.03289473684210526
segmentation is  0.2727272727272727
a necessary  0.001226993865030675
necessary  1.0
necessary subtask  0.1
subtask  1.0
subtask of  1.0
see below  0.05
below  1.0
below -RRB-  0.4
Note also  0.1111111111111111
also that  0.014492753623188406
that in  0.0070921985815602835
in most  0.00749063670411985
most spoken  0.034482758620689655
spoken  1.0
spoken languages  0.14285714285714285
languages ,  0.22
the sounds  0.001384083044982699
sounds  1.0
sounds representing  0.06666666666666667
representing successive  0.5
successive letters  0.5
letters  1.0
letters blend  0.1
blend  1.0
blend into  0.3333333333333333
into each  0.01282051282051282
each other  0.13333333333333333
other in  0.014285714285714285
a process  0.0049079754601227
process termed  0.027777777777777776
termed coarticulation  0.25
coarticulation  1.0
coarticulation ,  1.0
, so  0.00673778775968557
so the  0.23333333333333334
the conversion  0.0006920415224913495
conversion  1.0
conversion of  0.6666666666666666
the analog  0.0006920415224913495
analog  1.0
analog signal  0.5
signal  1.0
signal to  0.16666666666666666
to discrete  0.0013280212483399733
discrete  1.0
discrete characters  0.3333333333333333
very difficult  0.04878048780487805
difficult process  0.03571428571428571
Speech segmentation  0.0967741935483871
, separate  0.0011229646266142617
separate it  0.2
it into  0.042735042735042736
into words  0.038461538461538464
A subtask  0.02
recognition and  0.05785123966942149
typically grouped  0.05555555555555555
grouped  1.0
grouped with  0.5
with it  0.01092896174863388
Topic segmentation  1.0
segmentation and  0.06060606060606061
and recognition  0.001445086705202312
into segments  0.02564102564102564
segments  1.0
segments each  0.2
is devoted  0.0020325203252032522
a topic  0.001226993865030675
topic  1.0
topic ,  0.25
the topic  0.001384083044982699
topic of  0.125
the segment  0.0006920415224913495
segment  1.0
segment .  0.1111111111111111
Word segmentation  0.14285714285714285
Separate a  0.5
of continuous  0.0017825311942959
continuous  1.0
continuous text  0.16666666666666666
into separate  0.02564102564102564
a language  0.007361963190184049
language like  0.006756756756756757
like  1.0
like English  0.07142857142857142
this is  0.0989010989010989
is fairly  0.0020325203252032522
fairly trivial  0.25
trivial  1.0
trivial ,  0.25
, since  0.002807411566535654
since  1.0
since words  0.1
words are  0.09174311926605505
are usually  0.012448132780082987
usually separated  0.03125
separated  1.0
separated by  0.6666666666666666
by spaces  0.005714285714285714
spaces  1.0
spaces .  0.2
, some  0.0050533408197641775
some written  0.024096385542168676
written languages  0.19230769230769232
languages like  0.02
like Chinese  0.07142857142857142
Chinese ,  0.2857142857142857
, Japanese  0.0011229646266142617
Japanese  1.0
Japanese and  0.25
and Thai  0.001445086705202312
Thai  1.0
Thai do  0.5
not mark  0.008928571428571428
mark  1.0
mark word  0.3333333333333333
word boundaries  0.016666666666666666
boundaries in  0.09090909090909091
in such  0.0056179775280898875
a fashion  0.001226993865030675
fashion  1.0
fashion ,  1.0
in those  0.0018726591760299626
those languages  0.09090909090909091
languages text  0.02
text segmentation  0.0440251572327044
a significant  0.001226993865030675
significant task  0.1111111111111111
task requiring  0.023809523809523808
requiring knowledge  0.5
knowledge of  0.14814814814814814
the vocabulary  0.0006920415224913495
vocabulary  1.0
vocabulary and  0.25
and morphology  0.001445086705202312
morphology of  0.14285714285714285
words in  0.09174311926605505
Word sense  0.2857142857142857
sense  1.0
sense disambiguation  0.25
disambiguation :  0.1
: Many  0.00980392156862745
Many  0.08333333333333333
words have  0.009174311926605505
than one  0.06666666666666667
one meaning  0.03076923076923077
meaning ;  0.043478260869565216
; we  0.02127659574468085
we  1.0
we have  0.06666666666666667
have to  0.019230769230769232
to select  0.005312084993359893
select  1.0
select the  0.3333333333333333
meaning which  0.043478260869565216
which makes  0.021739130434782608
makes the  0.25
most sense  0.017241379310344827
sense in  0.125
in context  0.00749063670411985
context  1.0
context .  0.21212121212121213
For this  0.04918032786885246
this problem  0.07692307692307693
problem ,  0.09090909090909091
, we  0.009545199326221224
we are  0.044444444444444446
are typically  0.012448132780082987
typically given  0.05555555555555555
given a  0.16666666666666666
words and  0.06422018348623854
and associated  0.001445086705202312
associated  1.0
associated word  0.25
word senses  0.016666666666666666
senses  1.0
senses ,  0.5
, e.g.  0.005614823133071308
e.g. from  0.017857142857142856
a dictionary  0.0036809815950920245
dictionary or  0.14285714285714285
or from  0.0045045045045045045
from an  0.009615384615384616
an online  0.007575757575757576
online resource  0.125
resource  1.0
resource such  0.2
as WordNet  0.003484320557491289
WordNet  1.0
WordNet .  0.5
In some  0.0380952380952381
some cases  0.04819277108433735
, sets  0.0005614823133071309
tasks are  0.125
are grouped  0.004149377593360996
grouped into  0.5
into subfields  0.01282051282051282
subfields  1.0
subfields of  1.0
NLP that  0.02127659574468085
often considered  0.022727272727272728
considered separately  0.1111111111111111
separately  1.0
separately from  1.0
from NLP  0.009615384615384616
NLP as  0.02127659574468085
a whole  0.00245398773006135
whole  1.0
whole .  0.1111111111111111
Examples include  0.3333333333333333
include  1.0
include :  0.1111111111111111
: Information  0.00980392156862745
Information  0.8
Information retrieval  0.2
retrieval  1.0
retrieval -LRB-  0.14285714285714285
-LRB- IR  0.0027100271002710027
IR  1.0
IR -RRB-  0.3333333333333333
is concerned  0.0040650406504065045
with storing  0.00546448087431694
storing  1.0
storing ,  1.0
, searching  0.0005614823133071309
searching  1.0
searching and  0.3333333333333333
and retrieving  0.001445086705202312
retrieving  1.0
retrieving information  1.0
information .  0.08695652173913043
a separate  0.00245398773006135
separate field  0.1
field within  0.037037037037037035
within computer  0.05555555555555555
science -LRB-  0.1
-LRB- closer  0.0027100271002710027
closer  1.0
closer to  1.0
to databases  0.0013280212483399733
databases -RRB-  0.125
but IR  0.014705882352941176
IR relies  0.3333333333333333
relies  1.0
relies on  1.0
on some  0.04245283018867924
some NLP  0.012048192771084338
NLP methods  0.02127659574468085
methods -LRB-  0.045454545454545456
-LRB- for  0.018970189701897018
, stemming  0.0005614823133071309
stemming  1.0
stemming -RRB-  0.5
Some current  0.09523809523809523
current  1.0
current research  0.14285714285714285
research and  0.11904761904761904
and applications  0.001445086705202312
applications seek  0.04
seek  1.0
seek to  1.0
to bridge  0.0013280212483399733
bridge  1.0
bridge the  1.0
the gap  0.0006920415224913495
gap  1.0
gap between  1.0
between IR  0.02564102564102564
IR and  0.3333333333333333
and NLP  0.002890173410404624
Information extraction  0.2
extraction -LRB-  0.06451612903225806
-LRB- IE  0.005420054200542005
IE  1.0
IE -RRB-  0.6666666666666666
concerned in  0.2
in general  0.009363295880149813
general with  0.045454545454545456
the extraction  0.002768166089965398
extraction of  0.0967741935483871
of semantic  0.004456327985739751
semantic information  0.09523809523809523
from text  0.019230769230769232
This covers  0.031746031746031744
covers  1.0
covers tasks  0.25
tasks such  0.0625
as named  0.003484320557491289
recognition ,  0.11570247933884298
, coreference  0.0005614823133071309
coreference  1.0
coreference resolution  1.0
resolution ,  0.25
, relationship  0.0005614823133071309
relationship extraction  0.3333333333333333
extraction ,  0.1935483870967742
etc. .  0.4090909090909091
Speech processing  0.03225806451612903
processing :  0.018518518518518517
covers speech  0.25
, text-to-speech  0.0011229646266142617
text-to-speech  1.0
text-to-speech and  0.5
and related  0.004335260115606936
Other tasks  0.14285714285714285
tasks include  0.03125
: Stemming  0.00980392156862745
Stemming  1.0
Stemming Text  1.0
Text  0.6666666666666666
Text simplification  0.16666666666666666
simplification  1.0
simplification Text-to-speech  1.0
Text-to-speech  1.0
Text-to-speech Text-proofing  1.0
Text-proofing  1.0
Text-proofing Natural  1.0
Natural  0.46153846153846156
language search  0.006756756756756757
search  1.0
search Query  0.09090909090909091
Query  1.0
Query expansion  1.0
expansion  1.0
expansion Automated  0.3333333333333333
Automated  1.0
Automated essay  0.5
essay  1.0
essay scoring  1.0
scoring  1.0
scoring Truecasing  0.5
Truecasing  1.0
Truecasing Statistical  1.0
Statistical  0.6666666666666666
Statistical NLP  0.2222222222222222
NLP Main  0.02127659574468085
Main  0.9166666666666666
Main article  1.0
article :  0.4482758620689655
: statistical  0.00980392156862745
statistical natural  0.030303030303030304
processing Statistical  0.018518518518518517
Statistical natural-language  0.1111111111111111
natural-language  1.0
natural-language processing  1.0
processing uses  0.018518518518518517
uses  1.0
uses stochastic  0.07142857142857142
stochastic  1.0
stochastic ,  0.25
probabilistic and  0.14285714285714285
and statistical  0.004335260115606936
statistical methods  0.12121212121212122
methods to  0.09090909090909091
to resolve  0.00398406374501992
resolve  1.0
resolve some  0.25
the difficulties  0.0006920415224913495
difficulties  1.0
difficulties discussed  0.5
discussed  1.0
discussed above  0.14285714285714285
especially those  0.2
those which  0.045454545454545456
which arise  0.007246376811594203
arise  1.0
arise because  1.0
because longer  0.03333333333333333
longer  1.0
longer sentences  1.0
sentences are  0.09210526315789473
are highly  0.004149377593360996
highly  1.0
highly ambiguous  0.1111111111111111
ambiguous when  0.08333333333333333
when processed  0.02857142857142857
processed with  0.3333333333333333
with realistic  0.00546448087431694
realistic  1.0
realistic grammars  1.0
grammars  1.0
grammars ,  0.14285714285714285
, yielding  0.0005614823133071309
yielding  1.0
yielding thousands  1.0
thousands or  0.3333333333333333
or millions  0.0045045045045045045
millions  1.0
millions of  1.0
Methods for  0.5
for disambiguation  0.0036101083032490976
disambiguation often  0.1
often involve  0.022727272727272728
involve  1.0
involve the  0.16666666666666666
the use  0.010380622837370242
of corpora  0.00089126559714795
corpora and  0.09090909090909091
and Markov  0.001445086705202312
Markov  0.9444444444444444
Markov models  0.3333333333333333
NLP comprises  0.02127659574468085
comprises  1.0
comprises all  1.0
all quantitative  0.023255813953488372
quantitative  1.0
quantitative approaches  0.25
to automated  0.0013280212483399733
automated language  0.14285714285714285
processing ,  0.16666666666666666
including probabilistic  0.07142857142857142
probabilistic modeling  0.14285714285714285
modeling  1.0
modeling ,  0.14285714285714285
, information  0.0011229646266142617
information theory  0.021739130434782608
and linear  0.001445086705202312
linear  1.0
linear algebra  0.14285714285714285
algebra  1.0
algebra .  0.5
The technology  0.005208333333333333
technology  1.0
technology for  0.09090909090909091
for statistical  0.007220216606498195
NLP comes  0.02127659574468085
comes  1.0
comes mainly  0.2
mainly  1.0
mainly from  0.16666666666666666
from machine  0.019230769230769232
learning and  0.023255813953488372
and data  0.005780346820809248
data mining  0.025974025974025976
mining  1.0
mining ,  0.2
, both  0.0016844469399213925
both of  0.06451612903225806
are fields  0.004149377593360996
fields of  0.3333333333333333
of artificial  0.00089126559714795
intelligence that  0.25
that involve  0.0035460992907801418
involve learning  0.16666666666666666
Evaluation of  0.1111111111111111
processing Objectives  0.018518518518518517
Objectives  1.0
Objectives The  1.0
The goal  0.005208333333333333
goal  1.0
goal of  0.2857142857142857
NLP evaluation  0.06382978723404255
evaluation is  0.07407407407407407
is to  0.03861788617886179
to measure  0.005312084993359893
measure  1.0
measure one  0.09090909090909091
one or  0.03076923076923077
more qualities  0.010526315789473684
qualities  1.0
qualities of  0.5
an algorithm  0.022727272727272728
algorithm or  0.03571428571428571
or a  0.08558558558558559
a system  0.012269938650306749
system ,  0.10752688172043011
determine whether  0.043478260869565216
whether -LRB-  0.07692307692307693
or to  0.009009009009009009
to what  0.005312084993359893
what extent  0.03125
extent  1.0
extent -RRB-  0.25
-RRB- the  0.0028169014084507044
the system  0.01384083044982699
system answers  0.010752688172043012
answers the  0.08333333333333333
the goals  0.0006920415224913495
goals  1.0
goals of  1.0
of its  0.0071301247771836
its designers  0.02857142857142857
designers  1.0
designers ,  1.0
or meets  0.0045045045045045045
meets the  0.5
the needs  0.0006920415224913495
needs  1.0
needs of  0.1
its users  0.02857142857142857
users  1.0
users .  0.2222222222222222
Research in  0.125
evaluation has  0.018518518518518517
has received  0.011904761904761904
received  1.0
received considerable  0.5
considerable  1.0
considerable attention  0.2
attention  1.0
attention ,  0.5
, because  0.004491858506457047
because the  0.13333333333333333
the definition  0.0020761245674740486
definition  1.0
definition of  0.6
of proper  0.00089126559714795
proper evaluation  0.14285714285714285
evaluation criteria  0.037037037037037035
criteria  1.0
criteria is  0.25
one way  0.015384615384615385
way  1.0
way to  0.4166666666666667
to specify  0.0013280212483399733
specify  1.0
specify precisely  1.0
precisely  1.0
precisely an  1.0
an NLP  0.022727272727272728
NLP problem  0.0425531914893617
, going  0.0005614823133071309
going  1.0
going thus  0.25
thus beyond  0.1
beyond the  0.5
the vagueness  0.0006920415224913495
vagueness  1.0
vagueness of  1.0
of tasks  0.00089126559714795
tasks defined  0.03125
defined  1.0
defined only  0.16666666666666666
only as  0.02631578947368421
as language  0.003484320557491289
understanding or  0.030303030303030304
or language  0.0045045045045045045
generation .  0.2222222222222222
A precise  0.02
precise  1.0
precise set  0.3333333333333333
of evaluation  0.004456327985739751
criteria ,  0.25
which includes  0.014492753623188406
includes mainly  0.14285714285714285
mainly evaluation  0.16666666666666666
evaluation data  0.018518518518518517
data and  0.025974025974025976
and evaluation  0.004335260115606936
evaluation metrics  0.018518518518518517
metrics  0.8888888888888888
metrics ,  0.1111111111111111
, enables  0.0005614823133071309
enables  1.0
enables several  1.0
several teams  0.045454545454545456
teams  1.0
teams to  0.5
to compare  0.005312084993359893
compare  1.0
compare their  0.14285714285714285
their  1.0
their solutions  0.029411764705882353
solutions to  0.5
given NLP  0.041666666666666664
Short history  1.0
history  1.0
history of  0.5
evaluation in  0.05555555555555555
first evaluation  0.030303030303030304
evaluation campaign  0.018518518518518517
campaign  1.0
campaign on  0.2
on written  0.0047169811320754715
written texts  0.07692307692307693
texts seems  0.058823529411764705
a campaign  0.001226993865030675
campaign dedicated  0.2
dedicated  1.0
dedicated to  0.6666666666666666
to message  0.0013280212483399733
message  1.0
message understanding  0.5
understanding in  0.030303030303030304
in 1987  0.003745318352059925
1987  1.0
1987 -LRB-  0.3333333333333333
-LRB- Pallet  0.0027100271002710027
Pallet  1.0
Pallet 1998  0.5
1998  1.0
1998 -RRB-  0.5
Then ,  0.4
the Parseval\/GEIG  0.0006920415224913495
Parseval\/GEIG  1.0
Parseval\/GEIG project  1.0
project  1.0
project compared  0.15384615384615385
compared  1.0
compared phrase-structure  0.14285714285714285
phrase-structure  1.0
phrase-structure grammars  1.0
grammars -LRB-  0.07142857142857142
-LRB- Black  0.0027100271002710027
Black  1.0
Black 1991  0.5
1991  1.0
1991 -RRB-  0.6666666666666666
A series  0.02
series  1.0
series of  0.875
of campaigns  0.00089126559714795
campaigns  1.0
campaigns within  0.5
within Tipster  0.05555555555555555
Tipster  1.0
Tipster project  1.0
project were  0.07692307692307693
were realized  0.024390243902439025
realized  1.0
realized on  1.0
on tasks  0.009433962264150943
tasks like  0.0625
like summarization  0.03571428571428571
summarization ,  0.08
, translation  0.0011229646266142617
translation and  0.04054054054054054
and searching  0.001445086705202312
searching -LRB-  0.3333333333333333
-LRB- Hirschman  0.0027100271002710027
Hirschman  1.0
Hirschman 1998  0.5
In 1994  0.009523809523809525
1994  1.0
1994 ,  1.0
in Germany  0.003745318352059925
Germany  1.0
Germany ,  0.5
the Morpholympics  0.0006920415224913495
Morpholympics  1.0
Morpholympics compared  1.0
compared German  0.14285714285714285
German taggers  0.25
taggers  1.0
taggers .  0.14285714285714285
the Senseval  0.0006920415224913495
Senseval  1.0
Senseval and  1.0
and Romanseval  0.001445086705202312
Romanseval  1.0
Romanseval campaigns  1.0
campaigns were  0.5
were conducted  0.024390243902439025
conducted with  0.2
the objectives  0.0006920415224913495
objectives of  0.5
semantic disambiguation  0.047619047619047616
disambiguation .  0.1
In 1996  0.009523809523809525
1996  1.0
1996 ,  1.0
the Sparkle  0.0006920415224913495
Sparkle  1.0
Sparkle campaign  1.0
campaign compared  0.2
compared syntactic  0.14285714285714285
syntactic  1.0
syntactic parsers  0.07692307692307693
parsers  1.0
parsers in  0.07692307692307693
in four  0.003745318352059925
four  1.0
four different  0.2857142857142857
different languages  0.02040816326530612
languages -LRB-  0.04
-LRB- English  0.0027100271002710027
, French  0.0005614823133071309
French ,  0.125
German and  0.25
and Italian  0.001445086705202312
Italian  1.0
Italian -RRB-  0.5
In France  0.01904761904761905
France  1.0
France ,  0.5
the Grace  0.0006920415224913495
Grace  1.0
Grace project  1.0
compared a  0.14285714285714285
of 21  0.00089126559714795
21  1.0
21 taggers  1.0
taggers for  0.14285714285714285
for French  0.010830324909747292
French in  0.125
in 1997  0.003745318352059925
1997  1.0
1997 -LRB-  0.5
-LRB- Adda  0.0027100271002710027
Adda  0.5
Adda 1999  0.5
1999  1.0
1999 -RRB-  0.5
In 2004  0.009523809523809525
2004  1.0
2004 ,  0.3333333333333333
, during  0.0005614823133071309
during the  0.4
the Technolangue\/Easy  0.0006920415224913495
Technolangue\/Easy  1.0
Technolangue\/Easy project  0.5
project ,  0.38461538461538464
, 13  0.0005614823133071309
13  1.0
13 parsers  0.5
parsers for  0.15384615384615385
French were  0.25
were compared  0.04878048780487805
compared .  0.2857142857142857
Large-scale evaluation  1.0
evaluation of  0.07407407407407407
of dependency  0.00089126559714795
dependency  1.0
dependency parsers  0.2
parsers were  0.07692307692307693
were performed  0.024390243902439025
performed  1.0
performed in  0.2
the context  0.004152249134948097
context of  0.15151515151515152
the CoNLL  0.0006920415224913495
CoNLL  1.0
CoNLL shared  1.0
shared  1.0
shared tasks  0.5
in 2006  0.0018726591760299626
2006  1.0
2006 and  0.3333333333333333
and 2007  0.001445086705202312
2007  1.0
2007 .  0.4
In Italy  0.009523809523809525
Italy  1.0
Italy ,  1.0
the EVALITA  0.0006920415224913495
EVALITA  1.0
EVALITA campaign  0.5
campaign was  0.2
conducted in  0.4
in 2007  0.0018726591760299626
2007 and  0.2
and 2009  0.001445086705202312
2009  1.0
2009 to  0.3333333333333333
compare various  0.14285714285714285
various NLP  0.05555555555555555
NLP and  0.02127659574468085
and speech  0.001445086705202312
speech tools  0.006578947368421052
tools  1.0
tools for  0.16666666666666666
for Italian  0.0036101083032490976
Italian ;  0.5
; the  0.0851063829787234
the 2011  0.0006920415224913495
2011  1.0
2011 campaign  0.5
campaign is  0.2
is in  0.006097560975609756
in full  0.0018726591760299626
full  1.0
full progress  0.2
progress -  0.14285714285714285
-  1.0
- EVALITA  0.0625
EVALITA web  0.5
web site  0.25
site  1.0
site .  1.0
, within  0.0005614823133071309
the ANR-Passage  0.0006920415224913495
ANR-Passage  1.0
ANR-Passage project  1.0
project -LRB-  0.07692307692307693
-LRB- end  0.0027100271002710027
end  1.0
end of  0.25
of 2007  0.0017825311942959
2007 -RRB-  0.2
, 10  0.0011229646266142617
10  1.0
10 parsers  0.125
compared -  0.14285714285714285
- passage  0.0625
passage  1.0
passage web  1.0
Adda G.  0.5
G.  1.0
G. ,  0.5
, Mariani  0.0005614823133071309
Mariani  1.0
Mariani J.  1.0
J.  1.0
J. ,  0.6666666666666666
, Paroubek  0.0005614823133071309
Paroubek  1.0
Paroubek P.  1.0
P.  1.0
P. ,  1.0
, Rajman  0.0005614823133071309
Rajman  1.0
Rajman M.  1.0
M.  1.0
M. 1999  0.25
1999 L'action  0.5
L'action  1.0
L'action GRACE  1.0
GRACE  1.0
GRACE d'valuation  1.0
d'valuation  1.0
d'valuation de  1.0
de  1.0
de l'assignation  0.5
l'assignation  1.0
l'assignation des  1.0
des  1.0
des parties  1.0
parties  1.0
parties du  1.0
du  1.0
du discors  1.0
discors  1.0
discors pour  1.0
pour  1.0
pour le  1.0
le  1.0
le franais  1.0
franais  1.0
franais .  1.0
Langues vol-2  1.0
vol-2  1.0
vol-2 Black  1.0
Black E.  0.5
E.  0.75
E. ,  0.25
, Abney  0.0005614823133071309
Abney  1.0
Abney S.  1.0
S.  1.0
S. ,  1.0
, Flickinger  0.0005614823133071309
Flickinger  1.0
Flickinger D.  1.0
D.  1.0
D. ,  0.4
, Gdaniec  0.0005614823133071309
Gdaniec  1.0
Gdaniec C.  1.0
C.  1.0
C. ,  1.0
, Grishman  0.0005614823133071309
Grishman  1.0
Grishman R.  1.0
R.  1.0
R. ,  0.3333333333333333
, Harrison  0.0005614823133071309
Harrison  1.0
Harrison P.  1.0
, Hindle  0.0005614823133071309
Hindle  1.0
Hindle D.  1.0
, Ingria  0.0005614823133071309
Ingria  1.0
Ingria R.  1.0
, Jelinek  0.0005614823133071309
Jelinek  1.0
Jelinek F.  0.5
F.  1.0
F. ,  1.0
, Klavans  0.0005614823133071309
Klavans  1.0
Klavans J.  1.0
, Liberman  0.0005614823133071309
Liberman  1.0
Liberman M.  1.0
M. ,  0.5
, Marcus  0.0005614823133071309
Marcus  1.0
Marcus M.  1.0
, Reukos  0.0005614823133071309
Reukos  1.0
Reukos S.  1.0
, Santoni  0.0005614823133071309
Santoni  1.0
Santoni B.  1.0
B.  1.0
B. ,  1.0
, Strzalkowski  0.0005614823133071309
Strzalkowski  1.0
Strzalkowski T.  1.0
T.  1.0
T. 1991  1.0
1991 A  0.3333333333333333
A  0.12
A procedure  0.02
procedure  1.0
procedure for  0.3333333333333333
for quantitatively  0.0036101083032490976
quantitatively  1.0
quantitatively comparing  1.0
comparing  1.0
comparing the  0.5
the syntactic  0.0006920415224913495
syntactic coverage  0.07692307692307693
coverage  1.0
coverage of  0.3333333333333333
of English  0.00267379679144385
English grammars  0.02702702702702703
grammars .  0.14285714285714285
DARPA Speech  0.25
Speech  0.5161290322580645
Speech and  0.16129032258064516
and Natural  0.001445086705202312
Natural Language  0.23076923076923078
Language  0.9166666666666666
Language Workshop  0.08333333333333333
Workshop  1.0
Workshop Hirschman  1.0
Hirschman L.  0.5
L.  1.0
L. 1998  1.0
1998 Language  0.25
Language understanding  0.08333333333333333
understanding evaluation  0.030303030303030304
evaluation :  0.037037037037037035
: lessons  0.00980392156862745
lessons  1.0
lessons learned  1.0
learned from  0.2
from MUC  0.009615384615384616
MUC  1.0
MUC and  1.0
and ATIS  0.001445086705202312
ATIS  1.0
ATIS .  1.0
LREC Granada  1.0
Granada  1.0
Granada Pallet  0.5
Pallet D.S.  0.5
D.S.  1.0
D.S. 1998  1.0
1998 The  0.25
The NIST  0.005208333333333333
NIST  1.0
NIST role  0.5
role  1.0
role in  0.25
in automatic  0.003745318352059925
automatic speech  0.13043478260869565
recognition benchmark  0.008264462809917356
benchmark  1.0
benchmark tests  1.0
tests  1.0
tests .  0.25
Granada Different  0.5
Different  1.0
Different types  1.0
evaluation Depending  0.018518518518518517
Depending  1.0
Depending on  1.0
evaluation procedures  0.018518518518518517
procedures ,  0.25
of distinctions  0.00089126559714795
distinctions  1.0
distinctions are  0.5
are traditionally  0.008298755186721992
traditionally  1.0
traditionally made  0.5
made in  0.125
evaluation .  0.05555555555555555
Intrinsic vs.  0.3333333333333333
vs. extrinsic  0.08333333333333333
extrinsic  1.0
extrinsic evaluation  0.5
evaluation Intrinsic  0.018518518518518517
Intrinsic  0.3333333333333333
Intrinsic evaluation  0.3333333333333333
evaluation considers  0.018518518518518517
considers  1.0
considers an  0.5
an isolated  0.007575757575757576
isolated  1.0
isolated NLP  0.2
NLP system  0.0851063829787234
system and  0.03225806451612903
and characterizes  0.001445086705202312
characterizes  1.0
characterizes its  1.0
its performance  0.02857142857142857
performance  1.0
performance mainly  0.05555555555555555
mainly with  0.16666666666666666
with respect  0.03825136612021858
respect  1.0
respect to  1.0
a gold  0.00245398773006135
gold  1.0
gold standard  0.8333333333333334
standard result  0.07142857142857142
, pre-defined  0.0005614823133071309
pre-defined  1.0
pre-defined by  0.5
the evaluators  0.0006920415224913495
evaluators  1.0
evaluators .  1.0
Extrinsic evaluation  0.5
evaluation ,  0.05555555555555555
, also  0.002807411566535654
called evaluation  0.1111111111111111
in use  0.003745318352059925
use considers  0.013888888888888888
considers the  0.5
the NLP  0.0006920415224913495
system in  0.021505376344086023
a more  0.0049079754601227
more complex  0.08421052631578947
complex setting  0.041666666666666664
, either  0.0011229646266142617
either  1.0
either as  0.3
an embedded  0.007575757575757576
embedded  1.0
embedded system  0.25
system or  0.021505376344086023
or serving  0.0045045045045045045
serving  1.0
serving a  1.0
a precise  0.001226993865030675
precise function  0.3333333333333333
function for  0.125
human user  0.043478260869565216
user  1.0
user .  0.21428571428571427
The extrinsic  0.005208333333333333
extrinsic performance  0.16666666666666666
performance of  0.1111111111111111
system is  0.0967741935483871
is then  0.01016260162601626
then characterized  0.02857142857142857
characterized  1.0
characterized in  0.25
in terms  0.011235955056179775
terms  1.0
terms of  0.5384615384615384
its utility  0.02857142857142857
utility  1.0
utility with  0.5
the overall  0.0020761245674740486
overall  1.0
overall task  0.16666666666666666
the complex  0.0006920415224913495
complex system  0.08333333333333333
or the  0.04054054054054054
the human  0.0020761245674740486
, consider  0.0005614823133071309
consider  0.75
consider a  0.25
a syntactic  0.001226993865030675
syntactic parser  0.07692307692307693
parser  1.0
parser that  0.0625
is based  0.008130081300813009
the output  0.0020761245674740486
output of  0.15384615384615385
some new  0.012048192771084338
new part  0.041666666666666664
speech -LRB-  0.02631578947368421
-LRB- POS  0.005420054200542005
POS  0.9230769230769231
POS -RRB-  0.07692307692307693
-RRB- tagger  0.0028169014084507044
tagger .  0.1111111111111111
An intrinsic  0.125
intrinsic  1.0
intrinsic evaluation  0.5
evaluation would  0.05555555555555555
would run  0.03773584905660377
run  1.0
run the  0.4
the POS  0.0020761245674740486
POS tagger  0.3076923076923077
tagger on  0.1111111111111111
some labeled  0.012048192771084338
labeled  1.0
labeled data  0.3333333333333333
and compare  0.002890173410404624
compare the  0.2857142857142857
system output  0.010752688172043012
tagger to  0.1111111111111111
the gold  0.0020761245674740486
standard -LRB-  0.14285714285714285
-LRB- correct  0.0027100271002710027
correct -RRB-  0.06666666666666667
-RRB- output  0.0028169014084507044
An extrinsic  0.0625
the parser  0.002768166089965398
parser with  0.0625
with some  0.02185792349726776
other POS  0.014285714285714285
and then  0.010115606936416185
then with  0.02857142857142857
the new  0.0006920415224913495
new POS  0.041666666666666664
the parsing  0.001384083044982699
parsing  1.0
parsing accuracy  0.03571428571428571
accuracy .  0.22580645161290322
Black-box vs.  0.5
vs. glass-box  0.08333333333333333
glass-box  1.0
glass-box evaluation  1.0
evaluation Black-box  0.018518518518518517
Black-box  0.5
Black-box evaluation  0.5
evaluation requires  0.018518518518518517
requires one  0.0625
one to  0.03076923076923077
to run  0.0013280212483399733
run an  0.2
system on  0.010752688172043012
given data  0.041666666666666664
data set  0.012987012987012988
set and  0.02564102564102564
measure a  0.09090909090909091
of parameters  0.00089126559714795
parameters  1.0
parameters related  0.25
related to  0.26666666666666666
the quality  0.0034602076124567475
quality  1.0
quality of  0.5
process -LRB-  0.027777777777777776
-LRB- speed  0.0027100271002710027
speed  1.0
speed ,  0.2857142857142857
, reliability  0.0005614823133071309
reliability  1.0
reliability ,  0.5
, resource  0.0005614823133071309
resource consumption  0.2
consumption  1.0
consumption -RRB-  1.0
and ,  0.004335260115606936
most importantly  0.017241379310344827
importantly  1.0
importantly ,  1.0
, to  0.0072992700729927005
the result  0.002768166089965398
result -LRB-  0.09090909090909091
e.g. the  0.05357142857142857
the accuracy  0.0006920415224913495
accuracy of  0.12903225806451613
data annotation  0.012987012987012988
annotation or  0.25
the fidelity  0.0006920415224913495
fidelity  1.0
fidelity of  1.0
a translation  0.00245398773006135
translation -RRB-  0.02702702702702703
Glass-box evaluation  1.0
evaluation looks  0.018518518518518517
looks  1.0
looks at  0.25
at the  0.22058823529411764
design of  0.25
the algorithms  0.001384083044982699
algorithms that  0.05714285714285714
are implemented  0.004149377593360996
implemented ,  0.2
the linguistic  0.0006920415224913495
linguistic  1.0
linguistic resources  0.0625
resources  1.0
resources it  0.16666666666666666
it uses  0.017094017094017096
uses -LRB-  0.07142857142857142
e.g. vocabulary  0.017857142857142856
vocabulary size  0.125
size  1.0
size -RRB-  0.16666666666666666
Given the  0.07142857142857142
NLP problems  0.0425531914893617
often difficult  0.022727272727272728
difficult to  0.39285714285714285
to predict  0.0026560424966799467
predict  1.0
predict performance  0.16666666666666666
performance only  0.05555555555555555
only on  0.05263157894736842
of glass-box  0.00089126559714795
but this  0.058823529411764705
this type  0.03296703296703297
is more  0.006097560975609756
more informative  0.010526315789473684
informative  1.0
informative with  0.5
to error  0.0013280212483399733
error  1.0
error analysis  0.08333333333333333
analysis or  0.046153846153846156
or future  0.0045045045045045045
future  1.0
future developments  0.3333333333333333
developments  1.0
developments of  0.3333333333333333
Automatic vs.  0.1111111111111111
vs. manual  0.08333333333333333
manual  1.0
manual evaluation  0.5
evaluation In  0.018518518518518517
In  0.0761904761904762
In many  0.01904761904761905
many cases  0.038461538461538464
, automatic  0.0016844469399213925
automatic procedures  0.043478260869565216
be defined  0.004219409282700422
defined to  0.16666666666666666
to evaluate  0.005312084993359893
evaluate  1.0
evaluate an  0.25
system by  0.010752688172043012
by comparing  0.005714285714285714
comparing its  0.5
its output  0.08571428571428572
output with  0.038461538461538464
or desired  0.0045045045045045045
desired -RRB-  0.2
-RRB- one  0.0028169014084507044
one .  0.03076923076923077
Although the  0.375
the cost  0.0006920415224913495
cost  1.0
cost of  0.5
of producing  0.00089126559714795
producing the  0.3333333333333333
standard can  0.07142857142857142
be quite  0.004219409282700422
quite  1.0
quite high  0.125
high ,  0.05555555555555555
automatic evaluation  0.13043478260869565
evaluation can  0.037037037037037035
be repeated  0.004219409282700422
repeated  1.0
repeated as  0.5
as often  0.003484320557491289
often as  0.045454545454545456
as needed  0.003484320557491289
needed  1.0
needed without  0.047619047619047616
without much  0.07692307692307693
much additional  0.045454545454545456
additional  1.0
additional costs  0.16666666666666666
costs  1.0
costs -LRB-  1.0
-LRB- on  0.005420054200542005
same input  0.08
for many  0.007220216606498195
many NLP  0.019230769230769232
standard is  0.07142857142857142
a complex  0.006134969325153374
complex task  0.041666666666666664
and can  0.011560693641618497
can prove  0.0055248618784530384
prove  1.0
prove impossible  1.0
impossible  1.0
impossible when  0.5
when inter-annotator  0.02857142857142857
inter-annotator  1.0
inter-annotator agreement  1.0
agreement  1.0
agreement is  0.3333333333333333
is insufficient  0.0020325203252032522
Manual evaluation  0.6666666666666666
is performed  0.0040650406504065045
performed by  0.2
by human  0.017142857142857144
human judges  0.043478260869565216
judges  1.0
judges ,  0.5
are instructed  0.004149377593360996
instructed  1.0
instructed to  1.0
to estimate  0.00398406374501992
estimate  1.0
estimate the  0.5
or most  0.0045045045045045045
most often  0.017241379310344827
often of  0.022727272727272728
a sample  0.001226993865030675
sample  1.0
sample of  0.3333333333333333
output ,  0.038461538461538464
, based  0.0011229646266142617
of criteria  0.00089126559714795
criteria .  0.25
Although ,  0.125
, thanks  0.0005614823133071309
thanks  1.0
thanks to  1.0
to their  0.0026560424966799467
their linguistic  0.029411764705882353
linguistic competence  0.0625
competence  1.0
competence ,  1.0
, human  0.003368893879842785
judges can  0.5
be considered  0.008438818565400843
considered as  0.1111111111111111
the reference  0.0006920415224913495
reference  1.0
reference for  0.125
processing tasks  0.018518518518518517
tasks ,  0.125
also considerable  0.014492753623188406
considerable variation  0.2
variation  1.0
variation across  1.0
across  1.0
across their  0.2
their ratings  0.029411764705882353
ratings  1.0
ratings .  0.1111111111111111
is why  0.0020325203252032522
why  1.0
why automatic  0.14285714285714285
as objective  0.003484320557491289
objective evaluation  0.2
while the  0.05
human kind  0.021739130434782608
kind  1.0
kind appears  0.09090909090909091
appears  1.0
appears to  0.2
be more  0.02109704641350211
more subjective  0.010526315789473684
subjective .  0.3333333333333333
Shared tasks  1.0
tasks -LRB-  0.03125
-LRB- Campaigns  0.0027100271002710027
Campaigns  1.0
Campaigns -RRB-  1.0
-RRB- BioCreative  0.0028169014084507044
BioCreative  1.0
BioCreative Message  1.0
Message  1.0
Message Understanding  1.0
Understanding  1.0
Understanding Conference  0.5
Conference  1.0
Conference Technolangue\/Easy  0.5
Technolangue\/Easy Text  0.5
Text Retrieval  0.16666666666666666
Retrieval  1.0
Retrieval Conference  1.0
Conference Evaluation  0.5
Evaluation  0.4444444444444444
Evaluation exercises  0.1111111111111111
exercises  1.0
exercises on  1.0
on Semantic  0.0047169811320754715
Semantic  0.6666666666666666
Semantic Evaluation  0.3333333333333333
Evaluation -LRB-  0.1111111111111111
-LRB- SemEval  0.0027100271002710027
SemEval  1.0
SemEval -RRB-  1.0
-RRB- MorphoChallenge  0.0028169014084507044
MorphoChallenge  1.0
MorphoChallenge Semi-supervised  1.0
Semi-supervised  1.0
Semi-supervised and  1.0
and Unsupervised  0.001445086705202312
Unsupervised  0.16666666666666666
Unsupervised Morpheme  0.16666666666666666
Morpheme  1.0
Morpheme Analysis  1.0
Analysis  1.0
Analysis Standardization  0.2
Standardization  1.0
Standardization in  1.0
NLP An  0.02127659574468085
An ISO  0.0625
ISO  1.0
ISO sub-committee  0.5
sub-committee  1.0
sub-committee is  1.0
is working  0.0040650406504065045
to ease  0.0013280212483399733
ease  1.0
ease interoperability  1.0
interoperability  1.0
interoperability between  1.0
between lexical  0.05128205128205128
lexical  1.0
lexical resources  0.07692307692307693
resources and  0.16666666666666666
NLP programs  0.02127659574468085
programs .  0.2727272727272727
The sub-committee  0.005208333333333333
is part  0.0020325203252032522
of ISO\/TC37  0.00089126559714795
ISO\/TC37  1.0
ISO\/TC37 and  1.0
is called  0.012195121951219513
called ISO\/TC37\/SC4  0.05555555555555555
ISO\/TC37\/SC4  1.0
ISO\/TC37\/SC4 .  1.0
Some ISO  0.047619047619047616
ISO standards  0.5
standards  1.0
standards are  0.2
are already  0.004149377593360996
already published  0.2
published but  0.14285714285714285
but most  0.029411764705882353
of them  0.0017825311942959
them are  0.10526315789473684
are under  0.004149377593360996
under  1.0
under construction  0.2
construction ,  0.3333333333333333
, mainly  0.0005614823133071309
mainly on  0.16666666666666666
on lexicon  0.0047169811320754715
lexicon  1.0
lexicon representation  0.1111111111111111
representation -LRB-  0.10526315789473684
see LMF  0.05
LMF  1.0
LMF -RRB-  1.0
, annotation  0.0005614823133071309
annotation and  0.25
data category  0.012987012987012988
category  1.0
category registry  0.5
registry  1.0
registry .  1.0
summarization is  0.12
the creation  0.0006920415224913495
a shortened  0.001226993865030675
shortened  1.0
shortened version  1.0
version  1.0
version of  0.6666666666666666
a text  0.01717791411042945
text by  0.006289308176100629
by a  0.10285714285714286
program .  0.13636363636363635
The product  0.010416666666666666
product  1.0
product of  0.14285714285714285
this procedure  0.01098901098901099
procedure still  0.3333333333333333
still  1.0
still contains  0.06666666666666667
contains the  0.2
most important  0.034482758620689655
important points  0.0625
points  1.0
points of  0.5
the original  0.006920415224913495
original  1.0
original text  0.46153846153846156
analysis -LRB-  0.06153846153846154
-LRB- DA  0.005420054200542005
DA  1.0
DA -RRB-  0.6666666666666666
or discourse  0.009009009009009009
discourse studies  0.027777777777777776
studies  1.0
studies ,  0.5
, is  0.0072992700729927005
a general  0.0036809815950920245
general term  0.045454545454545456
term  1.0
term for  0.16666666666666666
of approaches  0.00089126559714795
to analyzing  0.0013280212483399733
analyzing  1.0
analyzing written  0.2
written ,  0.038461538461538464
, spoken  0.0005614823133071309
spoken ,  0.14285714285714285
, signed  0.0005614823133071309
signed  1.0
signed language  1.0
language use  0.02702702702702703
use or  0.027777777777777776
or any  0.013513513513513514
any significant  0.03225806451612903
significant semiotic  0.1111111111111111
semiotic  1.0
semiotic event  1.0
event  1.0
event .  0.3333333333333333
, sometimes  0.0005614823133071309
to by  0.0026560424966799467
the abbreviation  0.0006920415224913495
abbreviation  1.0
abbreviation MT  0.5
MT  1.0
MT -LRB-  0.2
-LRB- not  0.0027100271002710027
not to  0.008928571428571428
be confused  0.004219409282700422
confused  1.0
confused with  1.0
with computer-aided  0.00546448087431694
computer-aided  1.0
computer-aided translation  0.3333333333333333
, machine-aided  0.0005614823133071309
machine-aided  1.0
machine-aided human  1.0
human translation  0.043478260869565216
translation MAHT  0.013513513513513514
MAHT  1.0
MAHT and  1.0
and interactive  0.001445086705202312
interactive  1.0
interactive translation  0.25
a sub-field  0.001226993865030675
sub-field  1.0
sub-field of  1.0
that investigates  0.0035460992907801418
investigates  1.0
investigates the  1.0
of software  0.00089126559714795
software  1.0
software to  0.07407407407407407
to translate  0.00398406374501992
text or  0.018867924528301886
or speech  0.0045045045045045045
speech from  0.006578947368421052
one natural  0.03076923076923077
On a  0.16666666666666666
a basic  0.00245398773006135
basic  1.0
basic level  0.07692307692307693
level  1.0
level ,  0.2
, MT  0.0011229646266142617
MT performs  0.2
performs  1.0
performs simple  1.0
simple substitution  0.038461538461538464
substitution  1.0
substitution of  1.0
in one  0.00749063670411985
language for  0.02027027027027027
for words  0.0036101083032490976
in another  0.00749063670411985
another ,  0.07692307692307693
but that  0.04411764705882353
that alone  0.0035460992907801418
alone usually  0.25
usually can  0.03125
not produce  0.008928571428571428
produce a  0.13636363636363635
a good  0.0049079754601227
good  1.0
good translation  0.07692307692307693
because recognition  0.03333333333333333
recognition of  0.0743801652892562
of whole  0.0017825311942959
whole phrases  0.1111111111111111
phrases  1.0
phrases and  0.1875
and their  0.008670520231213872
their closest  0.029411764705882353
closest  1.0
closest counterparts  0.5
counterparts  1.0
counterparts in  1.0
the target  0.006228373702422145
target  1.0
target language  0.7272727272727273
language is  0.033783783783783786
is needed  0.0020325203252032522
needed .  0.09523809523809523
Solving this  0.5
problem with  0.022727272727272728
with corpus  0.00546448087431694
corpus and  0.03225806451612903
statistical techniques  0.06060606060606061
techniques  1.0
techniques is  0.043478260869565216
a rapidly  0.001226993865030675
rapidly  1.0
rapidly growing  0.5
growing  1.0
growing field  0.5
field that  0.07407407407407407
is leading  0.0020325203252032522
leading  1.0
leading to  1.0
to better  0.00398406374501992
better  1.0
better translations  0.1111111111111111
translations  1.0
translations ,  0.5
handling differences  0.5
differences  1.0
differences in  0.3333333333333333
in linguistic  0.003745318352059925
linguistic typology  0.0625
typology  1.0
typology ,  1.0
of idioms  0.00089126559714795
idioms  1.0
idioms ,  1.0
the isolation  0.0006920415224913495
isolation  1.0
isolation of  0.5
of anomalies  0.00089126559714795
anomalies  1.0
anomalies .  1.0
-LRB- citation  0.03523035230352303
citation  1.0
citation needed  1.0
needed -RRB-  0.6190476190476191
-RRB- Current  0.0028169014084507044
Current  0.4
Current machine  0.2
translation software  0.04054054054054054
software often  0.037037037037037035
often allows  0.022727272727272728
allows  1.0
allows for  0.125
for customisation  0.0036101083032490976
customisation  1.0
customisation by  1.0
by domain  0.005714285714285714
domain  1.0
domain or  0.05
or profession  0.0045045045045045045
profession  1.0
profession -LRB-  1.0
as weather  0.003484320557491289
weather  1.0
weather reports  0.2857142857142857
reports  1.0
reports -RRB-  0.4
, improving  0.0005614823133071309
improving  1.0
improving output  1.0
output by  0.038461538461538464
by limiting  0.005714285714285714
limiting  1.0
limiting the  1.0
the scope  0.001384083044982699
scope  1.0
scope of  1.0
of allowable  0.00089126559714795
allowable  1.0
allowable substitutions  0.5
substitutions  1.0
substitutions .  1.0
This technique  0.015873015873015872
technique  1.0
technique is  0.14285714285714285
is particularly  0.0040650406504065045
particularly effective  0.2
effective  1.0
effective in  0.3333333333333333
in domains  0.0018726591760299626
domains  1.0
domains where  0.125
where formal  0.02857142857142857
formal or  0.1111111111111111
or formulaic  0.0045045045045045045
formulaic  1.0
formulaic language  1.0
used .  0.04424778761061947
It follows  0.02631578947368421
follows  1.0
follows that  0.5
that machine  0.010638297872340425
government and  0.3333333333333333
and legal  0.001445086705202312
legal  1.0
legal documents  0.3333333333333333
documents more  0.02631578947368421
more readily  0.010526315789473684
readily produces  0.3333333333333333
produces usable  0.25
usable  1.0
usable output  1.0
output than  0.038461538461538464
than conversation  0.022222222222222223
conversation or  0.25
or less  0.018018018018018018
less standardised  0.08333333333333333
standardised  1.0
standardised text  1.0
Improved output  1.0
output quality  0.038461538461538464
quality can  0.1
can also  0.04419889502762431
be achieved  0.02109704641350211
achieved  1.0
achieved by  0.2
human intervention  0.021739130434782608
intervention  1.0
intervention :  1.0
: for  0.00980392156862745
some systems  0.024096385542168676
systems are  0.11607142857142858
translate more  0.16666666666666666
more accurately  0.010526315789473684
accurately  1.0
accurately if  0.5
if  1.0
if the  0.35714285714285715
the user  0.0034602076124567475
user has  0.07142857142857142
has unambiguously  0.011904761904761904
unambiguously  1.0
unambiguously identified  1.0
identified which  0.2
text are  0.006289308176100629
are names  0.004149377593360996
With the  0.2857142857142857
the assistance  0.0006920415224913495
assistance  1.0
assistance of  1.0
these techniques  0.023809523809523808
techniques ,  0.08695652173913043
MT has  0.2
has proven  0.011904761904761904
proven  1.0
proven useful  1.0
useful as  0.07142857142857142
a tool  0.00245398773006135
tool  1.0
tool to  0.5
to assist  0.0013280212483399733
assist  1.0
assist human  1.0
human translators  0.021739130434782608
translators  1.0
translators and  1.0
very limited  0.04878048780487805
limited number  0.2
of cases  0.00089126559714795
can even  0.0055248618784530384
even produce  0.037037037037037035
produce output  0.09090909090909091
output that  0.07692307692307693
be used  0.08016877637130802
used as  0.04424778761061947
e.g. ,  0.4642857142857143
, weather  0.0005614823133071309
The progress  0.005208333333333333
progress and  0.14285714285714285
and potential  0.001445086705202312
potential of  0.2857142857142857
translation has  0.02702702702702703
been debated  0.014705882352941176
debated  1.0
debated much  1.0
much through  0.045454545454545456
through its  0.125
its history  0.02857142857142857
history .  0.25
Since the  0.2
the 1950s  0.0020761245674740486
1950s  1.0
1950s ,  0.5
of scholars  0.00089126559714795
scholars  1.0
scholars have  0.5
have questioned  0.009615384615384616
questioned  1.0
questioned the  1.0
the possibility  0.002768166089965398
possibility  1.0
possibility of  0.75
of achieving  0.0017825311942959
achieving  1.0
achieving fully  0.5
automatic machine  0.043478260869565216
of high  0.00089126559714795
high quality  0.05555555555555555
quality .  0.1
Some critics  0.047619047619047616
critics  1.0
critics claim  1.0
claim  1.0
claim that  1.0
that there  0.0070921985815602835
are in-principle  0.004149377593360996
in-principle  1.0
in-principle obstacles  1.0
obstacles  1.0
obstacles to  1.0
to automatizing  0.0013280212483399733
automatizing  1.0
automatizing the  1.0
translation process  0.02702702702702703
In 1629  0.009523809523809525
1629  1.0
1629 ,  1.0
, Ren  0.0005614823133071309
Ren  1.0
Ren Descartes  1.0
Descartes  1.0
Descartes proposed  1.0
proposed a  0.2222222222222222
a universal  0.001226993865030675
universal  1.0
universal language  0.3333333333333333
language ,  0.04054054054054054
, with  0.004491858506457047
with equivalent  0.01092896174863388
equivalent  1.0
equivalent ideas  0.2
ideas  1.0
ideas in  0.5
in different  0.0056179775280898875
different tongues  0.02040816326530612
tongues  1.0
tongues sharing  1.0
sharing  1.0
sharing one  1.0
one symbol  0.015384615384615385
symbol  1.0
symbol .  0.5
, The  0.0005614823133071309
experiment -LRB-  0.2
-LRB- 1954  0.0027100271002710027
1954 -RRB-  0.3333333333333333
-RRB- involved  0.0028169014084507044
of over  0.00089126559714795
over sixty  0.08333333333333333
The experiment  0.005208333333333333
experiment was  0.4
great success  0.3333333333333333
success and  0.2
and ushered  0.001445086705202312
ushered  1.0
ushered in  1.0
in an  0.0149812734082397
an era  0.007575757575757576
era  1.0
era of  1.0
of substantial  0.00089126559714795
substantial  1.0
substantial funding  0.2
for machine-translation  0.0036101083032490976
machine-translation  1.0
machine-translation research  0.5
research .  0.09523809523809523
three to  0.3333333333333333
to five  0.0013280212483399733
Real progress  0.5
report -LRB-  0.25
-LRB- 1966  0.0027100271002710027
1966 -RRB-  0.3333333333333333
the ten-year-long  0.0006920415224913495
ten-year-long  1.0
ten-year-long research  1.0
fulfill expectations  0.5
funding was  0.125
was greatly  0.012987012987012988
greatly reduced  0.14285714285714285
Beginning in  0.5
as computational  0.003484320557491289
power increased  0.25
increased  1.0
increased and  0.2
and became  0.001445086705202312
became  1.0
became less  0.2
less expensive  0.08333333333333333
expensive  1.0
expensive ,  0.42857142857142855
, more  0.0022459292532285235
more interest  0.010526315789473684
interest  1.0
interest was  0.09090909090909091
was shown  0.025974025974025976
shown  1.0
shown in  0.4
models for  0.23076923076923078
translation .  0.05405405405405406
The idea  0.010416666666666666
idea  1.0
idea of  0.2857142857142857
of using  0.00089126559714795
using digital  0.01694915254237288
digital  1.0
digital computers  0.14285714285714285
computers for  0.1111111111111111
for translation  0.0036101083032490976
languages was  0.02
was proposed  0.025974025974025976
proposed as  0.1111111111111111
as early  0.003484320557491289
early as  0.1
as 1946  0.003484320557491289
1946  1.0
1946 by  1.0
by A.  0.005714285714285714
A.  1.0
A. D.  0.2
D. Booth  0.2
Booth  1.0
Booth and  1.0
and possibly  0.001445086705202312
possibly  1.0
possibly others  0.5
Warren Weaver  1.0
Weaver  1.0
Weaver wrote  1.0
wrote  1.0
wrote an  0.3333333333333333
an important  0.015151515151515152
important memorandum  0.0625
memorandum  1.0
memorandum ``  1.0
`` Translation  0.005291005291005291
Translation  0.6666666666666666
Translation ''  0.3333333333333333
'' in  0.03763440860215054
in 1949  0.0018726591760299626
1949  1.0
1949 .  0.5
was by  0.025974025974025976
by no  0.005714285714285714
no means  0.07692307692307693
means  1.0
means the  0.16666666666666666
first such  0.030303030303030304
such application  0.008130081300813009
application ,  0.14285714285714285
a demonstration  0.0036809815950920245
demonstration  1.0
demonstration was  0.2
was made  0.012987012987012988
1954 on  0.3333333333333333
the APEXC  0.0006920415224913495
APEXC  1.0
APEXC machine  1.0
machine at  0.012658227848101266
at Birkbeck  0.029411764705882353
Birkbeck  1.0
Birkbeck College  1.0
College  1.0
College -LRB-  0.5
-LRB- University  0.0027100271002710027
University  1.0
University of  0.3333333333333333
of London  0.00089126559714795
London  1.0
London -RRB-  1.0
a rudimentary  0.001226993865030675
rudimentary  1.0
rudimentary translation  0.5
English into  0.02702702702702703
into French  0.02564102564102564
French .  0.25
Several papers  0.3333333333333333
papers  1.0
papers on  0.6666666666666666
topic were  0.125
were published  0.024390243902439025
published at  0.14285714285714285
the time  0.004152249134948097
even articles  0.037037037037037035
in popular  0.0018726591760299626
popular  1.0
popular journals  0.1111111111111111
journals  1.0
journals -LRB-  0.5
see for  0.05
example Wireless  0.012345679012345678
Wireless  1.0
Wireless World  1.0
World ,  0.14285714285714285
, Sept.  0.0005614823133071309
Sept.  1.0
Sept. 1955  1.0
1955  1.0
1955 ,  1.0
, Cleave  0.0005614823133071309
Cleave  1.0
Cleave and  1.0
and Zacharov  0.001445086705202312
Zacharov  1.0
Zacharov -RRB-  1.0
A similar  0.02
similar application  0.037037037037037035
also pioneered  0.014492753623188406
pioneered  1.0
pioneered at  0.3333333333333333
College at  0.5
, was  0.0022459292532285235
was reading  0.012987012987012988
reading  1.0
reading and  0.25
and composing  0.001445086705202312
composing  1.0
composing Braille  1.0
Braille  1.0
Braille texts  1.0
texts by  0.058823529411764705
by computer  0.017142857142857144
computer .  0.06818181818181818
Translation process  0.6666666666666666
process Main  0.027777777777777776
: Translation  0.00980392156862745
process The  0.027777777777777776
The human  0.005208333333333333
process may  0.027777777777777776
be described  0.004219409282700422
described as  0.3333333333333333
as :  0.003484320557491289
: Decoding  0.00980392156862745
Decoding  0.5
Decoding the  0.5
the source  0.008304498269896194
source  1.0
source text  0.20833333333333334
text ;  0.006289308176100629
and Re-encoding  0.001445086705202312
Re-encoding  1.0
Re-encoding this  1.0
this meaning  0.01098901098901099
meaning in  0.08695652173913043
Behind this  1.0
this ostensibly  0.01098901098901099
ostensibly  1.0
ostensibly simple  1.0
simple procedure  0.038461538461538464
procedure lies  0.3333333333333333
lies  1.0
lies a  0.5
complex cognitive  0.041666666666666664
cognitive  1.0
cognitive operation  0.5
operation  1.0
operation .  0.5
To decode  0.1111111111111111
decode  1.0
decode the  1.0
text in  0.050314465408805034
in its  0.003745318352059925
its entirety  0.02857142857142857
entirety  1.0
entirety ,  1.0
the translator  0.0006920415224913495
translator  1.0
translator must  0.14285714285714285
must  1.0
must interpret  0.07142857142857142
interpret  1.0
interpret and  1.0
and analyze  0.001445086705202312
analyze  1.0
analyze all  0.25
all the  0.13953488372093023
features of  0.15384615384615385
process that  0.05555555555555555
that requires  0.0070921985815602835
requires in-depth  0.0625
in-depth  1.0
in-depth knowledge  0.6666666666666666
the grammar  0.006228373702422145
, syntax  0.0011229646266142617
syntax  1.0
syntax ,  0.45454545454545453
, idioms  0.0005614823133071309
etc. ,  0.045454545454545456
, of  0.0022459292532285235
source language  0.125
the culture  0.0006920415224913495
culture  1.0
culture of  1.0
its speakers  0.02857142857142857
speakers  1.0
speakers .  0.25
The translator  0.005208333333333333
translator needs  0.14285714285714285
needs the  0.1
same in-depth  0.04
knowledge to  0.037037037037037035
to re-encode  0.0013280212483399733
re-encode  1.0
re-encode the  1.0
Therein lies  1.0
lies the  0.5
the challenge  0.0006920415224913495
challenge  1.0
challenge in  1.0
: how  0.00980392156862745
how  1.0
how to  0.10344827586206896
to program  0.0013280212483399733
program a  0.09090909090909091
computer that  0.022727272727272728
that will  0.0070921985815602835
will ``  0.05714285714285714
`` understand  0.005291005291005291
understand  1.0
understand ''  0.14285714285714285
'' a  0.016129032258064516
text as  0.006289308176100629
person does  0.05263157894736842
does  1.0
does ,  0.1
and that  0.002890173410404624
`` create  0.005291005291005291
create  1.0
create ''  0.058823529411764705
a new  0.007361963190184049
new text  0.08333333333333333
language that  0.006756756756756757
that ``  0.02127659574468085
`` sounds  0.005291005291005291
sounds ''  0.06666666666666667
'' as  0.026881720430107527
as if  0.003484320557491289
if it  0.07142857142857142
it has  0.03418803418803419
been written  0.014705882352941176
person .  0.05263157894736842
This problem  0.06349206349206349
problem may  0.022727272727272728
be approached  0.004219409282700422
approached  1.0
approached in  0.5
of ways  0.0017825311942959
ways  1.0
ways .  0.125
Approaches Bernard  0.3333333333333333
Bernard  1.0
Bernard Vauquois  1.0
Vauquois  1.0
Vauquois '  1.0
' pyramid  0.05263157894736842
pyramid  1.0
pyramid showing  1.0
showing  1.0
showing comparative  0.5
comparative  1.0
comparative depths  1.0
depths  1.0
depths of  1.0
of intermediary  0.00089126559714795
intermediary  1.0
intermediary representation  0.6666666666666666
representation ,  0.15789473684210525
, interlingual  0.0011229646266142617
interlingual  1.0
interlingual machine  0.75
translation at  0.013513513513513514
the peak  0.0006920415224913495
peak  1.0
peak ,  1.0
, followed  0.0005614823133071309
followed  1.0
followed by  0.5
by transfer-based  0.005714285714285714
transfer-based  1.0
transfer-based ,  0.3333333333333333
, then  0.006176305446378439
then direct  0.02857142857142857
direct translation  0.16666666666666666
translation can  0.02702702702702703
can use  0.016574585635359115
use a  0.05555555555555555
a method  0.0049079754601227
method based  0.125
on linguistic  0.0047169811320754715
linguistic rules  0.0625
which means  0.028985507246376812
means that  0.6666666666666666
that words  0.0070921985815602835
words will  0.01834862385321101
be translated  0.012658227848101266
translated  1.0
translated in  0.25
a linguistic  0.00245398773006135
linguistic way  0.0625
way --  0.041666666666666664
most suitable  0.017241379310344827
suitable  1.0
suitable -LRB-  0.25
-LRB- orally  0.0027100271002710027
orally  1.0
orally speaking  1.0
speaking -RRB-  0.125
-RRB- words  0.0028169014084507044
words of  0.027522935779816515
language will  0.006756756756756757
will replace  0.02857142857142857
replace  1.0
replace the  1.0
the ones  0.001384083044982699
ones in  0.1
often argued  0.022727272727272728
argued  1.0
argued that  1.0
translation requires  0.013513513513513514
requires the  0.1875
the problem  0.006228373702422145
problem of  0.18181818181818182
understanding to  0.06060606060606061
be solved  0.004219409282700422
solved first  0.2
first .  0.030303030303030304
, rule-based  0.0005614823133071309
rule-based  1.0
rule-based methods  0.14285714285714285
methods parse  0.022727272727272728
parse a  0.1111111111111111
, usually  0.002807411566535654
usually creating  0.03125
creating an  0.2857142857142857
an intermediary  0.007575757575757576
intermediary ,  0.3333333333333333
, symbolic  0.0005614823133071309
symbolic  1.0
symbolic representation  1.0
, from  0.0005614823133071309
from which  0.028846153846153848
According to  1.0
the intermediary  0.0006920415224913495
is described  0.0020325203252032522
as interlingual  0.003484320557491289
translation or  0.013513513513513514
or transfer-based  0.0045045045045045045
transfer-based machine  0.6666666666666666
These methods  0.17647058823529413
methods require  0.022727272727272728
extensive lexicons  0.3333333333333333
lexicons  1.0
lexicons with  0.5
with morphological  0.00546448087431694
morphological  1.0
morphological ,  0.3333333333333333
, syntactic  0.0016844469399213925
syntactic ,  0.07692307692307693
and semantic  0.004335260115606936
information ,  0.043478260869565216
and large  0.001445086705202312
Given enough  0.07142857142857142
enough  1.0
enough data  0.4
translation programs  0.013513513513513514
programs often  0.09090909090909091
often work  0.022727272727272728
work well  0.041666666666666664
well enough  0.03571428571428571
enough for  0.2
a native  0.00245398773006135
native  1.0
native speaker  1.0
speaker  1.0
speaker of  0.1111111111111111
of one  0.0035650623885918
one language  0.03076923076923077
to get  0.005312084993359893
get  1.0
get the  0.2857142857142857
the approximate  0.0006920415224913495
approximate  1.0
approximate meaning  0.5
of what  0.0035650623885918
is written  0.0020325203252032522
other native  0.014285714285714285
speaker .  0.16666666666666666
difficulty is  0.14285714285714285
is getting  0.0040650406504065045
getting  1.0
getting enough  0.25
data of  0.012987012987012988
the right  0.0020761245674740486
right kind  0.1
kind to  0.09090909090909091
to support  0.0026560424966799467
support  1.0
support the  0.25
the particular  0.001384083044982699
particular method  0.07692307692307693
method .  0.125
the large  0.0006920415224913495
large multilingual  0.043478260869565216
multilingual corpus  0.3333333333333333
data needed  0.012987012987012988
needed for  0.09523809523809523
work is  0.125
not necessary  0.008928571428571428
necessary for  0.3
the grammar-based  0.0006920415224913495
grammar-based  1.0
grammar-based methods  1.0
methods .  0.022727272727272728
But then  0.16666666666666666
then ,  0.08571428571428572
grammar methods  0.02702702702702703
methods need  0.045454545454545456
need  1.0
need a  0.19047619047619047
a skilled  0.001226993865030675
skilled  1.0
skilled linguist  1.0
linguist  1.0
linguist to  0.5
to carefully  0.0013280212483399733
carefully  1.0
carefully design  1.0
design the  0.25
grammar that  0.02702702702702703
they use  0.025
use .  0.041666666666666664
To translate  0.1111111111111111
translate between  0.16666666666666666
between closely  0.02564102564102564
closely  1.0
closely related  0.4
related languages  0.06666666666666667
a technique  0.001226993865030675
technique referred  0.14285714285714285
as shallow-transfer  0.003484320557491289
shallow-transfer  1.0
shallow-transfer machine  1.0
translation may  0.013513513513513514
Rule-based The  0.5
The rule-based  0.005208333333333333
rule-based machine  0.14285714285714285
translation paradigm  0.013513513513513514
paradigm includes  0.3333333333333333
includes transfer-based  0.14285714285714285
and dictionary-based  0.001445086705202312
dictionary-based  1.0
dictionary-based machine  1.0
translation paradigms  0.013513513513513514
paradigms  1.0
paradigms .  1.0
: Rule-based  0.00980392156862745
Rule-based  0.5
Rule-based machine  0.5
translation Transfer-based  0.013513513513513514
Transfer-based  1.0
Transfer-based machine  1.0
translation Main  0.013513513513513514
: Transfer-based  0.00980392156862745
translation Interlingual  0.02702702702702703
Interlingual  1.0
Interlingual Main  0.3333333333333333
: Interlingual  0.00980392156862745
Interlingual machine  0.6666666666666666
translation is  0.013513513513513514
one instance  0.015384615384615385
instance  1.0
instance of  0.14285714285714285
of rule-based  0.00089126559714795
rule-based machine-translation  0.14285714285714285
machine-translation approaches  0.5
approaches .  0.10714285714285714
In this  0.047619047619047616
this approach  0.02197802197802198
approach ,  0.05714285714285714
translated ,  0.25
is transformed  0.0020325203252032522
transformed  1.0
transformed into  1.0
into an  0.02564102564102564
an interlingual  0.007575757575757576
interlingual ,  0.25
i.e. source  0.05263157894736842
source -  0.041666666666666664
- \/  0.0625
\/  1.0
\/ target-language-independent  0.3333333333333333
target-language-independent  1.0
target-language-independent representation  1.0
representation .  0.21052631578947367
The target  0.005208333333333333
then generated  0.02857142857142857
generated out  0.06666666666666667
out of  0.07142857142857142
the interlingua  0.0006920415224913495
interlingua  1.0
interlingua .  1.0
Dictionary-based Main  0.5
: Dictionary-based  0.00980392156862745
Dictionary-based  0.5
Dictionary-based machine  0.5
translation Machine  0.013513513513513514
Machine  0.6666666666666666
on dictionary  0.0047169811320754715
dictionary entries  0.14285714285714285
entries  1.0
entries ,  0.5
translated as  0.25
as they  0.010452961672473868
they are  0.175
are by  0.004149377593360996
dictionary .  0.14285714285714285
Statistical Main  0.1111111111111111
: Statistical  0.00980392156862745
Statistical machine  0.2222222222222222
translation Statistical  0.013513513513513514
translation tries  0.013513513513513514
tries  1.0
tries to  1.0
to generate  0.00796812749003984
generate  1.0
generate translations  0.05555555555555555
translations using  0.5
using statistical  0.03389830508474576
methods based  0.022727272727272728
on bilingual  0.0047169811320754715
bilingual  1.0
bilingual text  0.5
text corpora  0.006289308176100629
corpora ,  0.09090909090909091
the Canadian  0.001384083044982699
Canadian  1.0
Canadian Hansard  0.5
Hansard  1.0
Hansard corpus  1.0
corpus ,  0.06451612903225806
the English-French  0.0006920415224913495
English-French  1.0
English-French record  1.0
record  1.0
record of  1.0
Canadian parliament  0.5
parliament  1.0
parliament and  1.0
and EUROPARL  0.001445086705202312
EUROPARL  1.0
EUROPARL ,  1.0
the record  0.0006920415224913495
European Parliament  0.3333333333333333
Parliament .  0.5
Where such  1.0
such corpora  0.016260162601626018
corpora are  0.18181818181818182
are available  0.008298755186721992
available ,  0.23529411764705882
, impressive  0.0005614823133071309
impressive  1.0
impressive results  0.5
results can  0.047619047619047616
achieved translating  0.1
translating  1.0
translating texts  0.25
texts of  0.11764705882352941
a similar  0.001226993865030675
similar kind  0.037037037037037035
kind ,  0.09090909090909091
but such  0.014705882352941176
are still  0.016597510373443983
still very  0.06666666666666667
very rare  0.024390243902439025
rare  1.0
rare .  0.5
software was  0.037037037037037035
was CANDIDE  0.012987012987012988
CANDIDE  1.0
CANDIDE from  1.0
from IBM  0.009615384615384616
IBM .  0.3333333333333333
Google used  0.25
used SYSTRAN  0.008849557522123894
SYSTRAN  1.0
SYSTRAN for  1.0
for several  0.007220216606498195
several years  0.09090909090909091
but switched  0.014705882352941176
switched  1.0
switched to  1.0
a statistical  0.0036809815950920245
statistical translation  0.030303030303030304
translation method  0.013513513513513514
method in  0.0625
in October  0.0018726591760299626
October  1.0
October 2007  1.0
Recently ,  1.0
, they  0.004491858506457047
they improved  0.025
improved  1.0
improved their  0.25
their translation  0.029411764705882353
translation capabilities  0.013513513513513514
capabilities  1.0
capabilities by  0.2
by inputting  0.005714285714285714
inputting  1.0
inputting approximately  1.0
approximately  1.0
approximately 200  0.5
200  1.0
200 billion  0.5
billion  1.0
billion words  1.0
words from  0.01834862385321101
from United  0.009615384615384616
United  1.0
United Nations  0.2222222222222222
Nations  1.0
Nations materials  0.5
materials  1.0
materials to  0.5
to train  0.0013280212483399733
train  1.0
train their  1.0
their system  0.029411764705882353
Accuracy of  0.42857142857142855
has improved  0.011904761904761904
improved .  0.25
Example-based Main  0.3333333333333333
: Example-based  0.00980392156862745
Example-based  0.6666666666666666
Example-based machine  0.6666666666666666
translation Example-based  0.013513513513513514
translation -LRB-  0.02702702702702703
-LRB- EBMT  0.0027100271002710027
EBMT  1.0
EBMT -RRB-  1.0
-RRB- approach  0.0028169014084507044
approach was  0.02857142857142857
proposed by  0.1111111111111111
by Makoto  0.005714285714285714
Makoto  1.0
Makoto Nagao  1.0
Nagao  1.0
Nagao in  1.0
in 1984  0.0018726591760299626
1984  1.0
1984 .  1.0
often characterised  0.022727272727272728
characterised  1.0
characterised by  1.0
by its  0.005714285714285714
its use  0.02857142857142857
a bilingual  0.001226993865030675
bilingual corpus  0.5
corpus as  0.03225806451612903
as its  0.010452961672473868
its main  0.02857142857142857
main  1.0
main knowledge  0.125
, at  0.0016844469399213925
at run-time  0.014705882352941176
run-time  1.0
run-time .  1.0
is essentially  0.006097560975609756
essentially  1.0
essentially a  0.25
translation by  0.013513513513513514
by analogy  0.005714285714285714
analogy  1.0
analogy and  1.0
be viewed  0.016877637130801686
viewed  1.0
viewed as  1.0
an implementation  0.007575757575757576
of case-based  0.00089126559714795
case-based  1.0
case-based reasoning  1.0
reasoning  1.0
reasoning approach  0.14285714285714285
approach of  0.02857142857142857
Hybrid MT  0.5
MT Hybrid  0.2
Hybrid  0.5
Hybrid machine  0.5
-LRB- HMT  0.0027100271002710027
HMT  1.0
HMT -RRB-  1.0
-RRB- leverages  0.0028169014084507044
leverages  1.0
leverages the  1.0
the strengths  0.0006920415224913495
strengths  1.0
strengths of  0.5
statistical and  0.030303030303030304
and rule-based  0.001445086705202312
rule-based translation  0.14285714285714285
translation methodologies  0.013513513513513514
methodologies  1.0
methodologies .  1.0
Several MT  0.3333333333333333
MT companies  0.2
companies  1.0
companies -LRB-  0.5
-LRB- Asia  0.0027100271002710027
Asia  1.0
Asia Online  1.0
Online  1.0
Online ,  0.5
, LinguaSys  0.0005614823133071309
LinguaSys  1.0
LinguaSys ,  1.0
, Systran  0.0005614823133071309
Systran  1.0
Systran ,  1.0
, PangeaMT  0.0005614823133071309
PangeaMT  1.0
PangeaMT ,  1.0
, UPV  0.0005614823133071309
UPV  1.0
UPV -RRB-  1.0
are claiming  0.004149377593360996
claiming  1.0
claiming to  1.0
to have  0.013280212483399735
a hybrid  0.00245398773006135
hybrid  1.0
hybrid approach  0.5
approach using  0.02857142857142857
using both  0.01694915254237288
both rules  0.03225806451612903
rules and  0.023255813953488372
The approaches  0.005208333333333333
approaches differ  0.03571428571428571
differ in  0.3333333333333333
ways :  0.25
: Rules  0.0196078431372549
Rules  0.6666666666666666
Rules post-processed  0.3333333333333333
post-processed  1.0
post-processed by  1.0
by statistics  0.005714285714285714
statistics :  0.125
: Translations  0.00980392156862745
Translations  1.0
Translations are  1.0
are performed  0.004149377593360996
performed using  0.1
a rules  0.001226993865030675
rules based  0.023255813953488372
based engine  0.018518518518518517
engine  1.0
engine .  0.6666666666666666
Statistics are  0.3333333333333333
are then  0.004149377593360996
then used  0.02857142857142857
an attempt  0.022727272727272728
attempt  1.0
attempt to  1.0
to adjust\/correct  0.0013280212483399733
adjust\/correct  1.0
adjust\/correct the  1.0
output from  0.038461538461538464
rules engine  0.023255813953488372
Statistics guided  0.3333333333333333
guided  1.0
guided by  1.0
by rules  0.005714285714285714
Rules are  0.6666666666666666
to pre-process  0.0013280212483399733
pre-process  1.0
pre-process data  1.0
data in  0.025974025974025976
better guide  0.1111111111111111
guide  1.0
guide the  1.0
the statistical  0.001384083044982699
statistical engine  0.030303030303030304
also used  0.028985507246376812
to post-process  0.0013280212483399733
post-process  1.0
post-process the  1.0
statistical output  0.030303030303030304
output to  0.038461538461538464
to perform  0.005312084993359893
perform functions  0.09090909090909091
functions  1.0
functions such  0.5
as normalization  0.003484320557491289
normalization  1.0
normalization .  0.3333333333333333
This approach  0.031746031746031744
approach has  0.02857142857142857
has a  0.047619047619047616
a lot  0.0036809815950920245
lot  1.0
lot more  0.3333333333333333
more power  0.010526315789473684
power ,  0.25
, flexibility  0.0005614823133071309
flexibility  1.0
flexibility and  1.0
and control  0.004335260115606936
control  1.0
control when  0.2
when translating  0.02857142857142857
translating .  0.25
Major issues  0.5
issues  1.0
issues Disambiguation  0.2
Disambiguation  1.0
Disambiguation Main  1.0
: Word  0.00980392156862745
Word  0.2857142857142857
disambiguation Word-sense  0.1
Word-sense  1.0
Word-sense disambiguation  1.0
disambiguation concerns  0.1
concerns  1.0
concerns finding  0.5
finding  1.0
finding a  0.4
a suitable  0.0036809815950920245
suitable translation  0.25
translation when  0.013513513513513514
when a  0.11428571428571428
word can  0.03333333333333333
can have  0.011049723756906077
The problem  0.015625
problem was  0.022727272727272728
was first  0.012987012987012988
first raised  0.030303030303030304
raised  1.0
raised in  1.0
1950s by  0.25
by Yehoshua  0.005714285714285714
Yehoshua  1.0
Yehoshua Bar-Hillel  1.0
Bar-Hillel  1.0
Bar-Hillel .  1.0
He pointed  0.125
pointed  1.0
pointed out  1.0
out that  0.07142857142857142
that without  0.0035460992907801418
without a  0.07692307692307693
a ``  0.007361963190184049
`` universal  0.010582010582010581
universal encyclopedia  0.3333333333333333
encyclopedia  1.0
encyclopedia ''  1.0
a machine  0.008588957055214725
machine would  0.02531645569620253
would never  0.018867924528301886
never be  0.4
be able  0.02109704641350211
distinguish between  0.4
the two  0.0034602076124567475
two meanings  0.034482758620689655
meanings  1.0
meanings of  0.25
Today there  1.0
are numerous  0.004149377593360996
numerous  1.0
numerous approaches  1.0
approaches designed  0.03571428571428571
designed  1.0
designed to  0.7142857142857143
to overcome  0.0013280212483399733
overcome  1.0
overcome this  0.5
They can  0.3333333333333333
be approximately  0.004219409282700422
approximately divided  0.5
divided  1.0
divided into  0.6666666666666666
into ``  0.01282051282051282
`` shallow  0.005291005291005291
shallow  1.0
shallow ''  0.16666666666666666
'' approaches  0.010752688172043012
approaches and  0.03571428571428571
`` deep  0.005291005291005291
deep  1.0
deep ''  0.14285714285714285
Shallow approaches  0.5
approaches assume  0.03571428571428571
assume  1.0
assume no  0.5
no knowledge  0.07692307692307693
They simply  0.3333333333333333
simply apply  0.08333333333333333
apply  1.0
apply statistical  0.2
words surrounding  0.009174311926605505
surrounding the  0.2
the ambiguous  0.001384083044982699
ambiguous word  0.08333333333333333
Deep approaches  1.0
approaches presume  0.03571428571428571
presume  1.0
presume a  1.0
a comprehensive  0.0036809815950920245
comprehensive  1.0
comprehensive knowledge  0.2
So far  0.3333333333333333
far  1.0
far ,  0.125
, shallow  0.0005614823133071309
shallow approaches  0.16666666666666666
approaches have  0.07142857142857142
been more  0.029411764705882353
more successful  0.031578947368421054
successful .  0.1111111111111111
-RRB- The  0.005633802816901409
The late  0.005208333333333333
late Claude  0.1111111111111111
Claude  1.0
Claude Piron  1.0
Piron  1.0
Piron ,  0.3333333333333333
a long-time  0.001226993865030675
long-time  1.0
long-time translator  1.0
translator for  0.14285714285714285
the United  0.004844290657439446
Nations and  0.5
World Health  0.14285714285714285
Health  1.0
Health Organization  0.5
Organization  1.0
Organization ,  1.0
, wrote  0.0005614823133071309
wrote that  0.16666666666666666
at its  0.014705882352941176
its best  0.02857142857142857
best ,  0.05555555555555555
, automates  0.0005614823133071309
automates  1.0
automates the  1.0
the easier  0.0006920415224913495
easier part  0.125
a translator  0.0036809815950920245
translator 's  0.2857142857142857
's job  0.0392156862745098
job  1.0
job ;  0.5
the harder  0.0020761245674740486
harder  1.0
harder and  0.14285714285714285
more time-consuming  0.010526315789473684
time-consuming part  0.3333333333333333
part usually  0.037037037037037035
usually involves  0.03125
involves doing  0.1
doing  1.0
doing extensive  0.5
extensive research  0.3333333333333333
research to  0.023809523809523808
resolve ambiguities  0.5
ambiguities  1.0
ambiguities in  0.25
the grammatical  0.0020761245674740486
grammatical and  0.09090909090909091
and lexical  0.001445086705202312
lexical exigencies  0.07692307692307693
exigencies  1.0
exigencies of  1.0
language require  0.006756756756756757
require to  0.045454545454545456
be resolved  0.004219409282700422
resolved  1.0
resolved :  1.0
: Why  0.00980392156862745
Why does  0.2857142857142857
does a  0.2
translator need  0.14285714285714285
whole workday  0.1111111111111111
workday  1.0
workday to  1.0
translate five  0.16666666666666666
five pages  0.2
pages  1.0
pages ,  0.42857142857142855
and not  0.011560693641618497
not an  0.008928571428571428
an hour  0.007575757575757576
hour  1.0
hour or  1.0
or two  0.009009009009009009
two ?  0.034482758620689655
... About  0.5
About  0.5
About 90  0.5
90  1.0
90 %  1.0
%  1.0
% of  0.20512820512820512
an average  0.007575757575757576
average  1.0
average text  0.5
text corresponds  0.006289308176100629
corresponds  1.0
corresponds to  1.0
to these  0.0026560424966799467
these simple  0.023809523809523808
simple conditions  0.038461538461538464
conditions  1.0
conditions .  0.2
But unfortunately  0.16666666666666666
unfortunately  1.0
unfortunately ,  1.0
there 's  0.025
's the  0.0196078431372549
other 10  0.014285714285714285
10 %  0.25
% .  0.23076923076923078
It 's  0.05263157894736842
's that  0.0196078431372549
that part  0.0035460992907801418
part that  0.037037037037037035
requires six  0.0625
six  1.0
six -LRB-  0.5
-LRB- more  0.005420054200542005
more -RRB-  0.010526315789473684
-RRB- hours  0.0028169014084507044
hours  1.0
hours of  0.5
of work  0.00089126559714795
work .  0.08333333333333333
There are  0.5454545454545454
are ambiguities  0.004149377593360996
ambiguities one  0.25
one has  0.015384615384615385
has to  0.05952380952380952
resolve .  0.25
For instance  0.11475409836065574
instance ,  0.6428571428571429
the author  0.0020761245674740486
author  1.0
author of  0.3333333333333333
an Australian  0.007575757575757576
Australian  1.0
Australian physician  0.5
physician  1.0
physician ,  1.0
, cited  0.0005614823133071309
cited  1.0
cited the  1.0
the example  0.0020761245674740486
an epidemic  0.007575757575757576
epidemic  1.0
epidemic which  1.0
was declared  0.012987012987012988
declared  1.0
declared during  0.5
during World  0.1
World War  0.14285714285714285
War  1.0
War II  1.0
II  1.0
II in  0.5
`` Japanese  0.005291005291005291
Japanese prisoner  0.125
prisoner  1.0
prisoner of  1.0
of war  0.00089126559714795
war  1.0
war camp  1.0
camp  1.0
camp ''  0.25
Was he  1.0
he  1.0
he talking  0.14285714285714285
talking  1.0
talking about  1.0
about an  0.025
an American  0.007575757575757576
American  1.0
American camp  0.2
camp with  0.5
with Japanese  0.00546448087431694
Japanese prisoners  0.125
prisoners  1.0
prisoners or  0.5
a Japanese  0.001226993865030675
Japanese camp  0.125
with American  0.00546448087431694
American prisoners  0.2
prisoners ?  0.5
The English  0.005208333333333333
has two  0.011904761904761904
two senses  0.034482758620689655
senses .  0.5
's necessary  0.0196078431372549
necessary therefore  0.1
therefore  1.0
therefore to  0.2
to do  0.00398406374501992
do research  0.038461538461538464
research ,  0.07142857142857142
, maybe  0.0005614823133071309
maybe  1.0
maybe to  1.0
the extent  0.001384083044982699
extent of  0.25
a phone  0.001226993865030675
phone  1.0
phone call  0.25
call  1.0
call to  0.3333333333333333
to Australia  0.0013280212483399733
Australia  1.0
Australia .  1.0
The ideal  0.005208333333333333
ideal  1.0
ideal deep  1.0
deep approach  0.14285714285714285
approach would  0.02857142857142857
would require  0.05660377358490566
require the  0.18181818181818182
do all  0.038461538461538464
the research  0.0020761245674740486
research necessary  0.023809523809523808
for this  0.018050541516245487
this kind  0.01098901098901099
kind of  0.7272727272727273
of disambiguation  0.00089126559714795
disambiguation on  0.1
on its  0.009433962264150943
its own  0.14285714285714285
own  1.0
own ;  0.16666666666666666
; but  0.0425531914893617
this would  0.01098901098901099
require a  0.22727272727272727
a higher  0.00245398773006135
higher  1.0
higher degree  0.14285714285714285
degree  1.0
degree of  0.5
of AI  0.00089126559714795
AI  1.0
AI than  0.3333333333333333
than has  0.022222222222222223
has yet  0.011904761904761904
yet  1.0
yet been  0.5
been attained  0.014705882352941176
attained  1.0
attained .  1.0
A shallow  0.04
shallow approach  0.3333333333333333
approach which  0.05714285714285714
which simply  0.007246376811594203
simply guessed  0.08333333333333333
guessed  1.0
guessed at  1.0
the sense  0.0006920415224913495
sense of  0.125
ambiguous English  0.08333333333333333
English phrase  0.02702702702702703
phrase  1.0
phrase that  0.1
that Piron  0.0035460992907801418
Piron mentions  0.3333333333333333
mentions -LRB-  0.3333333333333333
-LRB- based  0.005420054200542005
based ,  0.037037037037037035
perhaps ,  0.16666666666666666
, on  0.0039303761931499155
which kind  0.007246376811594203
of prisoner-of-war  0.00089126559714795
prisoner-of-war  1.0
prisoner-of-war camp  1.0
camp is  0.25
more often  0.010526315789473684
often mentioned  0.022727272727272728
mentioned  1.0
mentioned in  0.16666666666666666
given corpus  0.041666666666666664
corpus -RRB-  0.12903225806451613
-RRB- would  0.005633802816901409
would have  0.05660377358490566
a reasonable  0.00245398773006135
reasonable  1.0
reasonable chance  0.5
chance  1.0
chance of  1.0
of guessing  0.00089126559714795
guessing  1.0
guessing wrong  1.0
wrong  1.0
wrong fairly  1.0
fairly often  0.25
often .  0.022727272727272728
approach that  0.05714285714285714
that involves  0.0035460992907801418
involves ``  0.1
`` ask  0.005291005291005291
ask  1.0
ask the  0.5
user about  0.07142857142857142
about each  0.025
each ambiguity  0.022222222222222223
ambiguity ''  0.125
'' would  0.010752688172043012
would ,  0.018867924528301886
, by  0.002807411566535654
by Piron  0.005714285714285714
Piron 's  0.3333333333333333
's estimate  0.0196078431372549
estimate ,  0.25
only automate  0.02631578947368421
automate  1.0
automate about  0.3333333333333333
about 25  0.025
25  1.0
25 %  1.0
a professional  0.001226993865030675
professional  1.0
professional translator  1.0
job ,  0.5
, leaving  0.0005614823133071309
leaving  1.0
leaving the  1.0
harder 75  0.14285714285714285
75  1.0
75 %  1.0
% still  0.02564102564102564
still to  0.06666666666666667
be done  0.02109704641350211
done  1.0
done by  0.18181818181818182
The objects  0.005208333333333333
objects of  0.2
of discourse  0.00980392156862745
discourse analysis  0.2222222222222222
analysis --  0.015384615384615385
-- discourse  0.04
discourse ,  0.08333333333333333
, writing  0.0011229646266142617
writing ,  0.2222222222222222
, conversation  0.0005614823133071309
conversation ,  0.25
, communicative  0.0005614823133071309
communicative  1.0
communicative event  0.3333333333333333
event ,  0.6666666666666666
etc. --  0.045454545454545456
-- are  0.08
are variously  0.004149377593360996
variously  1.0
variously defined  1.0
defined in  0.16666666666666666
of coherent  0.00089126559714795
coherent  1.0
coherent sequences  0.2
sequences  1.0
sequences of  0.3333333333333333
, propositions  0.0011229646266142617
propositions  1.0
propositions ,  1.0
, speech  0.004491858506457047
acts or  0.3333333333333333
or turns-at-talk  0.0045045045045045045
turns-at-talk  1.0
turns-at-talk .  1.0
Contrary to  1.0
to much  0.0013280212483399733
much of  0.09090909090909091
of traditional  0.00089126559714795
traditional  1.0
traditional linguistics  1.0
, discourse  0.0005614823133071309
discourse analysts  0.05555555555555555
analysts  1.0
analysts not  0.5
only study  0.02631578947368421
study  1.0
study language  0.25
use `  0.013888888888888888
` beyond  0.0625
boundary '  0.16666666666666666
but also  0.07352941176470588
also prefer  0.014492753623188406
prefer  1.0
prefer to  0.5
to analyze  0.0013280212483399733
analyze `  0.25
` naturally  0.0625
naturally  1.0
naturally occurring  0.5
occurring  1.0
occurring '  1.0
' language  0.05263157894736842
use ,  0.05555555555555555
not invented  0.008928571428571428
invented  1.0
invented examples  0.5
Text linguistics  0.16666666666666666
is related  0.0020325203252032522
related .  0.06666666666666667
The essential  0.005208333333333333
essential  1.0
essential difference  1.0
difference  1.0
difference between  0.25
between discourse  0.1282051282051282
analysis and  0.03076923076923077
and text  0.005780346820809248
text linguistics  0.006289308176100629
it aims  0.008547008547008548
aims  1.0
aims at  0.3333333333333333
at revealing  0.014705882352941176
revealing  1.0
revealing socio-psychological  1.0
socio-psychological  1.0
socio-psychological characteristics  1.0
characteristics  1.0
characteristics of  0.5
a person\/persons  0.001226993865030675
person\/persons  1.0
person\/persons rather  1.0
than text  0.022222222222222223
text structure  0.006289308176100629
structure .  0.16666666666666666
analysis has  0.015384615384615385
been taken  0.014705882352941176
taken  1.0
taken up  0.3333333333333333
up in  0.09090909090909091
a variety  0.008588957055214725
variety  1.0
variety of  1.0
of social  0.0017825311942959
social science  0.07142857142857142
science disciplines  0.1
disciplines  1.0
disciplines ,  1.0
, sociology  0.0005614823133071309
sociology  1.0
sociology ,  1.0
, anthropology  0.0005614823133071309
anthropology  1.0
anthropology ,  1.0
, social  0.0011229646266142617
social work  0.07142857142857142
work ,  0.125
, cognitive  0.0005614823133071309
cognitive psychology  0.5
psychology  1.0
psychology ,  0.75
social psychology  0.07142857142857142
, international  0.0005614823133071309
international  1.0
international relations  0.5
relations  1.0
relations ,  0.16666666666666666
human geography  0.021739130434782608
geography  1.0
geography ,  1.0
, communication  0.0005614823133071309
communication  1.0
communication studies  0.2
studies and  0.25
and translation  0.004335260115606936
translation studies  0.013513513513513514
is subject  0.0020325203252032522
subject  1.0
subject to  0.125
to its  0.0013280212483399733
own assumptions  0.16666666666666666
assumptions ,  0.2
, dimensions  0.0005614823133071309
dimensions  1.0
dimensions of  0.6666666666666666
of analysis  0.00089126559714795
analysis ,  0.1076923076923077
and methodologies  0.001445086705202312
The examples  0.005208333333333333
examples and  0.16666666666666666
and perspective  0.001445086705202312
perspective in  0.25
in this  0.018726591760299626
this article  0.04395604395604396
article deal  0.034482758620689655
deal primarily  0.25
primarily  1.0
primarily with  0.5
United States  0.7777777777777778
States  1.0
States and  0.14285714285714285
and do  0.001445086705202312
not represent  0.008928571428571428
represent  1.0
represent a  0.2222222222222222
a worldwide  0.001226993865030675
worldwide  1.0
worldwide view  1.0
view  1.0
view of  0.3333333333333333
the subject  0.0034602076124567475
subject .  0.25
Please improve  0.3333333333333333
improve  1.0
improve this  0.15384615384615385
article and  0.06896551724137931
and discuss  0.001445086705202312
discuss  1.0
discuss the  1.0
the issue  0.002768166089965398
issue  1.0
issue on  0.125
the talk  0.0006920415224913495
talk  1.0
talk page  1.0
page .  0.14285714285714285
-LRB- December  0.0027100271002710027
December  1.0
December 2010  1.0
2010  1.0
2010 -RRB-  0.3333333333333333
-RRB- Some  0.0028169014084507044
Some  0.23809523809523808
Some scholars  0.047619047619047616
scholars -LRB-  0.5
-LRB- which  0.008130081300813009
which ?  0.007246376811594203
? -RRB-  0.25
consider the  0.5
the Austrian  0.0006920415224913495
Austrian  1.0
Austrian emigre  1.0
emigre  1.0
emigre Leo  1.0
Leo  1.0
Leo Spitzer  1.0
Spitzer  1.0
Spitzer 's  1.0
's Stilstudien  0.0196078431372549
Stilstudien  1.0
Stilstudien -LRB-  1.0
-LRB- Style  0.0027100271002710027
Style  1.0
Style Studies  1.0
Studies  1.0
Studies -RRB-  1.0
of 1928  0.00089126559714795
1928  1.0
1928 the  1.0
the earliest  0.0006920415224913495
earliest  1.0
earliest example  0.5
; Michel  0.02127659574468085
Michel  1.0
Michel Foucault  1.0
Foucault  1.0
Foucault himself  0.3333333333333333
himself  1.0
himself translated  0.5
translated it  0.25
But the  0.16666666666666666
the term  0.0034602076124567475
term first  0.05555555555555555
first came  0.030303030303030304
came  1.0
came into  0.5
into general  0.01282051282051282
general use  0.045454545454545456
use following  0.013888888888888888
following the  0.06666666666666667
the publication  0.001384083044982699
publication of  0.6666666666666666
a series  0.007361963190184049
of papers  0.00089126559714795
papers by  0.3333333333333333
by Zellig  0.005714285714285714
Zellig  1.0
Zellig Harris  1.0
Harris  0.8888888888888888
Harris beginning  0.1111111111111111
beginning  1.0
beginning in  0.5
in 1952  0.0018726591760299626
1952  1.0
1952 and  0.5
and reporting  0.001445086705202312
reporting  1.0
reporting on  0.3333333333333333
on work  0.0047169811320754715
work from  0.041666666666666664
which he  0.007246376811594203
he developed  0.14285714285714285
developed transformational  0.038461538461538464
grammar in  0.02702702702702703
late 1930s  0.1111111111111111
1930s  1.0
1930s .  1.0
Formal equivalence  1.0
equivalence  1.0
equivalence relations  0.5
relations among  0.16666666666666666
among the  0.125
the sentences  0.005536332179930796
sentences of  0.02631578947368421
a coherent  0.00245398773006135
coherent discourse  0.2
discourse are  0.027777777777777776
are made  0.012448132780082987
made explicit  0.0625
explicit by  0.2
by using  0.017142857142857144
using sentence  0.01694915254237288
sentence transformations  0.020833333333333332
transformations  1.0
transformations to  0.5
to put  0.0026560424966799467
put  1.0
put the  0.25
a canonical  0.001226993865030675
canonical  1.0
canonical form  1.0
form .  0.1
Words and  0.25
and sentences  0.002890173410404624
sentences with  0.013157894736842105
equivalent information  0.2
information then  0.021739130434782608
then appear  0.02857142857142857
appear  1.0
appear in  0.4375
same column  0.04
column  1.0
column of  1.0
an array  0.007575757575757576
array  1.0
array .  1.0
This work  0.031746031746031744
work progressed  0.041666666666666664
progressed  1.0
progressed over  1.0
over the  0.25
the next  0.004152249134948097
next  1.0
next four  0.14285714285714285
four decades  0.14285714285714285
decades  1.0
decades -LRB-  1.0
see references  0.05
references  1.0
references -RRB-  0.5
-RRB- into  0.005633802816901409
a science  0.00245398773006135
science of  0.1
of sublanguage  0.0017825311942959
sublanguage  1.0
sublanguage analysis  0.3333333333333333
-LRB- Kittredge  0.0027100271002710027
Kittredge  1.0
Kittredge &  0.5
&  1.0
& Lehrberger  0.125
Lehrberger  1.0
Lehrberger 1982  1.0
1982  1.0
1982 -RRB-  0.3333333333333333
, culminating  0.0005614823133071309
culminating  1.0
culminating in  1.0
demonstration of  0.4
the informational  0.0006920415224913495
informational  1.0
informational structures  0.5
structures in  0.2
in texts  0.0018726591760299626
a sublanguage  0.001226993865030675
sublanguage of  0.3333333333333333
of science  0.00089126559714795
, that  0.0022459292532285235
of immunology  0.00089126559714795
immunology  1.0
immunology ,  1.0
, -LRB-  0.0016844469399213925
-LRB- Harris  0.005420054200542005
Harris et  0.1111111111111111
et  1.0
et al.  1.0
al.  1.0
al. 1989  1.0
1989  1.0
1989 -RRB-  0.5
a fully  0.001226993865030675
fully articulated  0.16666666666666666
articulated  1.0
articulated theory  1.0
theory of  0.07692307692307693
of linguistic  0.0017825311942959
linguistic informational  0.0625
informational content  0.5
content -LRB-  0.08333333333333333
Harris 1991  0.1111111111111111
most linguists  0.017241379310344827
linguists  1.0
linguists decided  0.3333333333333333
decided  1.0
decided a  0.3333333333333333
a succession  0.001226993865030675
succession  1.0
succession of  1.0
of elaborate  0.00089126559714795
elaborate  1.0
elaborate theories  1.0
of sentence-level  0.00089126559714795
sentence-level  1.0
sentence-level syntax  1.0
syntax and  0.09090909090909091
and semantics  0.004335260115606936
semantics .  0.07142857142857142
Although Harris  0.125
Harris had  0.1111111111111111
had mentioned  0.07142857142857142
mentioned the  0.16666666666666666
whole discourses  0.1111111111111111
discourses  1.0
discourses ,  0.5
, he  0.0011229646266142617
he had  0.14285714285714285
had not  0.07142857142857142
not worked  0.008928571428571428
worked out  0.2
out a  0.07142857142857142
comprehensive model  0.2
model ,  0.1
as of  0.006968641114982578
of January  0.00089126559714795
January  1.0
January ,  0.25
, 1952  0.0005614823133071309
1952 .  0.5
A linguist  0.02
linguist working  0.5
working for  0.14285714285714285
the American  0.001384083044982699
American Bible  0.2
Bible  1.0
Bible Society  1.0
Society  1.0
Society ,  1.0
, James  0.0022459292532285235
James  1.0
James A.  0.5
A. Lauriault\/Loriot  0.4
Lauriault\/Loriot  1.0
Lauriault\/Loriot ,  1.0
, needed  0.0005614823133071309
needed to  0.09523809523809523
to find  0.010624169986719787
find answers  0.07692307692307693
answers to  0.08333333333333333
to some  0.006640106241699867
some fundamental  0.012048192771084338
fundamental  1.0
fundamental errors  0.5
errors in  0.2
in translating  0.0018726591760299626
translating Quechua  0.25
Quechua  1.0
Quechua ,  1.0
the Cuzco  0.0006920415224913495
Cuzco  1.0
Cuzco area  1.0
area  1.0
area of  0.45454545454545453
of Peru  0.00089126559714795
Peru  1.0
Peru .  0.5
He took  0.125
took  1.0
took Harris  1.0
Harris 's  0.2222222222222222
's idea  0.0196078431372549
idea ,  0.14285714285714285
, recorded  0.0005614823133071309
recorded  1.0
recorded all  0.5
the legends  0.0006920415224913495
legends  1.0
legends and  1.0
, after  0.0005614823133071309
after going  0.08333333333333333
going over  0.25
meaning and  0.043478260869565216
and placement  0.001445086705202312
placement  1.0
placement of  1.0
word with  0.016666666666666666
of Quechua  0.00089126559714795
was able  0.05194805194805195
to form  0.005312084993359893
form logical  0.05
logical  1.0
logical ,  0.16666666666666666
, mathematical  0.0005614823133071309
mathematical  1.0
mathematical rules  0.5
that transcended  0.0035460992907801418
transcended  1.0
transcended the  1.0
the simple  0.0006920415224913495
simple sentence  0.038461538461538464
sentence structure  0.020833333333333332
He then  0.125
then applied  0.05714285714285714
applied the  0.06666666666666667
process to  0.1111111111111111
another language  0.23076923076923078
language of  0.006756756756756757
of Eastern  0.00089126559714795
Eastern  1.0
Eastern Peru  1.0
Peru ,  0.5
, Shipibo  0.0005614823133071309
Shipibo  1.0
Shipibo .  0.5
He taught  0.125
taught  1.0
taught the  0.6666666666666666
the theory  0.0020761245674740486
theory in  0.07692307692307693
in Norman  0.0018726591760299626
Norman  1.0
Norman ,  0.5
, Oklahoma  0.0005614823133071309
Oklahoma  1.0
Oklahoma ,  1.0
the summers  0.0006920415224913495
summers  1.0
summers of  1.0
of 1956  0.00089126559714795
1956  1.0
1956 and  1.0
and 1957  0.001445086705202312
1957  1.0
1957 and  1.0
and entered  0.001445086705202312
entered the  0.5
the University  0.0006920415224913495
of Pennsylvania  0.00089126559714795
Pennsylvania  1.0
Pennsylvania in  1.0
the interim  0.0006920415224913495
interim  1.0
interim year  1.0
year  1.0
year .  0.5
He tried  0.125
tried  1.0
tried to  0.3333333333333333
to publish  0.0013280212483399733
publish  1.0
publish a  1.0
a paper  0.001226993865030675
paper  1.0
paper Shipibo  0.09090909090909091
Shipibo Paragraph  0.5
Paragraph  1.0
Paragraph Structure  1.0
Structure  1.0
Structure ,  1.0
but it  0.058823529411764705
it was  0.02564102564102564
was delayed  0.012987012987012988
delayed  1.0
delayed until  1.0
until 1970  0.5
1970  1.0
1970 -LRB-  0.3333333333333333
-LRB- Loriot  0.0027100271002710027
Loriot  1.0
Loriot &  1.0
& Hollenbach  0.125
Hollenbach  1.0
Hollenbach 1970  1.0
1970 -RRB-  0.3333333333333333
the meantime  0.0006920415224913495
meantime  1.0
meantime ,  1.0
, Dr.  0.0005614823133071309
Dr.  1.0
Dr. Kenneth  1.0
Kenneth  1.0
Kenneth Lee  1.0
Lee  1.0
Lee Pike  1.0
Pike  1.0
Pike ,  1.0
a professor  0.001226993865030675
professor  1.0
professor at  1.0
at University  0.014705882352941176
of Michigan  0.00089126559714795
Michigan  1.0
Michigan ,  1.0
, Ann  0.0005614823133071309
Ann  1.0
Ann Arbor  1.0
Arbor  1.0
Arbor ,  1.0
, taught  0.0005614823133071309
and one  0.001445086705202312
of his  0.00267379679144385
his students  0.16666666666666666
students  1.0
students ,  0.3333333333333333
, Robert  0.0016844469399213925
Robert  1.0
Robert E.  0.5
E. Longacre  0.5
Longacre  1.0
Longacre ,  1.0
to disseminate  0.0013280212483399733
disseminate  1.0
disseminate it  1.0
it in  0.008547008547008548
a dissertation  0.001226993865030675
dissertation  1.0
dissertation .  0.3333333333333333
's methodology  0.0196078431372549
methodology  1.0
methodology was  0.5
was developed  0.012987012987012988
developed into  0.038461538461538464
system for  0.021505376344086023
the computer-aided  0.0006920415224913495
computer-aided analysis  0.3333333333333333
language by  0.006756756756756757
a team  0.001226993865030675
team  1.0
team led  1.0
led  1.0
led by  0.3333333333333333
by Naomi  0.005714285714285714
Naomi  1.0
Naomi Sager  1.0
Sager  1.0
Sager at  0.5
at NYU  0.014705882352941176
NYU  1.0
NYU ,  1.0
sublanguage domains  0.3333333333333333
domains ,  0.125
most notably  0.017241379310344827
notably to  0.3333333333333333
to medical  0.0013280212483399733
medical  1.0
medical informatics  0.16666666666666666
informatics  1.0
informatics .  1.0
The software  0.005208333333333333
software for  0.037037037037037035
the Medical  0.0006920415224913495
Medical  1.0
Medical Language  0.5
Language Processor  0.08333333333333333
Processor  1.0
Processor is  1.0
is publicly  0.0020325203252032522
publicly  1.0
publicly available  1.0
available on  0.058823529411764705
on SourceForge  0.0047169811320754715
SourceForge  1.0
SourceForge .  1.0
late 1960s  0.1111111111111111
1960s and  0.3333333333333333
and 1970s  0.001445086705202312
1970s  1.0
1970s ,  0.3333333333333333
and without  0.002890173410404624
without reference  0.07692307692307693
reference to  0.25
to this  0.00796812749003984
this prior  0.01098901098901099
prior work  0.3333333333333333
of other  0.0035650623885918
other approaches  0.014285714285714285
new cross-discipline  0.041666666666666664
cross-discipline  1.0
cross-discipline of  1.0
of DA  0.00089126559714795
DA began  0.3333333333333333
to develop  0.006640106241699867
develop  1.0
develop in  0.2
the humanities  0.0006920415224913495
humanities  1.0
humanities and  1.0
and social  0.004335260115606936
social sciences  0.14285714285714285
sciences  1.0
sciences concurrently  0.5
concurrently  1.0
concurrently with  1.0
with ,  0.00546448087431694
, other  0.0005614823133071309
other disciplines  0.014285714285714285
as semiotics  0.003484320557491289
semiotics  1.0
semiotics ,  1.0
, psycholinguistics  0.0005614823133071309
psycholinguistics  1.0
psycholinguistics ,  0.5
, sociolinguistics  0.0005614823133071309
sociolinguistics  1.0
sociolinguistics ,  0.5
and pragmatics  0.001445086705202312
pragmatics  1.0
pragmatics .  0.3333333333333333
these approaches  0.047619047619047616
approaches ,  0.03571428571428571
those influenced  0.045454545454545456
influenced  1.0
influenced by  1.0
sciences ,  0.5
, favor  0.0005614823133071309
favor  1.0
favor a  0.5
more dynamic  0.010526315789473684
dynamic  1.0
dynamic study  0.2
study of  0.25
of oral  0.00089126559714795
oral  1.0
oral talk-in-interaction  1.0
talk-in-interaction  1.0
talk-in-interaction .  1.0
Mention must  1.0
must also  0.07142857142857142
made of  0.1875
term ``  0.1111111111111111
`` Conversational  0.005291005291005291
Conversational  1.0
Conversational analysis  1.0
analysis ''  0.015384615384615385
was influenced  0.012987012987012988
the Sociologist  0.0006920415224913495
Sociologist  1.0
Sociologist Harold  1.0
Harold  1.0
Harold Garfinkel  1.0
Garfinkel  1.0
Garfinkel who  1.0
the founder  0.0006920415224913495
founder  1.0
founder of  1.0
of Ethnomethodology  0.00089126559714795
Ethnomethodology  1.0
Ethnomethodology .  1.0
In Europe  0.01904761904761905
Europe  1.0
Europe ,  0.6
, Michel  0.0011229646266142617
Foucault became  0.3333333333333333
became one  0.2
the key  0.0006920415224913495
key  1.0
key theorists  0.16666666666666666
theorists  1.0
theorists of  1.0
subject ,  0.25
especially of  0.06666666666666667
and wrote  0.001445086705202312
wrote The  0.16666666666666666
The Archaeology  0.005208333333333333
Archaeology  1.0
Archaeology of  1.0
of Knowledge  0.00089126559714795
Knowledge  0.5
Knowledge on  0.5
Topics of  1.0
of interest  0.00267379679144385
interest Topics  0.09090909090909091
Topics  0.5
analysis include  0.015384615384615385
The various  0.005208333333333333
various levels  0.05555555555555555
levels  1.0
levels or  0.045454545454545456
or dimensions  0.0045045045045045045
as sounds  0.003484320557491289
sounds -LRB-  0.13333333333333333
-LRB- intonation  0.0027100271002710027
intonation  1.0
intonation ,  1.0
, gestures  0.0005614823133071309
gestures  1.0
gestures ,  0.5
the lexicon  0.0006920415224913495
lexicon ,  0.1111111111111111
, style  0.0005614823133071309
style  1.0
style ,  0.5
, rhetoric  0.0005614823133071309
rhetoric  1.0
rhetoric ,  1.0
, meanings  0.0005614823133071309
meanings ,  0.25
acts ,  0.3333333333333333
, moves  0.0005614823133071309
moves  1.0
moves ,  1.0
, strategies  0.0005614823133071309
strategies  1.0
strategies ,  0.5
, turns  0.0005614823133071309
turns  1.0
turns and  0.3333333333333333
and other  0.01300578034682081
other aspects  0.014285714285714285
of interaction  0.00089126559714795
interaction Genres  0.125
Genres  1.0
Genres of  1.0
discourse -LRB-  0.05555555555555555
-LRB- various  0.0027100271002710027
various types  0.1111111111111111
discourse in  0.05555555555555555
in politics  0.0018726591760299626
politics  1.0
politics ,  1.0
the media  0.0006920415224913495
, education  0.0005614823133071309
education  1.0
education ,  1.0
, science  0.0005614823133071309
, business  0.0005614823133071309
business  1.0
business ,  0.25
The relations  0.026041666666666668
relations between  0.4166666666666667
discourse and  0.1111111111111111
the emergence  0.0006920415224913495
emergence  1.0
emergence of  1.0
of syntactic  0.0017825311942959
syntactic structure  0.07692307692307693
structure The  0.08333333333333333
between text  0.02564102564102564
-LRB- discourse  0.0027100271002710027
discourse -RRB-  0.027777777777777776
and context  0.004335260115606936
context The  0.06060606060606061
and power  0.001445086705202312
power The  0.25
and interaction  0.001445086705202312
interaction The  0.125
and cognition  0.001445086705202312
cognition  1.0
cognition and  1.0
and memory  0.001445086705202312
memory  1.0
memory Political  0.5
Political  0.6666666666666666
Political discourse  1.0
discourse Political  0.027777777777777776
analysis is  0.015384615384615385
analysis which  0.015384615384615385
which focuses  0.007246376811594203
focuses  1.0
focuses on  1.0
on discourse  0.0047169811320754715
in political  0.0018726591760299626
political  1.0
political forums  0.3333333333333333
forums  1.0
forums -LRB-  1.0
as debates  0.003484320557491289
debates  1.0
debates ,  1.0
, speeches  0.0005614823133071309
speeches  1.0
speeches ,  1.0
and hearings  0.001445086705202312
hearings  1.0
hearings -RRB-  1.0
the phenomenon  0.001384083044982699
phenomenon  1.0
phenomenon of  0.6
interest .  0.09090909090909091
discourse is  0.08333333333333333
the informal  0.0006920415224913495
informal  1.0
informal exchange  0.5
exchange  1.0
exchange of  1.0
of reasoned  0.00089126559714795
reasoned  1.0
reasoned views  1.0
views  1.0
views as  1.0
as to  0.013937282229965157
to which  0.006640106241699867
which of  0.007246376811594203
of several  0.00267379679144385
several alternative  0.045454545454545456
alternative  1.0
alternative courses  0.3333333333333333
courses  1.0
courses of  1.0
of action  0.00089126559714795
action  1.0
action should  0.2
be taken  0.004219409282700422
taken to  0.3333333333333333
solve a  0.25
a societal  0.001226993865030675
societal  1.0
societal problem  1.0
science that  0.1
been used  0.07352941176470588
used through  0.008849557522123894
the history  0.0006920415224913495
States .  0.2857142857142857
the essence  0.001384083044982699
essence  1.0
essence of  1.0
of democracy  0.00089126559714795
democracy  1.0
democracy .  1.0
Full of  1.0
problems and  0.11764705882352941
and persuasion  0.001445086705202312
persuasion  1.0
persuasion ,  1.0
, political  0.0005614823133071309
political discourse  0.3333333333333333
in many  0.0149812734082397
many debates  0.019230769230769232
, candidacies  0.0005614823133071309
candidacies  1.0
candidacies and  1.0
in our  0.0018726591760299626
our  1.0
our everyday  0.2
everyday  1.0
everyday life  1.0
life .  0.25
Perspectives The  1.0
following are  0.06666666666666667
are some  0.004149377593360996
specific theoretical  0.047619047619047616
theoretical perspectives  0.3333333333333333
perspectives  1.0
perspectives and  1.0
and analytical  0.001445086705202312
analytical  1.0
analytical approaches  0.5
approaches used  0.03571428571428571
linguistic discourse  0.0625
: Emergent  0.00980392156862745
Emergent  1.0
Emergent grammar  1.0
grammar Text  0.02702702702702703
Text grammar  0.16666666666666666
grammar -LRB-  0.02702702702702703
or `  0.0045045045045045045
` discourse  0.0625
discourse grammar  0.027777777777777776
grammar '  0.02702702702702703
' -RRB-  0.05263157894736842
-RRB- Cohesion  0.0028169014084507044
Cohesion  1.0
Cohesion and  1.0
and relevance  0.001445086705202312
relevance  1.0
relevance theory  0.3333333333333333
theory Functional  0.07692307692307693
Functional  1.0
Functional grammar  1.0
grammar Rhetoric  0.02702702702702703
Rhetoric  1.0
Rhetoric Stylistics  1.0
Stylistics  1.0
Stylistics -LRB-  1.0
-LRB- linguistics  0.005420054200542005
linguistics -RRB-  0.1
-RRB- Interactional  0.0028169014084507044
Interactional  1.0
Interactional sociolinguistics  1.0
sociolinguistics Ethnography  0.5
Ethnography  1.0
Ethnography of  1.0
of communication  0.0017825311942959
communication Pragmatics  0.2
Pragmatics  1.0
Pragmatics ,  1.0
, particularly  0.0011229646266142617
particularly speech  0.2
speech act  0.006578947368421052
act  1.0
act theory  0.25
theory Conversation  0.07692307692307693
Conversation  1.0
Conversation analysis  1.0
analysis Variation  0.015384615384615385
Variation  1.0
Variation analysis  1.0
analysis Applied  0.015384615384615385
Applied  1.0
Applied linguistics  0.5
linguistics Cognitive  0.05
Cognitive  1.0
Cognitive psychology  0.3333333333333333
often under  0.022727272727272728
under the  0.2
the label  0.0006920415224913495
label  1.0
label discourse  1.0
discourse processing  0.027777777777777776
, studying  0.0005614823133071309
studying  1.0
studying the  1.0
the production  0.0006920415224913495
production  1.0
production and  0.3333333333333333
and comprehension  0.001445086705202312
comprehension  1.0
comprehension of  0.2857142857142857
discourse .  0.027777777777777776
Discursive psychology  1.0
psychology Response  0.25
Response  1.0
Response based  1.0
based therapy  0.018518518518518517
therapy  1.0
therapy -LRB-  1.0
-LRB- counselling  0.0027100271002710027
counselling  1.0
counselling -RRB-  1.0
-RRB- Critical  0.0028169014084507044
Critical  1.0
Critical discourse  0.5
analysis Sublanguage  0.015384615384615385
Sublanguage  1.0
Sublanguage analysis  1.0
analysis Genre  0.015384615384615385
Genre  1.0
Genre Analysis  1.0
Analysis &  0.2
& Critical  0.125
Critical Genre  0.5
Analysis Although  0.2
Although  0.125
Although these  0.125
approaches emphasize  0.03571428571428571
emphasize  1.0
emphasize different  1.0
different aspects  0.02040816326530612
they all  0.05
all view  0.023255813953488372
view language  0.3333333333333333
language as  0.006756756756756757
as social  0.003484320557491289
social interaction  0.07142857142857142
interaction ,  0.125
and are  0.0072254335260115606
are concerned  0.004149377593360996
social contexts  0.07142857142857142
contexts  1.0
contexts in  0.14285714285714285
in which  0.0149812734082397
which discourse  0.007246376811594203
is embedded  0.0020325203252032522
embedded .  0.25
Often a  0.3333333333333333
a distinction  0.001226993865030675
distinction  1.0
distinction is  0.4
is made  0.0040650406504065045
made between  0.0625
between `  0.02564102564102564
` local  0.0625
local  1.0
local '  0.3333333333333333
' structures  0.10526315789473684
structures of  0.2
as relations  0.003484320557491289
among sentences  0.125
and turns  0.001445086705202312
turns -RRB-  0.3333333333333333
and `  0.001445086705202312
` global  0.0625
global  1.0
global '  0.3333333333333333
structures ,  0.2
as overall  0.003484320557491289
overall topics  0.16666666666666666
topics  1.0
topics and  0.14285714285714285
the schematic  0.0006920415224913495
schematic  1.0
schematic organization  1.0
organization of  0.4
of discourses  0.00089126559714795
discourses and  0.5
and conversations  0.001445086705202312
conversations  1.0
conversations .  0.3333333333333333
many types  0.019230769230769232
discourse begin  0.027777777777777776
begin  1.0
begin with  0.6666666666666666
some kind  0.04819277108433735
of global  0.00089126559714795
global `  0.3333333333333333
` summary  0.0625
summary '  0.023809523809523808
in titles  0.0018726591760299626
titles  1.0
titles ,  0.5
, headlines  0.0005614823133071309
headlines  1.0
headlines ,  1.0
, leads  0.0005614823133071309
leads  1.0
leads ,  1.0
, abstracts  0.0005614823133071309
abstracts  1.0
abstracts ,  0.5
and so  0.008670520231213872
so on  0.16666666666666666
on .  0.018867924528301886
A problem  0.02
problem for  0.022727272727272728
discourse analyst  0.027777777777777776
analyst  1.0
analyst is  1.0
to decide  0.0026560424966799467
decide  1.0
decide when  0.25
a particular  0.0049079754601227
particular feature  0.07692307692307693
feature is  0.07692307692307693
is relevant  0.0020325203252032522
relevant  1.0
relevant to  0.14285714285714285
the specification  0.001384083044982699
specification  1.0
specification is  0.5
is required  0.0040650406504065045
required  1.0
required .  0.14285714285714285
Are there  1.0
there general  0.025
general principles  0.045454545454545456
principles  1.0
principles which  1.0
will determine  0.02857142857142857
the relevance  0.0006920415224913495
relevance or  0.3333333333333333
or nature  0.0045045045045045045
specification .  0.5
Prominent discourse  1.0
analysts This  0.5
This article  0.015873015873015872
article contains  0.034482758620689655
contains embedded  0.1
embedded lists  0.25
lists  1.0
lists that  1.0
that may  0.0070921985815602835
be poorly  0.004219409282700422
poorly  1.0
poorly defined  1.0
defined ,  0.16666666666666666
, unverified  0.0005614823133071309
unverified  1.0
unverified or  1.0
or indiscriminate  0.0045045045045045045
indiscriminate  1.0
indiscriminate .  1.0
Please help  0.6666666666666666
help  1.0
help to  0.1111111111111111
to clean  0.0013280212483399733
clean  1.0
clean it  0.5
it up  0.008547008547008548
up to  0.22727272727272727
to meet  0.005312084993359893
meet  1.0
meet Wikipedia  0.25
Wikipedia  1.0
Wikipedia 's  0.5
's quality  0.0196078431372549
quality standards  0.1
standards .  0.4
-LRB- May  0.005420054200542005
May  1.0
May 2012  0.5
2012  1.0
2012 -RRB-  1.0
-RRB- Marc  0.0028169014084507044
Marc  1.0
Marc Angenot  1.0
Angenot  1.0
Angenot ,  1.0
Robert de  0.25
de Beaugrande  0.5
Beaugrande  1.0
Beaugrande ,  1.0
, Jan  0.0005614823133071309
Jan  1.0
Jan Blommaert  1.0
Blommaert  1.0
Blommaert ,  1.0
, Adriana  0.0005614823133071309
Adriana  1.0
Adriana Bolivar  1.0
Bolivar  1.0
Bolivar ,  1.0
, Carmen  0.0005614823133071309
Carmen  1.0
Carmen Rosa  1.0
Rosa  1.0
Rosa Caldas-Coulthard  1.0
Caldas-Coulthard  1.0
Caldas-Coulthard ,  1.0
, Robyn  0.0005614823133071309
Robyn  1.0
Robyn Carston  1.0
Carston  1.0
Carston ,  1.0
, Wallace  0.0005614823133071309
Wallace  1.0
Wallace Chafe  1.0
Chafe  1.0
Chafe ,  1.0
, Paul  0.0016844469399213925
Paul  0.8
Paul Chilton  0.2
Chilton  1.0
Chilton ,  1.0
, Guy  0.0005614823133071309
Guy  1.0
Guy Cook  1.0
Cook  1.0
Cook ,  1.0
, Malcolm  0.0005614823133071309
Malcolm  1.0
Malcolm Coulthard  1.0
Coulthard  1.0
Coulthard ,  1.0
James Deese  0.25
Deese  1.0
Deese ,  1.0
Paul Drew  0.2
Drew  1.0
Drew ,  1.0
, John  0.002807411566535654
John Du  0.125
Du  1.0
Du Bois  1.0
Bois  1.0
Bois ,  1.0
, Alessandro  0.0005614823133071309
Alessandro  1.0
Alessandro Duranti  1.0
Duranti  1.0
Duranti ,  1.0
, Brenton  0.0005614823133071309
Brenton  1.0
Brenton D.  1.0
D. Faber  0.2
Faber  1.0
Faber ,  1.0
, Norman  0.0005614823133071309
Norman Fairclough  0.5
Fairclough  1.0
Fairclough ,  1.0
Foucault ,  0.3333333333333333
, Roger  0.0005614823133071309
Roger  1.0
Roger Fowler  0.25
Fowler  1.0
Fowler ,  1.0
James Paul  0.25
Paul Gee  0.2
Gee  1.0
Gee ,  1.0
, Talmy  0.0005614823133071309
Talmy  1.0
Talmy Givn  1.0
Givn  1.0
Givn ,  1.0
, Charles  0.0005614823133071309
Charles  1.0
Charles Goodwin  1.0
Goodwin  1.0
Goodwin ,  1.0
, Art  0.0005614823133071309
Art  1.0
Art Graesser  1.0
Graesser  1.0
Graesser ,  1.0
, Michael  0.0022459292532285235
Michael  1.0
Michael Halliday  0.25
Halliday  1.0
Halliday ,  1.0
, Zellig  0.0011229646266142617
Harris ,  0.1111111111111111
John Heritage  0.125
Heritage  1.0
Heritage ,  1.0
, Janet  0.0005614823133071309
Janet  1.0
Janet Holmes  0.5
Holmes  1.0
Holmes ,  1.0
, David  0.0016844469399213925
David  1.0
David R.  0.25
R. Howarth  0.16666666666666666
Howarth  1.0
Howarth ,  1.0
Paul Hopper  0.2
Hopper  1.0
Hopper ,  1.0
, Gail  0.0005614823133071309
Gail  1.0
Gail Jefferson  1.0
Jefferson  1.0
Jefferson ,  1.0
, Barbara  0.0005614823133071309
Barbara  1.0
Barbara Johnstone  1.0
Johnstone  1.0
Johnstone ,  1.0
, Walter  0.0005614823133071309
Walter  1.0
Walter Kintsch  1.0
Kintsch  1.0
Kintsch ,  1.0
, Richard  0.0005614823133071309
Richard  1.0
Richard Kittredge  1.0
Kittredge ,  0.5
, Adam  0.0005614823133071309
Adam  1.0
Adam Jaworski  1.0
Jaworski  1.0
Jaworski ,  1.0
, William  0.0011229646266142617
William  1.0
William Labov  0.5
Labov  1.0
Labov ,  1.0
, George  0.0005614823133071309
George  1.0
George Lakoff  1.0
Lakoff  1.0
Lakoff ,  1.0
, Jay  0.0005614823133071309
Jay  1.0
Jay Lemke  1.0
Lemke  1.0
Lemke ,  1.0
, Stephen  0.0005614823133071309
Stephen  1.0
Stephen H.  1.0
H.  1.0
H. Levinsohn  0.5
Levinsohn  1.0
Levinsohn ,  1.0
, Jim  0.0005614823133071309
Jim  1.0
Jim Martin  1.0
Martin  1.0
Martin ,  0.5
, Aletta  0.0005614823133071309
Aletta  1.0
Aletta Norval  1.0
Norval  1.0
Norval ,  1.0
David Nunan  0.25
Nunan  1.0
Nunan ,  1.0
, Elinor  0.0005614823133071309
Elinor  1.0
Elinor Ochs  1.0
Ochs  1.0
Ochs ,  1.0
, Gina  0.0005614823133071309
Gina  1.0
Gina Poncini  1.0
Poncini  1.0
Poncini ,  1.0
, Jonathan  0.0005614823133071309
Jonathan  1.0
Jonathan Potter  1.0
Potter  1.0
Potter ,  1.0
, Edward  0.0005614823133071309
Edward  1.0
Edward Robinson  1.0
Robinson  1.0
Robinson ,  1.0
, Nikolas  0.0005614823133071309
Nikolas  1.0
Nikolas Rose  1.0
Rose  1.0
Rose ,  1.0
, Harvey  0.0005614823133071309
Harvey  1.0
Harvey Sacks  1.0
Sacks  1.0
Sacks ,  1.0
, Svenka  0.0005614823133071309
Svenka  1.0
Svenka Savic  1.0
Savic  1.0
Savic Naomi  1.0
Sager ,  0.5
, Emanuel  0.0011229646266142617
Emanuel  1.0
Emanuel Schegloff  0.5
Schegloff  1.0
Schegloff ,  1.0
, Deborah  0.0011229646266142617
Deborah  1.0
Deborah Schiffrin  0.5
Schiffrin  1.0
Schiffrin ,  1.0
Michael Schober  0.25
Schober  1.0
Schober ,  1.0
, Stef  0.0005614823133071309
Stef  1.0
Stef Slembrouck  1.0
Slembrouck  1.0
Slembrouck ,  1.0
Michael Stubbs  0.25
Stubbs  1.0
Stubbs ,  1.0
John Swales  0.25
Swales  1.0
Swales ,  1.0
Deborah Tannen  0.5
Tannen  1.0
Tannen ,  1.0
, Sandra  0.0005614823133071309
Sandra  1.0
Sandra Thompson  1.0
Thompson  1.0
Thompson ,  1.0
, Teun  0.0005614823133071309
Teun  1.0
Teun A.  1.0
A. van  0.2
van  1.0
van Dijk  0.5
Dijk  1.0
Dijk ,  1.0
, Theo  0.0005614823133071309
Theo  1.0
Theo van  1.0
van Leeuwen  0.5
Leeuwen  1.0
Leeuwen ,  1.0
, Jef  0.0005614823133071309
Jef  1.0
Jef Verschueren  1.0
Verschueren  1.0
Verschueren ,  1.0
, Henry  0.0005614823133071309
Henry  1.0
Henry Widdowson  0.5
Widdowson  1.0
Widdowson ,  1.0
, Carla  0.0005614823133071309
Carla  1.0
Carla Willig  1.0
Willig  1.0
Willig ,  1.0
, Deirdre  0.0005614823133071309
Deirdre  1.0
Deirdre Wilson  1.0
Wilson  1.0
Wilson ,  1.0
, Ruth  0.0005614823133071309
Ruth  1.0
Ruth Wodak  1.0
Wodak  1.0
Wodak ,  1.0
, Margaret  0.0005614823133071309
Margaret  1.0
Margaret Wetherell  1.0
Wetherell  1.0
Wetherell ,  1.0
, Ernesto  0.0005614823133071309
Ernesto  1.0
Ernesto Laclau  1.0
Laclau  1.0
Laclau ,  1.0
, Chantal  0.0005614823133071309
Chantal  1.0
Chantal Mouffe  1.0
Mouffe  1.0
Mouffe ,  1.0
, Judith  0.0005614823133071309
Judith  1.0
Judith M.  1.0
M. De  0.25
De  1.0
De Guzman  1.0
Guzman  1.0
Guzman ,  1.0
, Cynthia  0.0005614823133071309
Cynthia  1.0
Cynthia Hardy  1.0
Hardy  1.0
Hardy ,  1.0
, Louise  0.0005614823133071309
Louise  1.0
Louise J.  1.0
J. Phillips  0.3333333333333333
Phillips  1.0
Phillips .  1.0
-RRB- Bhatia  0.0028169014084507044
Bhatia  1.0
Bhatia ,  1.0
, V.J.  0.0005614823133071309
V.J.  1.0
V.J. ,  1.0
Harris The  0.1111111111111111
The phenomenon  0.005208333333333333
of information  0.004456327985739751
information overload  0.021739130434782608
overload  1.0
overload has  1.0
has meant  0.011904761904761904
meant  1.0
meant that  0.5
that access  0.0035460992907801418
access  1.0
access to  1.0
to coherent  0.0013280212483399733
coherent and  0.2
and correctly-developed  0.001445086705202312
correctly-developed  1.0
correctly-developed summaries  1.0
summaries is  0.06976744186046512
is vital  0.0020325203252032522
vital  1.0
vital .  1.0
As access  0.05555555555555555
to data  0.0026560424966799467
data has  0.012987012987012988
has increased  0.011904761904761904
increased so  0.2
so has  0.03333333333333333
has interest  0.011904761904761904
interest in  0.6363636363636364
automatic summarization  0.08695652173913043
summarization .  0.12
of summarization  0.0071301247771836
summarization technology  0.02
technology is  0.13636363636363635
is search  0.0020325203252032522
search engines  0.18181818181818182
engines  1.0
engines such  0.3333333333333333
as Google  0.003484320557491289
Google  0.75
Google .  0.5
Technologies that  1.0
make a  0.2
coherent summary  0.2
summary ,  0.14285714285714285
any kind  0.03225806451612903
, need  0.0005614823133071309
need to  0.47619047619047616
take into  0.3
into account  0.038461538461538464
account  1.0
account several  0.3333333333333333
several variables  0.045454545454545456
variables  1.0
variables such  1.0
as length  0.003484320557491289
length  1.0
length ,  0.25
writing style  0.1111111111111111
style and  0.5
and syntax  0.001445086705202312
syntax to  0.18181818181818182
to make  0.005312084993359893
a useful  0.001226993865030675
useful summary  0.07142857142857142
summary .  0.2619047619047619
Extractive methods  1.0
methods work  0.045454545454545456
work by  0.08333333333333333
by selecting  0.011428571428571429
selecting  1.0
selecting a  0.4
a subset  0.0036809815950920245
subset  1.0
subset of  1.0
existing words  0.2
, phrases  0.0005614823133071309
phrases ,  0.125
or sentences  0.0045045045045045045
sentences in  0.10526315789473684
form the  0.05
the summary  0.005536332179930796
In contrast  0.047619047619047616
contrast ,  0.625
, abstractive  0.0005614823133071309
abstractive  1.0
abstractive methods  0.3333333333333333
methods build  0.022727272727272728
build  1.0
build an  0.6666666666666666
an internal  0.022727272727272728
internal  1.0
internal semantic  0.2
semantic representation  0.047619047619047616
representation and  0.10526315789473684
then use  0.02857142857142857
use natural  0.013888888888888888
generation techniques  0.1111111111111111
techniques to  0.17391304347826086
to create  0.01195219123505976
create a  0.4117647058823529
a summary  0.0098159509202454
summary that  0.047619047619047616
is closer  0.0020325203252032522
what a  0.09375
human might  0.021739130434782608
might generate  0.038461538461538464
generate .  0.05555555555555555
Such a  0.125
summary might  0.023809523809523808
might contain  0.038461538461538464
contain  1.0
contain words  0.08333333333333333
words not  0.009174311926605505
not explicitly  0.008928571428571428
explicitly  1.0
explicitly present  0.25
present  1.0
present in  0.8333333333333334
original .  0.07692307692307693
The state-of-the-art  0.005208333333333333
state-of-the-art  1.0
state-of-the-art abstractive  0.5
methods are  0.045454545454545456
still quite  0.06666666666666667
quite weak  0.125
weak  1.0
weak ,  1.0
so most  0.03333333333333333
most research  0.017241379310344827
on extractive  0.0047169811320754715
extractive  1.0
extractive methods  0.14285714285714285
methods ,  0.09090909090909091
and this  0.001445086705202312
is what  0.0040650406504065045
what we  0.09375
we will  0.08888888888888889
will cover  0.02857142857142857
cover  1.0
cover .  1.0
Two particular  0.14285714285714285
particular types  0.07692307692307693
summarization often  0.02
often addressed  0.022727272727272728
addressed  1.0
addressed in  0.5
the literature  0.0006920415224913495
literature  1.0
literature are  1.0
are keyphrase  0.004149377593360996
keyphrase  1.0
keyphrase extraction  0.631578947368421
the goal  0.002768166089965398
goal is  0.42857142857142855
select individual  0.16666666666666666
individual words  0.08333333333333333
or phrases  0.009009009009009009
phrases to  0.0625
`` tag  0.010582010582010581
tag ''  0.0625
a document  0.008588957055214725
document  1.0
document ,  0.1388888888888889
and document  0.002890173410404624
document summarization  0.1388888888888889
select whole  0.16666666666666666
whole sentences  0.2222222222222222
sentences to  0.07894736842105263
a short  0.006134969325153374
short  1.0
short paragraph  0.125
paragraph  1.0
paragraph summary  0.3333333333333333
Extraction and  0.3333333333333333
and abstraction  0.002890173410404624
abstraction  1.0
abstraction Broadly  0.25
Broadly  1.0
Broadly ,  1.0
, one  0.003368893879842785
one distinguishes  0.015384615384615385
distinguishes two  0.5
two approaches  0.034482758620689655
approaches :  0.14285714285714285
: extraction  0.00980392156862745
extraction and  0.06451612903225806
abstraction .  0.25
Extraction techniques  0.3333333333333333
techniques merely  0.043478260869565216
merely  1.0
merely copy  0.5
copy  1.0
copy the  1.0
the information  0.0034602076124567475
information deemed  0.021739130434782608
deemed  1.0
deemed most  0.5
important by  0.0625
system to  0.053763440860215055
summary -LRB-  0.023809523809523808
, key  0.0005614823133071309
key clauses  0.16666666666666666
clauses  1.0
clauses ,  1.0
, sentences  0.0011229646266142617
sentences or  0.013157894736842105
or paragraphs  0.009009009009009009
paragraphs  1.0
paragraphs -RRB-  0.25
while abstraction  0.05
abstraction involves  0.25
involves paraphrasing  0.1
paraphrasing  1.0
paraphrasing sections  1.0
sections  1.0
sections of  1.0
source document  0.08333333333333333
document .  0.1388888888888889
In general  0.02857142857142857
general ,  0.2727272727272727
, abstraction  0.0005614823133071309
abstraction can  0.25
can condense  0.0055248618784530384
condense  1.0
condense a  1.0
text more  0.006289308176100629
more strongly  0.010526315789473684
strongly  1.0
strongly than  0.5
than extraction  0.022222222222222223
the programs  0.0006920415224913495
programs that  0.09090909090909091
can do  0.011049723756906077
do this  0.07692307692307693
this are  0.02197802197802198
are harder  0.004149377593360996
harder to  0.2857142857142857
develop as  0.2
they require  0.05
generation technology  0.1111111111111111
technology ,  0.13636363636363635
which itself  0.007246376811594203
itself is  0.2
a growing  0.001226993865030675
field .  0.14814814814814814
Types of  1.0
of summaries  0.0035650623885918
summaries There  0.023255813953488372
There  0.18181818181818182
are different  0.004149377593360996
summaries depending  0.046511627906976744
depending  1.0
depending what  0.25
the summarization  0.0020761245674740486
summarization program  0.02
program focuses  0.045454545454545456
on to  0.009433962264150943
make the  0.05
example generic  0.012345679012345678
generic summaries  0.3333333333333333
summaries or  0.023255813953488372
or query  0.0045045045045045045
query  1.0
query relevant  0.6666666666666666
relevant summaries  0.14285714285714285
summaries -LRB-  0.046511627906976744
-LRB- sometimes  0.0027100271002710027
sometimes called  0.07692307692307693
called query-biased  0.05555555555555555
query-biased  1.0
query-biased summaries  1.0
summaries -RRB-  0.023255813953488372
Summarization systems  0.25
create both  0.058823529411764705
both query  0.03225806451612903
relevant text  0.14285714285714285
text summaries  0.006289308176100629
summaries and  0.046511627906976744
and generic  0.001445086705202312
generic machine-generated  0.3333333333333333
machine-generated  1.0
machine-generated summaries  1.0
depending on  0.75
on what  0.009433962264150943
user needs  0.07142857142857142
needs .  0.1
Summarization of  0.25
of multimedia  0.00089126559714795
multimedia  1.0
multimedia documents  0.5
e.g. pictures  0.017857142857142856
pictures  1.0
pictures or  1.0
or movies  0.0045045045045045045
movies  1.0
movies ,  1.0
also possible  0.043478260869565216
possible .  0.125
Some systems  0.09523809523809523
systems will  0.008928571428571428
will generate  0.08571428571428572
generate a  0.3333333333333333
summary based  0.023809523809523808
a single  0.011042944785276074
single  1.0
single source  0.14285714285714285
others can  0.08333333333333333
use multiple  0.013888888888888888
multiple source  0.07692307692307693
source documents  0.125
a cluster  0.00245398773006135
cluster  1.0
cluster of  1.0
of news  0.0017825311942959
news  1.0
news stories  0.07692307692307693
stories  1.0
stories on  1.0
same topic  0.04
topic -RRB-  0.125
are known  0.012448132780082987
as multi-document  0.003484320557491289
multi-document  1.0
multi-document summarization  0.75
summarization systems  0.1
Keyphrase extraction  0.5
extraction Task  0.03225806451612903
Task  0.6666666666666666
Task description  0.3333333333333333
description  1.0
description and  1.0
and example  0.001445086705202312
example The  0.012345679012345678
The task  0.020833333333333332
the following  0.004844290657439446
following .  0.13333333333333333
You are  1.0
are given  0.012448132780082987
a piece  0.00245398773006135
piece  1.0
piece of  1.0
a journal  0.001226993865030675
journal article  0.3333333333333333
article ,  0.10344827586206896
and you  0.004335260115606936
you must  0.07692307692307693
must produce  0.07142857142857142
of keywords  0.00089126559714795
keywords  1.0
keywords or  0.5
or keyphrases  0.0045045045045045045
keyphrases  1.0
keyphrases that  0.08571428571428572
that capture  0.0035460992907801418
capture  1.0
capture the  0.5
the primary  0.001384083044982699
primary  1.0
primary topics  0.5
topics discussed  0.14285714285714285
discussed in  0.14285714285714285
the case  0.005536332179930796
case of  0.35294117647058826
research articles  0.023809523809523808
articles ,  0.125
many authors  0.019230769230769232
authors provide  0.2
provide manually  0.16666666666666666
manually  1.0
manually assigned  0.25
assigned  1.0
assigned keywords  0.5
keywords ,  0.5
most text  0.017241379310344827
text lacks  0.006289308176100629
lacks  1.0
lacks pre-existing  1.0
pre-existing  1.0
pre-existing keyphrases  0.5
keyphrases .  0.3142857142857143
, news  0.0005614823133071309
news articles  0.23076923076923078
articles rarely  0.125
rarely  1.0
rarely have  0.3333333333333333
have keyphrases  0.019230769230769232
keyphrases attached  0.02857142857142857
attached ,  0.5
it would  0.03418803418803419
be useful  0.012658227848101266
useful to  0.14285714285714285
automatically do  0.047619047619047616
do so  0.038461538461538464
so for  0.03333333333333333
of applications  0.00267379679144385
applications discussed  0.04
discussed below  0.42857142857142855
below .  0.4
example text  0.012345679012345678
a recent  0.001226993865030675
recent  1.0
recent news  0.125
news article  0.15384615384615385
: ``  0.0196078431372549
`` The  0.015873015873015872
The Army  0.005208333333333333
Army  1.0
Army Corps  0.5
Corps  1.0
Corps of  1.0
of Engineers  0.0017825311942959
Engineers  1.0
Engineers ,  0.5
, rushing  0.0005614823133071309
rushing  1.0
rushing to  1.0
meet President  0.25
President  1.0
President Bush  0.5
Bush  1.0
Bush 's  0.5
's promise  0.0196078431372549
promise  1.0
promise to  1.0
to protect  0.0013280212483399733
protect  1.0
protect New  1.0
New  1.0
New Orleans  1.0
Orleans  1.0
Orleans by  0.5
the start  0.002768166089965398
start  1.0
start of  0.2857142857142857
the 2006  0.0006920415224913495
2006 hurricane  0.3333333333333333
hurricane  1.0
hurricane season  1.0
season  1.0
season ,  1.0
, installed  0.0005614823133071309
installed  1.0
installed defective  0.3333333333333333
defective  1.0
defective flood-control  1.0
flood-control  1.0
flood-control pumps  1.0
pumps  1.0
pumps last  0.5
last  1.0
last year  0.2
year despite  0.16666666666666666
despite  1.0
despite warnings  0.3333333333333333
warnings  1.0
warnings from  1.0
from its  0.009615384615384616
own expert  0.16666666666666666
expert  1.0
expert that  1.0
the equipment  0.0006920415224913495
equipment  1.0
equipment would  0.3333333333333333
would fail  0.018867924528301886
fail  1.0
fail during  0.3333333333333333
during a  0.2
a storm  0.001226993865030675
storm  1.0
storm ,  1.0
, according  0.0005614823133071309
according  1.0
according to  1.0
to documents  0.0013280212483399733
documents obtained  0.02631578947368421
obtained  1.0
obtained by  0.5714285714285714
by The  0.005714285714285714
The Associated  0.005208333333333333
Associated  1.0
Associated Press  1.0
Press  1.0
Press ''  1.0
An extractive  0.0625
extractive keyphrase  0.14285714285714285
keyphrase extractor  0.05263157894736842
extractor  1.0
extractor might  0.5
might select  0.038461538461538464
select ``  0.16666666666666666
`` Army  0.005291005291005291
Engineers ''  0.5
`` President  0.005291005291005291
Bush ''  0.5
`` New  0.005291005291005291
Orleans ''  0.5
`` defective  0.005291005291005291
pumps ''  0.5
as keyphrases  0.003484320557491289
These are  0.11764705882352941
are pulled  0.004149377593360996
pulled  1.0
pulled directly  1.0
directly  1.0
directly from  0.2
an abstractive  0.015151515151515152
abstractive keyphrase  0.16666666666666666
keyphrase system  0.05263157894736842
system would  0.021505376344086023
would somehow  0.018867924528301886
somehow  1.0
somehow internalize  1.0
internalize  1.0
internalize the  1.0
the content  0.0020761245674740486
content and  0.16666666666666666
and generate  0.001445086705202312
generate keyphrases  0.05555555555555555
more descriptive  0.010526315789473684
descriptive  1.0
descriptive and  0.3333333333333333
more like  0.010526315789473684
like what  0.03571428571428571
human would  0.021739130434782608
would produce  0.03773584905660377
produce ,  0.045454545454545456
`` political  0.005291005291005291
political negligence  0.3333333333333333
negligence  1.0
negligence ''  1.0
'' or  0.026881720430107527
or ``  0.018018018018018018
`` inadequate  0.005291005291005291
inadequate  1.0
inadequate protection  1.0
protection  1.0
protection from  1.0
from floods  0.009615384615384616
floods  1.0
floods ''  1.0
that these  0.010638297872340425
these terms  0.023809523809523808
terms do  0.07692307692307693
not appear  0.008928571428571428
text and  0.018867924528301886
and require  0.004335260115606936
a deep  0.001226993865030675
deep understanding  0.2857142857142857
understanding ,  0.09090909090909091
makes it  0.25
it difficult  0.017094017094017096
difficult for  0.03571428571428571
computer to  0.045454545454545456
produce such  0.045454545454545456
such keyphrases  0.008130081300813009
Keyphrases have  1.0
many applications  0.038461538461538464
to improve  0.01195219123505976
improve document  0.07692307692307693
document browsing  0.027777777777777776
browsing  1.0
browsing by  1.0
by providing  0.005714285714285714
providing a  0.5
short summary  0.125
Also ,  1.0
, keyphrases  0.0005614823133071309
keyphrases can  0.05714285714285714
can improve  0.0055248618784530384
improve information  0.07692307692307693
information retrieval  0.10869565217391304
retrieval --  0.14285714285714285
-- if  0.08
if documents  0.03571428571428571
documents have  0.02631578947368421
keyphrases assigned  0.02857142857142857
assigned ,  0.5
a user  0.00245398773006135
user could  0.07142857142857142
could  1.0
could search  0.0625
search by  0.09090909090909091
by keyphrase  0.005714285714285714
keyphrase to  0.05263157894736842
reliable hits  0.25
hits  1.0
hits than  1.0
than a  0.1111111111111111
a full-text  0.001226993865030675
full-text  1.0
full-text search  1.0
search .  0.09090909090909091
automatic keyphrase  0.043478260869565216
extraction can  0.03225806451612903
useful in  0.14285714285714285
in generating  0.0018726591760299626
generating  1.0
generating index  0.2
index  1.0
index entries  1.0
entries for  0.5
large text  0.043478260869565216
text corpus  0.012578616352201259
corpus .  0.03225806451612903
extraction as  0.06451612903225806
as supervised  0.003484320557491289
learning Beginning  0.023255813953488372
Beginning  0.5
Beginning with  0.5
the Turney  0.0006920415224913495
Turney  0.8888888888888888
Turney paper  0.2222222222222222
paper ,  0.09090909090909091
many researchers  0.019230769230769232
researchers  1.0
researchers have  0.3
have approached  0.009615384615384616
approached keyphrase  0.5
a supervised  0.00245398773006135
supervised machine  0.0625
learning problem  0.023255813953488372
we construct  0.022222222222222223
construct  1.0
construct an  0.3333333333333333
example for  0.024691358024691357
each unigram  0.044444444444444446
unigram  1.0
unigram ,  0.6
, bigram  0.0016844469399213925
bigram  1.0
bigram ,  1.0
and trigram  0.001445086705202312
trigram  1.0
trigram found  0.3333333333333333
found in  0.21428571428571427
-LRB- though  0.0027100271002710027
though  1.0
though other  0.1
other text  0.02857142857142857
text units  0.018867924528301886
units  1.0
units are  0.2857142857142857
as discussed  0.003484320557491289
We then  0.14285714285714285
then compute  0.02857142857142857
compute  1.0
compute various  0.5
various features  0.05555555555555555
features describing  0.038461538461538464
describing  1.0
describing each  0.25
each example  0.044444444444444446
example -LRB-  0.012345679012345678
, does  0.0011229646266142617
does the  0.1
the phrase  0.002768166089965398
phrase begin  0.1
with an  0.0273224043715847
an upper-case  0.007575757575757576
upper-case  1.0
upper-case letter  1.0
letter  1.0
letter ?  0.16666666666666666
We assume  0.14285714285714285
assume there  0.5
known keyphrases  0.15384615384615385
keyphrases available  0.02857142857142857
available for  0.11764705882352941
training documents  0.10714285714285714
documents .  0.13157894736842105
Using the  0.5
the known  0.0034602076124567475
keyphrases ,  0.05714285714285714
we can  0.06666666666666667
can assign  0.0055248618784530384
assign  1.0
assign positive  0.2
positive  1.0
positive or  0.2857142857142857
or negative  0.009009009009009009
negative  1.0
negative labels  0.125
labels  1.0
labels to  1.0
the examples  0.0020761245674740486
Then we  0.2
we learn  0.022222222222222223
learn a  0.23076923076923078
a classifier  0.001226993865030675
classifier  1.0
classifier that  0.14285714285714285
can discriminate  0.0055248618784530384
discriminate  1.0
discriminate between  0.3333333333333333
between positive  0.02564102564102564
positive and  0.2857142857142857
and negative  0.002890173410404624
negative examples  0.125
examples as  0.041666666666666664
a function  0.001226993865030675
function of  0.125
features .  0.07692307692307693
Some classifiers  0.047619047619047616
classifiers  1.0
classifiers make  0.5
a binary  0.00245398773006135
binary  1.0
binary classification  0.5
classification  1.0
classification for  0.11764705882352941
a test  0.0036809815950920245
test example  0.1
others assign  0.08333333333333333
assign a  0.4
a probability  0.001226993865030675
probability  1.0
probability of  0.14285714285714285
of being  0.00089126559714795
being a  0.1111111111111111
a keyphrase  0.00245398773006135
keyphrase .  0.05263157894736842
the above  0.001384083044982699
above text  0.07692307692307693
we might  0.022222222222222223
might learn  0.038461538461538464
a rule  0.001226993865030675
rule  1.0
rule that  0.3333333333333333
that says  0.0035460992907801418
says  1.0
says phrases  1.0
phrases with  0.0625
with initial  0.00546448087431694
initial  1.0
initial capital  0.3333333333333333
capital letters  0.3333333333333333
letters are  0.1
are likely  0.016597510373443983
likely to  0.4375
be keyphrases  0.004219409282700422
After training  0.3333333333333333
training a  0.03571428571428571
a learner  0.001226993865030675
learner  1.0
learner ,  0.5
can select  0.0055248618784530384
select keyphrases  0.16666666666666666
keyphrases for  0.05714285714285714
for test  0.0036101083032490976
test documents  0.2
documents in  0.02631578947368421
following manner  0.06666666666666667
manner  1.0
manner .  0.75
We apply  0.14285714285714285
apply the  0.2
same example-generation  0.04
example-generation  1.0
example-generation strategy  1.0
strategy  1.0
strategy to  0.6
the test  0.001384083044982699
then run  0.02857142857142857
run each  0.2
example through  0.012345679012345678
the learner  0.0006920415224913495
learner .  0.5
We can  0.2857142857142857
can determine  0.011049723756906077
the keyphrases  0.001384083044982699
keyphrases by  0.02857142857142857
by looking  0.005714285714285714
looking  1.0
looking at  0.2
at binary  0.014705882352941176
classification decisions  0.058823529411764705
decisions or  0.1
or probabilities  0.0045045045045045045
probabilities  1.0
probabilities returned  0.09090909090909091
returned  1.0
returned from  0.5
from our  0.009615384615384616
our learned  0.2
learned model  0.2
model .  0.06666666666666667
If probabilities  0.1
probabilities are  0.09090909090909091
given ,  0.041666666666666664
a threshold  0.0036809815950920245
threshold  1.0
threshold is  0.25
Keyphrase extractors  0.25
extractors  1.0
extractors are  1.0
generally evaluated  0.09090909090909091
evaluated using  0.14285714285714285
using precision  0.01694915254237288
precision  1.0
precision and  0.4
and recall  0.002890173410404624
recall  1.0
recall .  0.6666666666666666
Precision measures  1.0
measures  1.0
measures how  0.3333333333333333
how many  0.10344827586206896
many of  0.038461538461538464
the proposed  0.001384083044982699
proposed keyphrases  0.2222222222222222
keyphrases are  0.05714285714285714
are actually  0.004149377593360996
actually  1.0
actually correct  0.3333333333333333
correct .  0.2
Recall measures  0.3333333333333333
the true  0.0006920415224913495
true  1.0
true keyphrases  0.5
keyphrases your  0.02857142857142857
your system  0.5
system proposed  0.010752688172043012
proposed .  0.1111111111111111
The two  0.010416666666666666
two measures  0.034482758620689655
measures can  0.3333333333333333
be combined  0.004219409282700422
combined  1.0
combined in  0.5
an F-score  0.007575757575757576
F-score  1.0
F-score ,  1.0
the harmonic  0.0006920415224913495
harmonic  1.0
harmonic mean  1.0
mean  1.0
mean of  0.5
two -LRB-  0.034482758620689655
-LRB- F  0.0027100271002710027
F  1.0
F =  1.0
=  1.0
= 2PR  0.1111111111111111
2PR  1.0
2PR \/  1.0
\/ -LRB-  0.3333333333333333
-LRB- P  0.0027100271002710027
P  1.0
P +  0.5
+  1.0
+ R  0.16666666666666666
R  1.0
R -RRB-  1.0
Matches between  1.0
keyphrases and  0.02857142857142857
be checked  0.004219409282700422
checked  1.0
checked after  0.5
after stemming  0.08333333333333333
stemming or  0.5
or applying  0.0045045045045045045
applying  1.0
applying some  0.25
text normalization  0.006289308176100629
Design choices  1.0
choices  1.0
choices Designing  0.2
Designing  1.0
Designing a  1.0
supervised keyphrase  0.125
extraction system  0.06451612903225806
system involves  0.010752688172043012
involves deciding  0.1
deciding  1.0
deciding on  0.16666666666666666
on several  0.0047169811320754715
several choices  0.045454545454545456
choices -LRB-  0.2
-LRB- some  0.0027100271002710027
these apply  0.023809523809523808
apply to  0.4
to unsupervised  0.0026560424966799467
unsupervised ,  0.125
, too  0.0005614823133071309
too  1.0
too -RRB-  0.16666666666666666
: What  0.00980392156862745
What are  0.36363636363636365
are the  0.04564315352697095
examples ?  0.041666666666666664
first choice  0.030303030303030304
choice  1.0
choice is  0.25
is exactly  0.0020325203252032522
exactly  1.0
exactly how  0.3333333333333333
generate examples  0.05555555555555555
Turney and  0.2222222222222222
and others  0.002890173410404624
others have  0.08333333333333333
have used  0.019230769230769232
used all  0.008849557522123894
possible unigrams  0.041666666666666664
unigrams  1.0
unigrams ,  0.25
, bigrams  0.0011229646266142617
bigrams  1.0
bigrams ,  1.0
and trigrams  0.002890173410404624
trigrams  1.0
trigrams without  0.5
without intervening  0.07692307692307693
intervening  1.0
intervening punctuation  1.0
punctuation and  0.2857142857142857
after removing  0.08333333333333333
removing  1.0
removing stopwords  0.5
stopwords  1.0
stopwords .  1.0
Hulth showed  0.3333333333333333
showed  1.0
showed that  0.75
that you  0.0035460992907801418
you can  0.15384615384615385
can get  0.0055248618784530384
get some  0.14285714285714285
some improvement  0.012048192771084338
improvement  1.0
improvement by  0.25
selecting examples  0.2
examples to  0.041666666666666664
be sequences  0.004219409282700422
of tokens  0.0017825311942959
tokens  1.0
tokens that  0.14285714285714285
that match  0.0035460992907801418
match  1.0
match certain  0.16666666666666666
certain  1.0
certain patterns  0.14285714285714285
patterns  1.0
patterns of  0.2
of part-of-speech  0.0017825311942959
part-of-speech tags  0.06666666666666667
tags  1.0
tags .  0.3333333333333333
Ideally ,  1.0
the mechanism  0.0006920415224913495
mechanism  1.0
mechanism for  1.0
for generating  0.0036101083032490976
generating examples  0.2
examples produces  0.041666666666666664
produces all  0.25
known labeled  0.038461538461538464
labeled keyphrases  0.3333333333333333
keyphrases as  0.02857142857142857
as candidates  0.003484320557491289
candidates  1.0
candidates ,  0.2
, though  0.003368893879842785
though this  0.1
case .  0.17647058823529413
, if  0.005614823133071308
if we  0.07142857142857142
we use  0.022222222222222223
use only  0.027777777777777776
only unigrams  0.02631578947368421
trigrams ,  0.5
then we  0.05714285714285714
will never  0.02857142857142857
to extract  0.00398406374501992
extract  1.0
extract a  0.25
known keyphrase  0.038461538461538464
keyphrase containing  0.05263157894736842
containing four  0.125
four words  0.14285714285714285
Thus ,  0.9166666666666666
, recall  0.0005614823133071309
recall may  0.3333333333333333
may suffer  0.019230769230769232
suffer  1.0
suffer .  1.0
, generating  0.0005614823133071309
generating too  0.2
too many  0.3333333333333333
many examples  0.019230769230769232
examples can  0.041666666666666664
also lead  0.014492753623188406
lead  1.0
lead to  1.0
to low  0.0013280212483399733
low  1.0
low precision  0.3333333333333333
precision .  0.2
features ?  0.038461538461538464
We also  0.14285714285714285
also need  0.014492753623188406
create features  0.058823529411764705
features that  0.07692307692307693
that describe  0.0035460992907801418
describe  1.0
describe the  0.3333333333333333
are informative  0.004149377593360996
informative enough  0.5
enough to  0.2
to allow  0.00398406374501992
allow  1.0
allow a  0.2
algorithm to  0.07142857142857142
to discriminate  0.0026560424966799467
discriminate keyphrases  0.3333333333333333
keyphrases from  0.02857142857142857
from non  0.009615384615384616
non  1.0
non -  1.0
- keyphrases  0.0625
Typically features  1.0
features involve  0.038461538461538464
involve various  0.16666666666666666
various term  0.05555555555555555
term frequencies  0.05555555555555555
frequencies  1.0
frequencies -LRB-  0.5
-LRB- how  0.008130081300813009
many times  0.019230769230769232
times  1.0
times a  0.2
a phrase  0.00245398773006135
phrase appears  0.1
appears in  0.2
the current  0.001384083044982699
current text  0.14285714285714285
or in  0.0045045045045045045
larger corpus  0.125
the length  0.001384083044982699
length of  0.25
, relative  0.0005614823133071309
relative position  0.3333333333333333
position  1.0
position of  0.25
first occurrence  0.030303030303030304
occurrence  1.0
occurrence ,  0.5
, various  0.0005614823133071309
various boolean  0.05555555555555555
boolean  1.0
boolean syntactic  1.0
syntactic features  0.07692307692307693
features -LRB-  0.038461538461538464
, contains  0.0005614823133071309
contains all  0.1
all caps  0.023255813953488372
caps  1.0
caps -RRB-  1.0
The Turney  0.005208333333333333
paper used  0.09090909090909091
used about  0.008849557522123894
about 12  0.025
12  1.0
12 such  0.2
such features  0.008130081300813009
Hulth uses  0.3333333333333333
uses a  0.2857142857142857
a reduced  0.001226993865030675
reduced set  0.25
of features  0.00089126559714795
features ,  0.038461538461538464
which were  0.014492753623188406
were found  0.04878048780487805
found most  0.07142857142857142
most successful  0.034482758620689655
successful in  0.1111111111111111
the KEA  0.0006920415224913495
KEA  1.0
KEA -LRB-  1.0
-LRB- Keyphrase  0.0027100271002710027
Keyphrase  0.25
Keyphrase Extraction  0.25
Extraction  0.3333333333333333
Extraction Algorithm  0.3333333333333333
Algorithm  1.0
Algorithm -RRB-  1.0
-RRB- work  0.0028169014084507044
work derived  0.041666666666666664
from Turney  0.009615384615384616
Turney 's  0.4444444444444444
's seminal  0.0196078431372549
seminal  1.0
seminal paper  1.0
paper .  0.09090909090909091
How many  0.14285714285714285
many keyphrases  0.019230769230769232
keyphrases to  0.02857142857142857
to return  0.0026560424966799467
return  1.0
return ?  0.5
the end  0.001384083044982699
end ,  0.125
system will  0.010752688172043012
will need  0.02857142857142857
return a  0.5
of keyphrases  0.00267379679144385
test document  0.1
so we  0.03333333333333333
we need  0.13333333333333333
a way  0.006134969325153374
to limit  0.0026560424966799467
limit the  0.5
number .  0.046511627906976744
Ensemble methods  1.0
i.e. ,  0.3684210526315789
, using  0.005614823133071308
using votes  0.01694915254237288
votes  1.0
votes from  1.0
from several  0.009615384615384616
several classifiers  0.045454545454545456
classifiers -RRB-  0.5
-RRB- have  0.005633802816901409
produce numeric  0.045454545454545456
numeric  1.0
numeric scores  1.0
scores  1.0
scores that  0.2
be thresholded  0.004219409282700422
thresholded  1.0
thresholded to  1.0
a user-provided  0.001226993865030675
user-provided  1.0
user-provided number  1.0
the technique  0.0006920415224913495
technique used  0.14285714285714285
used by  0.07964601769911504
by Turney  0.005714285714285714
Turney with  0.1111111111111111
with C4  0.00546448087431694
C4  1.0
C4 .5  1.0
.5  1.0
.5 decision  1.0
trees .  0.3333333333333333
Hulth used  0.3333333333333333
used a  0.02654867256637168
single binary  0.07142857142857142
binary classifier  0.25
classifier so  0.14285714285714285
the learning  0.0006920415224913495
algorithm implicitly  0.03571428571428571
implicitly  1.0
implicitly determines  1.0
determines  1.0
determines the  0.6666666666666666
the appropriate  0.0020761245674740486
appropriate  1.0
appropriate number  0.25
What learning  0.09090909090909091
algorithm ?  0.03571428571428571
Once examples  0.2
and features  0.001445086705202312
features are  0.11538461538461539
are created  0.012448132780082987
created ,  0.2857142857142857
learn to  0.07692307692307693
predict keyphrases  0.16666666666666666
Virtually any  1.0
any supervised  0.03225806451612903
algorithm could  0.03571428571428571
could be  0.25
used ,  0.07079646017699115
, Naive  0.0005614823133071309
Naive  1.0
Naive Bayes  1.0
Bayes  1.0
Bayes ,  0.3333333333333333
and rule  0.001445086705202312
rule induction  0.3333333333333333
induction  1.0
induction .  1.0
of Turney  0.0017825311942959
's GenEx  0.0196078431372549
GenEx  1.0
GenEx algorithm  1.0
algorithm ,  0.10714285714285714
a genetic  0.001226993865030675
genetic  1.0
genetic algorithm  1.0
learn parameters  0.07692307692307693
parameters for  0.5
a domain-specific  0.001226993865030675
domain-specific  1.0
domain-specific keyphrase  0.5
extraction algorithm  0.03225806451612903
algorithm .  0.14285714285714285
The extractor  0.005208333333333333
extractor follows  0.5
follows a  0.5
of heuristics  0.00089126559714795
heuristics  1.0
heuristics to  0.5
to identify  0.006640106241699867
identify keyphrases  0.08333333333333333
The genetic  0.005208333333333333
algorithm optimizes  0.03571428571428571
optimizes  1.0
optimizes parameters  1.0
for these  0.0036101083032490976
these heuristics  0.023809523809523808
heuristics with  0.5
to performance  0.0013280212483399733
performance on  0.05555555555555555
on training  0.0047169811320754715
documents with  0.05263157894736842
with known  0.01092896174863388
known key  0.038461538461538464
key phrases  0.16666666666666666
phrases .  0.3125
Unsupervised keyphrase  0.3333333333333333
: TextRank  0.0196078431372549
TextRank  0.7857142857142857
TextRank While  0.07142857142857142
While  0.4
While supervised  0.2
supervised methods  0.125
methods have  0.045454545454545456
have some  0.009615384615384616
some nice  0.012048192771084338
nice  1.0
nice properties  0.25
properties  1.0
properties ,  0.25
, like  0.0016844469399213925
like being  0.03571428571428571
being able  0.05555555555555555
produce interpretable  0.045454545454545456
interpretable  1.0
interpretable rules  1.0
for what  0.007220216606498195
what features  0.03125
features characterize  0.038461538461538464
characterize  1.0
characterize a  0.5
keyphrase ,  0.05263157894736842
they also  0.025
also require  0.014492753623188406
large amount  0.043478260869565216
Many documents  0.08333333333333333
are needed  0.004149377593360996
, training  0.0005614823133071309
training on  0.03571428571428571
specific domain  0.14285714285714285
domain tends  0.05
tends  1.0
tends to  1.0
to customize  0.0026560424966799467
customize  1.0
customize the  0.5
extraction process  0.03225806451612903
to that  0.0026560424966799467
that domain  0.0035460992907801418
domain ,  0.15
the resulting  0.001384083044982699
resulting classifier  0.25
classifier is  0.14285714285714285
not necessarily  0.017857142857142856
necessarily  1.0
necessarily portable  0.5
portable  1.0
portable ,  0.3333333333333333
as some  0.003484320557491289
's results  0.0196078431372549
results demonstrate  0.047619047619047616
demonstrate  1.0
demonstrate .  1.0
extraction removes  0.03225806451612903
removes  1.0
removes the  1.0
the need  0.0020761245674740486
need for  0.14285714285714285
It approaches  0.02631578947368421
approaches the  0.03571428571428571
problem from  0.022727272727272728
a different  0.0036809815950920245
different angle  0.02040816326530612
angle  1.0
angle .  1.0
Instead of  1.0
of trying  0.00089126559714795
trying  1.0
trying to  1.0
learn explicit  0.07692307692307693
explicit features  0.2
that characterize  0.0035460992907801418
characterize keyphrases  0.5
the TextRank  0.001384083044982699
TextRank algorithm  0.07142857142857142
algorithm exploits  0.03571428571428571
exploits  1.0
exploits the  1.0
text itself  0.006289308176100629
itself to  0.2
determine keyphrases  0.043478260869565216
that appear  0.014184397163120567
appear ``  0.0625
`` central  0.010582010582010581
central  1.0
central ''  0.6666666666666666
'' to  0.016129032258064516
same way  0.04
way that  0.125
that PageRank  0.0035460992907801418
PageRank  1.0
PageRank selects  0.16666666666666666
selects  1.0
selects important  0.5
important Web  0.0625
Web pages  0.2222222222222222
pages .  0.2857142857142857
Recall this  0.3333333333333333
the notion  0.001384083044982699
notion  1.0
notion of  0.75
`` prestige  0.005291005291005291
prestige  1.0
prestige ''  1.0
`` recommendation  0.010582010582010581
recommendation  1.0
recommendation ''  1.0
'' from  0.005376344086021506
from social  0.009615384615384616
social networks  0.21428571428571427
networks  1.0
networks .  0.21428571428571427
this way  0.02197802197802198
way ,  0.041666666666666664
, TextRank  0.0011229646266142617
TextRank does  0.07142857142857142
does not  0.5
not rely  0.017857142857142856
rely on  0.8571428571428571
on any  0.018867924528301886
any previous  0.03225806451612903
previous  1.0
previous training  0.3333333333333333
data at  0.012987012987012988
but rather  0.029411764705882353
rather can  0.0625
be run  0.004219409282700422
run on  0.2
any arbitrary  0.06451612903225806
arbitrary  1.0
arbitrary piece  0.3333333333333333
and it  0.0072254335260115606
it can  0.05128205128205128
can produce  0.0055248618784530384
output simply  0.038461538461538464
simply based  0.08333333333333333
text 's  0.006289308176100629
's intrinsic  0.0196078431372549
intrinsic properties  0.25
properties .  0.25
Thus the  0.08333333333333333
the algorithm  0.0006920415224913495
is easily  0.0020325203252032522
easily  1.0
easily portable  0.2222222222222222
portable to  0.3333333333333333
new domains  0.041666666666666664
domains and  0.25
and languages  0.001445086705202312
TextRank is  0.07142857142857142
general purpose  0.09090909090909091
purpose graph-based  0.4
graph-based  1.0
graph-based ranking  1.0
ranking  1.0
ranking algorithm  0.2857142857142857
algorithm for  0.07142857142857142
for NLP  0.0036101083032490976
Essentially ,  1.0
it runs  0.008547008547008548
runs  1.0
runs PageRank  1.0
PageRank on  0.16666666666666666
a graph  0.0036809815950920245
graph  1.0
graph specially  0.07692307692307693
specially  1.0
specially designed  1.0
designed for  0.14285714285714285
particular NLP  0.07692307692307693
NLP task  0.02127659574468085
For keyphrase  0.01639344262295082
it builds  0.008547008547008548
builds  1.0
builds a  0.5
graph using  0.07692307692307693
using some  0.03389830508474576
some set  0.012048192771084338
units as  0.14285714285714285
as vertices  0.003484320557491289
vertices  1.0
vertices .  0.2222222222222222
Edges are  1.0
are based  0.02074688796680498
some measure  0.012048192771084338
measure of  0.18181818181818182
semantic or  0.047619047619047616
or lexical  0.009009009009009009
lexical similarity  0.07692307692307693
similarity  1.0
similarity between  0.2
text unit  0.006289308176100629
unit  1.0
unit vertices  0.3333333333333333
Unlike PageRank  1.0
PageRank ,  0.16666666666666666
the edges  0.001384083044982699
edges  1.0
edges are  0.14285714285714285
typically undirected  0.05555555555555555
undirected  1.0
undirected and  1.0
be weighted  0.004219409282700422
weighted  1.0
weighted to  0.3333333333333333
to reflect  0.0013280212483399733
reflect  1.0
reflect a  1.0
a degree  0.001226993865030675
of similarity  0.00089126559714795
similarity .  0.1
Once the  0.2
the graph  0.004152249134948097
graph is  0.23076923076923078
is constructed  0.0040650406504065045
constructed  1.0
constructed ,  0.5
form a  0.15
a stochastic  0.001226993865030675
stochastic matrix  0.125
matrix  1.0
matrix ,  1.0
, combined  0.0005614823133071309
combined with  0.5
a damping  0.001226993865030675
damping  1.0
damping factor  1.0
factor  1.0
factor -LRB-  0.5
as in  0.027874564459930314
`` random  0.005291005291005291
random  1.0
random surfer  0.14285714285714285
surfer  1.0
surfer model  1.0
model ''  0.03333333333333333
the ranking  0.001384083044982699
ranking over  0.14285714285714285
over vertices  0.08333333333333333
vertices is  0.1111111111111111
is obtained  0.0020325203252032522
by finding  0.005714285714285714
finding the  0.4
the eigenvector  0.0006920415224913495
eigenvector  1.0
eigenvector corresponding  0.5
corresponding to  0.3333333333333333
to eigenvalue  0.0013280212483399733
eigenvalue  1.0
eigenvalue 1  1.0
1  1.0
1 -LRB-  0.25
the stationary  0.0006920415224913495
stationary  1.0
stationary distribution  0.2857142857142857
distribution  1.0
distribution of  0.25
the random  0.0006920415224913495
random walk  0.5714285714285714
walk  1.0
walk on  0.4
graph -RRB-  0.07692307692307693
choices What  0.4
What should  0.09090909090909091
should vertices  0.05263157894736842
vertices be  0.1111111111111111
be ?  0.004219409282700422
The vertices  0.005208333333333333
vertices should  0.1111111111111111
should correspond  0.05263157894736842
correspond  1.0
correspond to  1.0
we want  0.044444444444444446
want  1.0
want to  0.8333333333333334
to rank  0.00398406374501992
rank  1.0
rank .  0.16666666666666666
Potentially ,  1.0
we could  0.022222222222222223
could do  0.0625
do something  0.038461538461538464
something  1.0
something similar  1.0
the supervised  0.0006920415224913495
methods and  0.022727272727272728
and create  0.002890173410404624
a vertex  0.00245398773006135
vertex  1.0
vertex for  0.6666666666666666
, trigram  0.0011229646266142617
trigram ,  0.6666666666666666
to keep  0.0026560424966799467
keep  1.0
keep the  0.3333333333333333
graph small  0.07692307692307693
small ,  0.2222222222222222
the authors  0.0006920415224913495
authors decide  0.2
decide to  0.25
rank individual  0.16666666666666666
individual unigrams  0.08333333333333333
unigrams in  0.25
a first  0.00245398773006135
step ,  0.13333333333333333
then include  0.02857142857142857
include a  0.1111111111111111
a second  0.0036809815950920245
step that  0.13333333333333333
that merges  0.0035460992907801418
merges  1.0
merges highly  1.0
highly ranked  0.1111111111111111
ranked  1.0
ranked adjacent  0.2
adjacent  1.0
adjacent unigrams  0.16666666666666666
unigrams to  0.08333333333333333
form multi-word  0.05
multi-word  1.0
multi-word phrases  1.0
This has  0.015873015873015872
a nice  0.00245398773006135
nice side  0.25
side  1.0
side effect  1.0
effect  1.0
effect of  0.5
of allowing  0.00089126559714795
allowing  1.0
allowing us  0.3333333333333333
us  1.0
us to  0.5
produce keyphrases  0.045454545454545456
keyphrases of  0.02857142857142857
of arbitrary  0.00089126559714795
arbitrary length  0.3333333333333333
length .  0.125
we rank  0.022222222222222223
rank unigrams  0.16666666666666666
unigrams and  0.08333333333333333
and find  0.001445086705202312
find that  0.07692307692307693
`` advanced  0.005291005291005291
advanced  1.0
advanced ''  0.2
`` natural  0.015873015873015872
natural ''  0.02666666666666667
`` language  0.010582010582010581
language ''  0.013513513513513514
`` processing  0.010582010582010581
processing ''  0.037037037037037035
'' all  0.005376344086021506
all get  0.023255813953488372
get high  0.14285714285714285
high ranks  0.05555555555555555
ranks  1.0
ranks ,  0.5
we would  0.06666666666666667
would look  0.03773584905660377
look  1.0
look at  0.4
and see  0.001445086705202312
see that  0.05
these words  0.047619047619047616
words appear  0.009174311926605505
appear consecutively  0.0625
consecutively  1.0
consecutively and  1.0
a final  0.001226993865030675
final  1.0
final keyphrase  0.1111111111111111
keyphrase using  0.05263157894736842
using all  0.01694915254237288
all four  0.023255813953488372
four together  0.14285714285714285
together  1.0
together .  0.125
the unigrams  0.001384083044982699
unigrams placed  0.08333333333333333
placed  1.0
placed in  1.0
graph can  0.07692307692307693
be filtered  0.012658227848101266
filtered  1.0
filtered by  0.3333333333333333
by part  0.005714285714285714
authors found  0.2
that adjectives  0.0035460992907801418
adjectives and  0.3333333333333333
and nouns  0.001445086705202312
nouns were  0.1111111111111111
were the  0.024390243902439025
best to  0.1111111111111111
to include  0.009296148738379814
include .  0.037037037037037035
some linguistic  0.012048192771084338
linguistic knowledge  0.0625
knowledge comes  0.037037037037037035
comes into  0.2
into play  0.01282051282051282
play  1.0
play in  1.0
this step  0.01098901098901099
How should  0.14285714285714285
should we  0.05263157894736842
we create  0.044444444444444446
create edges  0.058823529411764705
edges ?  0.2857142857142857
created based  0.14285714285714285
on word  0.0047169811320754715
word co-occurrence  0.016666666666666666
co-occurrence  1.0
co-occurrence in  0.3333333333333333
this application  0.01098901098901099
application of  0.2857142857142857
of TextRank  0.00089126559714795
TextRank .  0.07142857142857142
Two vertices  0.14285714285714285
vertices are  0.1111111111111111
are connected  0.004149377593360996
connected by  0.2
by an  0.011428571428571429
an edge  0.015151515151515152
edge  1.0
edge if  0.3333333333333333
unigrams appear  0.08333333333333333
appear within  0.0625
within a  0.2777777777777778
a window  0.001226993865030675
window  1.0
window of  1.0
of size  0.00089126559714795
size N  0.16666666666666666
N  0.6666666666666666
N in  0.3333333333333333
N is  0.3333333333333333
typically around  0.05555555555555555
around  1.0
around 2  0.125
2  1.0
2 --  0.2
-- 10  0.04
10 .  0.125
'' and  0.06989247311827956
'' might  0.010752688172043012
be linked  0.008438818565400843
linked  1.0
linked in  0.3333333333333333
text about  0.012578616352201259
about NLP  0.025
`` Natural  0.005291005291005291
Natural ''  0.07692307692307693
would also  0.018867924528301886
linked because  0.3333333333333333
because they  0.13333333333333333
they would  0.025
would both  0.018867924528301886
both appear  0.03225806451612903
same string  0.04
string  1.0
string of  1.0
of N  0.00089126559714795
N words  0.3333333333333333
These edges  0.058823529411764705
edges build  0.14285714285714285
build on  0.3333333333333333
`` text  0.005291005291005291
text cohesion  0.006289308176100629
cohesion  1.0
cohesion ''  1.0
the idea  0.001384083044982699
idea that  0.2857142857142857
words that  0.009174311926605505
appear near  0.0625
near  1.0
near each  1.0
other are  0.014285714285714285
likely related  0.0625
related in  0.06666666666666667
a meaningful  0.00245398773006135
meaningful way  0.125
way and  0.041666666666666664
`` recommend  0.010582010582010581
recommend  1.0
recommend ''  1.0
'' each  0.005376344086021506
other to  0.014285714285714285
the reader  0.002768166089965398
reader  1.0
reader .  0.2
How are  0.2857142857142857
the final  0.002768166089965398
final keyphrases  0.2222222222222222
keyphrases formed  0.02857142857142857
formed  1.0
formed ?  0.4
Since this  0.2
this method  0.01098901098901099
method simply  0.0625
simply ranks  0.08333333333333333
ranks the  0.5
the individual  0.001384083044982699
individual vertices  0.08333333333333333
vertices ,  0.1111111111111111
to threshold  0.0013280212483399733
threshold or  0.5
or produce  0.0045045045045045045
a limited  0.00245398773006135
The technique  0.005208333333333333
technique chosen  0.14285714285714285
chosen  1.0
chosen is  0.2
to set  0.00398406374501992
set a  0.05128205128205128
a count  0.001226993865030675
count  1.0
count T  0.2
T  1.0
T to  0.16666666666666666
a user-specified  0.001226993865030675
user-specified  1.0
user-specified fraction  0.5
fraction  1.0
fraction of  1.0
the total  0.0006920415224913495
total  1.0
total number  0.5
of vertices  0.00089126559714795
vertices in  0.1111111111111111
graph .  0.15384615384615385
Then the  0.4
the top  0.002768166089965398
top  1.0
top T  0.4
T vertices\/unigrams  0.16666666666666666
vertices\/unigrams  1.0
vertices\/unigrams are  1.0
are selected  0.004149377593360996
selected  1.0
selected based  0.5
on their  0.009433962264150943
their stationary  0.029411764705882353
stationary probabilities  0.14285714285714285
probabilities .  0.18181818181818182
A post  0.02
post  1.0
post -  1.0
- processing  0.0625
processing step  0.018518518518518517
step is  0.06666666666666667
to merge  0.0013280212483399733
merge  1.0
merge adjacent  1.0
adjacent instances  0.16666666666666666
instances  1.0
instances of  0.6666666666666666
these T  0.023809523809523808
T unigrams  0.3333333333333333
unigrams .  0.16666666666666666
, potentially  0.0005614823133071309
potentially  1.0
potentially more  0.3333333333333333
more or  0.031578947368421054
less than  0.25
than T  0.022222222222222223
T final  0.16666666666666666
keyphrases will  0.02857142857142857
be produced  0.004219409282700422
produced ,  0.1111111111111111
number should  0.023255813953488372
be roughly  0.004219409282700422
roughly  1.0
roughly proportional  0.3333333333333333
proportional  1.0
proportional to  1.0
Why it  0.14285714285714285
it works  0.008547008547008548
works  1.0
works It  0.5
It  0.10526315789473684
not initially  0.008928571428571428
initially  1.0
initially clear  1.0
clear  1.0
clear why  0.25
why applying  0.14285714285714285
applying PageRank  0.5
PageRank to  0.3333333333333333
a co-occurrence  0.001226993865030675
co-occurrence graph  0.6666666666666666
graph would  0.07692307692307693
produce useful  0.045454545454545456
useful keyphrases  0.07142857142857142
One way  0.07692307692307693
to think  0.0013280212483399733
think  1.0
think about  0.3333333333333333
about it  0.025
A word  0.02
word that  0.03333333333333333
that appears  0.0035460992907801418
appears multiple  0.2
multiple times  0.07692307692307693
times throughout  0.2
throughout  1.0
throughout a  1.0
text may  0.006289308176100629
may have  0.038461538461538464
different co-occurring  0.02040816326530612
co-occurring  1.0
co-occurring neighbors  1.0
neighbors .  0.3333333333333333
about machine  0.025
the unigram  0.0006920415224913495
unigram ``  0.2
`` learning  0.021164021164021163
learning ''  0.11627906976744186
might co-occur  0.038461538461538464
co-occur  1.0
co-occur with  0.5
`` machine  0.005291005291005291
machine ''  0.012658227848101266
, supervised  0.0005614823133071309
supervised ''  0.1875
`` un-supervised  0.005291005291005291
un-supervised  1.0
un-supervised ''  1.0
`` semi-supervised  0.005291005291005291
semi-supervised ''  0.5
different sentences  0.061224489795918366
'' vertex  0.005376344086021506
vertex would  0.3333333333333333
a central  0.001226993865030675
central ``  0.3333333333333333
`` hub  0.005291005291005291
hub  1.0
hub ''  1.0
that connects  0.0035460992907801418
connects  1.0
connects to  1.0
these other  0.023809523809523808
other modifying  0.014285714285714285
modifying  1.0
modifying words  1.0
Running PageRank\/TextRank  1.0
PageRank\/TextRank  1.0
PageRank\/TextRank on  1.0
is likely  0.006097560975609756
rank ``  0.16666666666666666
'' highly  0.005376344086021506
highly .  0.1111111111111111
Similarly ,  1.0
text contains  0.006289308176100629
phrase ``  0.1
`` supervised  0.026455026455026454
supervised classification  0.125
classification ''  0.29411764705882354
then there  0.02857142857142857
there would  0.025
be an  0.004219409282700422
edge between  0.3333333333333333
between ``  0.02564102564102564
`` classification  0.015873015873015872
If ``  0.1
'' appears  0.005376344086021506
appears several  0.2
several other  0.045454545454545456
other places  0.014285714285714285
places and  0.5
thus has  0.1
has many  0.023809523809523808
many neighbors  0.019230769230769232
neighbors ,  0.3333333333333333
is importance  0.0020325203252032522
importance  1.0
importance would  0.16666666666666666
would contribute  0.018867924528301886
contribute  1.0
contribute to  1.0
the importance  0.001384083044982699
importance of  0.5
If it  0.1
it ends  0.017094017094017096
ends  1.0
ends up  0.5
up with  0.13636363636363635
a high  0.0036809815950920245
high rank  0.05555555555555555
rank ,  0.16666666666666666
it will  0.017094017094017096
be selected  0.004219409282700422
selected as  0.5
as one  0.006968641114982578
, along  0.0005614823133071309
along  1.0
along with  1.0
and probably  0.001445086705202312
probably  1.0
probably ``  0.25
final post-processing  0.1111111111111111
post-processing  1.0
post-processing step  0.6666666666666666
would then  0.018867924528301886
then end  0.02857142857142857
end up  0.25
with keyphrases  0.00546448087431694
keyphrases ``  0.02857142857142857
In short  0.009523809523809525
short ,  0.125
the co-occurrence  0.0006920415224913495
graph will  0.15384615384615385
will contain  0.02857142857142857
contain densely  0.08333333333333333
densely  1.0
densely connected  1.0
connected regions  0.2
regions  1.0
regions for  0.5
for terms  0.0036101083032490976
terms that  0.07692307692307693
appear often  0.0625
often and  0.022727272727272728
different contexts  0.02040816326530612
contexts .  0.2857142857142857
A random  0.02
on this  0.014150943396226415
this graph  0.01098901098901099
will have  0.05714285714285714
a stationary  0.00245398773006135
distribution that  0.5
that assigns  0.0035460992907801418
assigns  1.0
assigns large  1.0
large probabilities  0.043478260869565216
probabilities to  0.09090909090909091
the terms  0.0006920415224913495
terms in  0.07692307692307693
the centers  0.0006920415224913495
centers  1.0
centers of  1.0
the clusters  0.0006920415224913495
clusters  1.0
clusters .  1.0
is similar  0.0040650406504065045
to densely  0.0013280212483399733
connected Web  0.2
pages getting  0.14285714285714285
getting ranked  0.25
ranked highly  0.4
highly by  0.1111111111111111
by PageRank  0.005714285714285714
PageRank .  0.16666666666666666
Document summarization  0.25
summarization Like  0.02
Like  0.5
Like keyphrase  0.5
, document  0.0005614823133071309
summarization hopes  0.02
hopes  1.0
hopes to  1.0
The only  0.010416666666666666
only real  0.02631578947368421
real difference  0.1111111111111111
difference is  0.25
that now  0.0035460992907801418
now we  0.07692307692307693
are dealing  0.004149377593360996
dealing  1.0
dealing with  1.0
with larger  0.00546448087431694
larger text  0.0625
units --  0.14285714285714285
-- whole  0.04
sentences instead  0.013157894736842105
instead of  0.5714285714285714
and phrases  0.002890173410404624
While some  0.2
some work  0.012048192771084338
work has  0.08333333333333333
been done  0.029411764705882353
done in  0.45454545454545453
in abstractive  0.0018726591760299626
abstractive summarization  0.3333333333333333
summarization -LRB-  0.04
-LRB- creating  0.0027100271002710027
an abstract  0.007575757575757576
abstract  1.0
abstract synopsis  1.0
synopsis  1.0
synopsis like  1.0
like that  0.03571428571428571
the majority  0.0006920415224913495
majority  1.0
majority of  1.0
are extractive  0.004149377593360996
extractive -LRB-  0.14285714285714285
-LRB- selecting  0.0027100271002710027
to place  0.0026560424966799467
place  1.0
place in  0.5
summary -RRB-  0.023809523809523808
Before getting  0.5
getting into  0.25
into the  0.10256410256410256
the details  0.0006920415224913495
details  1.0
details of  0.5
some summarization  0.012048192771084338
summarization methods  0.02
will mention  0.02857142857142857
mention  1.0
mention how  0.3333333333333333
how summarization  0.034482758620689655
typically evaluated  0.05555555555555555
evaluated .  0.14285714285714285
The most  0.026041666666666668
common way  0.08
way is  0.041666666666666664
is using  0.0040650406504065045
using the  0.1016949152542373
the so-called  0.0006920415224913495
so-called  1.0
so-called ROUGE  0.3333333333333333
ROUGE  1.0
ROUGE -LRB-  0.2
-LRB- Recall-Oriented  0.005420054200542005
Recall-Oriented  1.0
Recall-Oriented Understudy  1.0
Understudy  1.0
Understudy for  1.0
for Gisting  0.007220216606498195
Gisting  1.0
Gisting Evaluation  1.0
Evaluation -RRB-  0.2222222222222222
-RRB- measure  0.0028169014084507044
measure -LRB-  0.09090909090909091
-LRB- http:\/\/haydn.isi.edu\/ROUGE\/  0.0027100271002710027
http:\/\/haydn.isi.edu\/ROUGE\/  1.0
http:\/\/haydn.isi.edu\/ROUGE\/ -RRB-  1.0
a recall-based  0.001226993865030675
recall-based  1.0
recall-based measure  0.5
measure that  0.09090909090909091
that determines  0.0070921985815602835
determines how  0.3333333333333333
how well  0.20689655172413793
well a  0.03571428571428571
a system-generated  0.001226993865030675
system-generated  1.0
system-generated summary  0.5
summary covers  0.023809523809523808
covers the  0.5
content present  0.08333333333333333
more human-generated  0.010526315789473684
human-generated  1.0
human-generated model  0.5
model summaries  0.06666666666666667
summaries known  0.023255813953488372
as references  0.003484320557491289
references .  0.25
is recall-based  0.0020325203252032522
recall-based to  0.5
to encourage  0.0013280212483399733
encourage  1.0
encourage systems  1.0
systems to  0.008928571428571428
include all  0.037037037037037035
the important  0.0006920415224913495
important topics  0.0625
topics in  0.14285714285714285
Recall can  0.3333333333333333
be computed  0.004219409282700422
computed  1.0
computed with  0.5
to unigram  0.0013280212483399733
or 4-gram  0.0045045045045045045
4-gram  1.0
4-gram matching  1.0
matching ,  0.2
though ROUGE-1  0.1
ROUGE-1  0.8
ROUGE-1 -LRB-  0.2
-LRB- unigram  0.0027100271002710027
unigram matching  0.2
matching -RRB-  0.2
-RRB- has  0.008450704225352112
been shown  0.014705882352941176
shown to  0.4
to correlate  0.0013280212483399733
correlate  1.0
correlate best  0.3333333333333333
best with  0.05555555555555555
with human  0.01092896174863388
human assessments  0.021739130434782608
assessments  1.0
assessments of  1.0
of system-generated  0.00089126559714795
system-generated summaries  0.5
the summaries  0.002768166089965398
summaries with  0.046511627906976744
with highest  0.01092896174863388
highest  1.0
highest ROUGE-1  0.3333333333333333
ROUGE-1 values  0.2
values correlate  0.125
correlate with  0.6666666666666666
summaries humans  0.023255813953488372
humans deemed  0.08333333333333333
deemed the  0.5
best -RRB-  0.05555555555555555
ROUGE-1 is  0.2
is computed  0.0020325203252032522
computed as  0.5
as division  0.003484320557491289
division  1.0
division of  0.5
of count  0.00089126559714795
count of  0.4
of unigrams  0.0017825311942959
in reference  0.003745318352059925
reference that  0.125
in system  0.0018726591760299626
and count  0.001445086705202312
reference summary  0.375
If there  0.1
are multiple  0.004149377593360996
multiple references  0.07692307692307693
references ,  0.25
the ROUGE-1  0.0006920415224913495
ROUGE-1 scores  0.2
scores are  0.2
are averaged  0.004149377593360996
averaged  1.0
averaged .  1.0
Because ROUGE  0.5
ROUGE is  0.4
based only  0.018518518518518517
on content  0.0047169811320754715
content overlap  0.16666666666666666
overlap  1.0
overlap ,  0.25
determine if  0.21739130434782608
same general  0.04
general concepts  0.045454545454545456
concepts are  0.4
are discussed  0.004149377593360996
discussed between  0.14285714285714285
between an  0.02564102564102564
an automatic  0.015151515151515152
automatic summary  0.08695652173913043
summary and  0.047619047619047616
a reference  0.00245398773006135
not determine  0.008928571428571428
result is  0.18181818181818182
is coherent  0.0020325203252032522
coherent or  0.2
sentences flow  0.013157894736842105
flow  1.0
flow together  1.0
together in  0.125
a sensible  0.001226993865030675
sensible  1.0
sensible manner  1.0
High-order n-gram  1.0
n-gram  1.0
n-gram ROUGE  0.5
ROUGE measures  0.2
measures try  0.16666666666666666
try  1.0
try to  1.0
to judge  0.0026560424966799467
judge fluency  0.25
fluency  1.0
fluency to  1.0
some degree  0.024096385542168676
degree .  0.16666666666666666
that ROUGE  0.0035460992907801418
the BLEU  0.0006920415224913495
BLEU  1.0
BLEU measure  0.3333333333333333
measure for  0.09090909090909091
but BLEU  0.014705882352941176
BLEU is  0.3333333333333333
is precision  0.0020325203252032522
precision -  0.2
- based  0.1875
because translation  0.03333333333333333
systems favor  0.008928571428571428
favor accuracy  0.5
A promising  0.02
promising  1.0
promising line  1.0
line  1.0
line in  0.3333333333333333
in document  0.003745318352059925
is adaptive  0.0020325203252032522
adaptive  1.0
adaptive document\/text  0.3333333333333333
document\/text  1.0
document\/text summarization  0.5
of adaptive  0.00089126559714795
adaptive summarization  0.6666666666666666
summarization involves  0.02
involves preliminary  0.1
preliminary  1.0
preliminary recognition  0.3333333333333333
of document\/text  0.00089126559714795
document\/text genre  0.5
genre  1.0
genre and  0.5
and subsequent  0.001445086705202312
subsequent  1.0
subsequent application  0.5
summarization algorithms  0.02
algorithms optimized  0.02857142857142857
optimized  1.0
optimized for  1.0
this genre  0.01098901098901099
genre .  0.5
First summarizes  1.0
summarizes  1.0
summarizes that  1.0
that perform  0.0035460992907801418
perform adaptive  0.09090909090909091
summarization have  0.02
been created  0.029411764705882353
created .  0.14285714285714285
Overview of  1.0
of supervised  0.00089126559714795
learning approaches  0.023255813953488372
approaches Supervised  0.03571428571428571
Supervised  1.0
Supervised text  1.0
text summarization  0.006289308176100629
very much  0.024390243902439025
much like  0.045454545454545456
like supervised  0.03571428571428571
and we  0.001445086705202312
will not  0.11428571428571428
not spend  0.008928571428571428
spend  1.0
spend much  1.0
much time  0.045454545454545456
time on  0.030303030303030304
on it  0.0047169811320754715
Basically ,  1.0
if you  0.07142857142857142
you have  0.15384615384615385
a collection  0.00245398773006135
collection  1.0
collection of  0.4
documents and  0.02631578947368421
and human-generated  0.001445086705202312
human-generated summaries  0.5
summaries for  0.023255813953488372
for them  0.007220216606498195
them ,  0.21052631578947367
, you  0.0005614823133071309
can learn  0.0055248618784530384
learn features  0.07692307692307693
make them  0.05
them good  0.05263157894736842
good candidates  0.23076923076923078
candidates for  0.2
for inclusion  0.0036101083032490976
inclusion  1.0
inclusion in  1.0
Features might  1.0
might include  0.038461538461538464
include the  0.18518518518518517
the position  0.0006920415224913495
position in  0.5
the document  0.004152249134948097
document -LRB-  0.05555555555555555
first few  0.030303030303030304
few  1.0
few sentences  1.0
are probably  0.004149377593360996
probably important  0.25
important -RRB-  0.0625
The main  0.015625
main difficulty  0.25
difficulty in  0.2857142857142857
in supervised  0.0018726591760299626
supervised extractive  0.0625
extractive summarization  0.42857142857142855
known summaries  0.038461538461538464
summaries must  0.023255813953488372
must be  0.42857142857142855
be manually  0.004219409282700422
manually created  0.25
created by  0.2857142857142857
by extracting  0.005714285714285714
extracting sentences  0.2
sentences so  0.013157894736842105
an original  0.007575757575757576
original training  0.07692307692307693
training document  0.03571428571428571
document can  0.027777777777777776
be labeled  0.004219409282700422
labeled as  0.3333333333333333
`` in  0.010582010582010581
in summary  0.003745318352059925
summary ''  0.047619047619047616
`` not  0.005291005291005291
not in  0.008928571428571428
not typically  0.017857142857142856
typically how  0.05555555555555555
how people  0.034482758620689655
people create  0.0625
create summaries  0.058823529411764705
summaries ,  0.06976744186046512
so simply  0.03333333333333333
simply using  0.08333333333333333
using journal  0.01694915254237288
journal abstracts  0.3333333333333333
abstracts or  0.5
or existing  0.0045045045045045045
existing summaries  0.2
is usually  0.016260162601626018
usually not  0.03125
not sufficient  0.008928571428571428
sufficient  1.0
sufficient .  0.6
The sentences  0.005208333333333333
in these  0.0056179775280898875
these summaries  0.047619047619047616
summaries do  0.023255813953488372
necessarily match  0.5
match up  0.16666666666666666
with sentences  0.00546448087431694
so it  0.06666666666666667
would difficult  0.018867924528301886
to assign  0.00398406374501992
assign labels  0.2
to examples  0.0013280212483399733
examples for  0.041666666666666664
training .  0.03571428571428571
Note ,  0.1111111111111111
these natural  0.023809523809523808
natural summaries  0.013333333333333334
summaries can  0.046511627906976744
can still  0.0055248618784530384
still be  0.06666666666666667
for evaluation  0.0036101083032490976
evaluation purposes  0.018518518518518517
purposes ,  0.25
since ROUGE-1  0.1
ROUGE-1 only  0.2
only cares  0.02631578947368421
cares  1.0
cares about  1.0
about unigrams  0.025
Unsupervised approaches  0.16666666666666666
TextRank and  0.14285714285714285
and LexRank  0.004335260115606936
LexRank  0.8333333333333334
LexRank The  0.08333333333333333
The unsupervised  0.005208333333333333
unsupervised approach  0.125
to summarization  0.0026560424966799467
also quite  0.014492753623188406
quite similar  0.125
similar in  0.037037037037037035
in spirit  0.0018726591760299626
spirit  1.0
spirit to  1.0
unsupervised keyphrase  0.125
and gets  0.001445086705202312
gets  1.0
gets around  0.5
around the  0.375
issue of  0.125
of costly  0.00089126559714795
costly  1.0
costly training  1.0
Some unsupervised  0.047619047619047616
unsupervised summarization  0.25
summarization approaches  0.02
approaches are  0.03571428571428571
on finding  0.0047169811320754715
`` centroid  0.005291005291005291
centroid  1.0
centroid ''  0.5
'' sentence  0.005376344086021506
the mean  0.0006920415224913495
mean word  0.5
word vector  0.016666666666666666
vector  1.0
vector of  0.3333333333333333
sentences can  0.039473684210526314
be ranked  0.004219409282700422
ranked with  0.2
with regard  0.02185792349726776
regard  1.0
regard to  0.8
their similarity  0.029411764705882353
similarity to  0.1
this centroid  0.01098901098901099
centroid sentence  0.5
A more  0.02
more principled  0.010526315789473684
principled  1.0
principled way  1.0
estimate sentence  0.25
sentence importance  0.020833333333333332
importance is  0.16666666666666666
using random  0.01694915254237288
random walks  0.2857142857142857
walks  1.0
walks and  0.5
and eigenvector  0.001445086705202312
eigenvector centrality  0.5
centrality  1.0
centrality .  0.5
LexRank is  0.08333333333333333
algorithm essentially  0.03571428571428571
essentially identical  0.125
to TextRank  0.0013280212483399733
TextRank ,  0.14285714285714285
and both  0.001445086705202312
both use  0.03225806451612903
use this  0.027777777777777776
approach for  0.02857142857142857
for document  0.0036101083032490976
two methods  0.034482758620689655
methods were  0.045454545454545456
developed by  0.038461538461538464
by different  0.005714285714285714
different groups  0.02040816326530612
groups  1.0
groups at  0.2
same time  0.12
LexRank simply  0.08333333333333333
simply focused  0.08333333333333333
on summarization  0.0047169811320754715
but could  0.014705882352941176
could just  0.0625
just  1.0
just as  0.2222222222222222
as easily  0.006968641114982578
easily be  0.1111111111111111
for keyphrase  0.0036101083032490976
extraction or  0.03225806451612903
any other  0.06451612903225806
other NLP  0.014285714285714285
NLP ranking  0.02127659574468085
ranking task  0.14285714285714285
the vertices  0.0006920415224913495
vertices ?  0.1111111111111111
In both  0.01904761904761905
both LexRank  0.03225806451612903
LexRank and  0.08333333333333333
and TextRank  0.001445086705202312
constructed by  0.5
by creating  0.005714285714285714
creating a  0.2857142857142857
each sentence  0.022222222222222223
sentence in  0.041666666666666664
The edges  0.005208333333333333
edges between  0.14285714285714285
some form  0.04819277108433735
semantic similarity  0.047619047619047616
similarity or  0.1
or content  0.0045045045045045045
overlap .  0.25
While LexRank  0.2
LexRank uses  0.08333333333333333
uses cosine  0.07142857142857142
cosine  1.0
cosine similarity  0.3333333333333333
similarity of  0.1
of TF-IDF  0.00089126559714795
TF-IDF  1.0
TF-IDF vectors  1.0
vectors  1.0
vectors ,  0.3333333333333333
TextRank uses  0.14285714285714285
very similar  0.0975609756097561
similar measure  0.037037037037037035
measure based  0.09090909090909091
words two  0.009174311926605505
two sentences  0.06896551724137931
have in  0.019230769230769232
common -LRB-  0.04
-LRB- normalized  0.0027100271002710027
normalized  1.0
normalized by  1.0
sentences '  0.013157894736842105
' lengths  0.05263157894736842
lengths  1.0
lengths -RRB-  1.0
The LexRank  0.005208333333333333
LexRank paper  0.08333333333333333
paper explored  0.09090909090909091
explored  1.0
explored using  0.5
using unweighted  0.01694915254237288
unweighted  1.0
unweighted edges  1.0
edges after  0.14285714285714285
after applying  0.08333333333333333
applying a  0.25
threshold to  0.25
the cosine  0.0006920415224913495
cosine values  0.3333333333333333
values ,  0.125
also experimented  0.014492753623188406
experimented  1.0
experimented with  1.0
with using  0.00546448087431694
using edges  0.01694915254237288
edges with  0.14285714285714285
with weights  0.00546448087431694
weights equal  0.2
equal  1.0
equal to  1.0
the similarity  0.0006920415224913495
similarity score  0.1
score  1.0
score .  0.5
uses continuous  0.07142857142857142
continuous similarity  0.16666666666666666
similarity scores  0.1
scores as  0.2
as weights  0.003484320557491289
weights .  0.4
are summaries  0.004149377593360996
summaries formed  0.023255813953488372
both algorithms  0.03225806451612903
are ranked  0.004149377593360996
ranked by  0.2
by applying  0.005714285714285714
resulting graph  0.25
A summary  0.02
summary is  0.047619047619047616
is formed  0.0020325203252032522
formed by  0.2
by combining  0.005714285714285714
combining  1.0
combining the  0.25
top ranking  0.2
ranking sentences  0.14285714285714285
or length  0.0045045045045045045
length cutoff  0.125
cutoff  1.0
cutoff to  1.0
the size  0.001384083044982699
size of  0.16666666666666666
LexRank differences  0.08333333333333333
differences It  0.3333333333333333
is worth  0.0040650406504065045
worth  1.0
worth noting  0.5
noting  1.0
noting that  1.0
that TextRank  0.0070921985815602835
TextRank was  0.14285714285714285
was applied  0.012987012987012988
summarization exactly  0.02
exactly as  0.3333333333333333
as described  0.006968641114982578
described here  0.16666666666666666
here  1.0
here ,  0.5
while LexRank  0.1
LexRank was  0.08333333333333333
was used  0.05194805194805195
as part  0.006968641114982578
larger summarization  0.0625
summarization system  0.06
system -LRB-  0.021505376344086023
-LRB- MEAD  0.0027100271002710027
MEAD  1.0
MEAD -RRB-  1.0
that combines  0.0035460992907801418
combines  1.0
combines the  1.0
the LexRank  0.0006920415224913495
LexRank score  0.08333333333333333
score -LRB-  0.16666666666666666
-LRB- stationary  0.0027100271002710027
stationary probability  0.14285714285714285
probability -RRB-  0.2857142857142857
-RRB- with  0.008450704225352112
with other  0.00546448087431694
other features  0.014285714285714285
features like  0.038461538461538464
like sentence  0.03571428571428571
sentence position  0.041666666666666664
position and  0.25
and length  0.001445086705202312
length using  0.125
a linear  0.00245398773006135
linear combination  0.14285714285714285
combination with  0.4
with either  0.00546448087431694
either user-specified  0.1
user-specified or  0.5
or automatically  0.009009009009009009
automatically tuned  0.047619047619047616
tuned  1.0
tuned weights  1.0
this case  0.01098901098901099
case ,  0.17647058823529413
some training  0.012048192771084338
documents might  0.02631578947368421
be needed  0.004219409282700422
needed ,  0.047619047619047616
though the  0.1
TextRank results  0.07142857142857142
results show  0.047619047619047616
show  1.0
show the  1.0
the additional  0.0006920415224913495
additional features  0.16666666666666666
are not  0.02074688796680498
not absolutely  0.008928571428571428
absolutely  1.0
absolutely necessary  1.0
necessary .  0.1
Another important  0.07692307692307693
important distinction  0.125
for single  0.0036101083032490976
single document  0.07142857142857142
LexRank has  0.08333333333333333
to multi-document  0.0013280212483399733
task remains  0.023809523809523808
remains  1.0
remains the  0.25
same in  0.04
in both  0.0018726591760299626
both cases  0.03225806451612903
cases --  0.05555555555555555
-- only  0.04
to choose  0.0013280212483399733
choose  1.0
choose from  0.5
from has  0.009615384615384616
has grown  0.011904761904761904
grown  1.0
grown .  1.0
when summarizing  0.02857142857142857
summarizing  1.0
summarizing multiple  1.0
multiple documents  0.15384615384615385
a greater  0.001226993865030675
greater  1.0
greater risk  0.3333333333333333
risk  1.0
risk of  0.5
of selecting  0.00089126559714795
selecting duplicate  0.2
duplicate  1.0
duplicate or  0.5
or highly  0.0045045045045045045
highly redundant  0.1111111111111111
redundant  1.0
redundant sentences  1.0
same summary  0.04
Imagine you  1.0
articles on  0.125
particular event  0.07692307692307693
you want  0.07692307692307693
produce one  0.045454545454545456
one summary  0.015384615384615385
Each article  0.16666666666666666
article is  0.034482758620689655
many similar  0.019230769230769232
similar sentences  0.1111111111111111
you would  0.07692307692307693
would only  0.018867924528301886
only want  0.02631578947368421
include distinct  0.037037037037037035
distinct ideas  0.14285714285714285
To address  0.1111111111111111
address  1.0
address this  0.25
this issue  0.01098901098901099
issue ,  0.125
, LexRank  0.0005614823133071309
LexRank applies  0.08333333333333333
applies  1.0
applies a  0.2857142857142857
a heuristic  0.00245398773006135
heuristic  1.0
heuristic post-processing  0.3333333333333333
that builds  0.0035460992907801418
builds up  0.5
up a  0.09090909090909091
summary by  0.023809523809523808
by adding  0.011428571428571429
adding  1.0
adding sentences  0.5
in rank  0.0018726591760299626
rank order  0.16666666666666666
order ,  0.14285714285714285
but discards  0.014705882352941176
discards  1.0
discards any  1.0
any sentences  0.03225806451612903
are too  0.004149377593360996
too similar  0.16666666666666666
to ones  0.0013280212483399733
ones already  0.1
already placed  0.2
The method  0.005208333333333333
method used  0.0625
used is  0.008849557522123894
called Cross-Sentence  0.05555555555555555
Cross-Sentence  1.0
Cross-Sentence Information  1.0
Information Subsumption  0.2
Subsumption  1.0
Subsumption -LRB-  1.0
-LRB- CSIS  0.0027100271002710027
CSIS  1.0
CSIS -RRB-  0.5
Why unsupervised  0.14285714285714285
summarization works  0.02
works These  0.5
These  0.058823529411764705
work based  0.041666666666666664
that sentences  0.0035460992907801418
sentences ``  0.02631578947368421
'' other  0.005376344086021506
other similar  0.014285714285714285
if one  0.03571428571428571
one sentence  0.03076923076923077
to many  0.005312084993359893
many others  0.019230769230769232
others ,  0.08333333333333333
will likely  0.05714285714285714
likely be  0.0625
sentence of  0.020833333333333332
of great  0.00089126559714795
great importance  0.3333333333333333
importance .  0.16666666666666666
The importance  0.005208333333333333
this sentence  0.01098901098901099
sentence also  0.020833333333333332
also stems  0.014492753623188406
stems  1.0
stems from  0.5
`` recommending  0.005291005291005291
recommending  1.0
recommending ''  1.0
'' it  0.005376344086021506
get ranked  0.14285714285714285
highly and  0.1111111111111111
and placed  0.001445086705202312
sentence must  0.020833333333333332
be similar  0.004219409282700422
many sentences  0.019230769230769232
are in  0.012448132780082987
in turn  0.009363295880149813
turn  1.0
turn also  0.16666666666666666
also similar  0.014492753623188406
other sentences  0.014285714285714285
This makes  0.015873015873015872
makes intuitive  0.125
intuitive  1.0
intuitive sense  1.0
sense and  0.125
and allows  0.002890173410404624
allows the  0.375
be applied  0.012658227848101266
to any  0.00398406374501992
arbitrary new  0.3333333333333333
The methods  0.010416666666666666
are domain-independent  0.004149377593360996
domain-independent  1.0
domain-independent and  1.0
and easily  0.001445086705202312
portable .  0.3333333333333333
One could  0.07692307692307693
could imagine  0.0625
imagine  1.0
imagine the  1.0
features indicating  0.038461538461538464
indicating  1.0
indicating important  1.0
important sentences  0.125
the news  0.001384083044982699
news domain  0.23076923076923078
domain might  0.05
might vary  0.038461538461538464
vary  1.0
vary considerably  0.16666666666666666
considerably  1.0
considerably from  1.0
the biomedical  0.0006920415224913495
biomedical  1.0
biomedical domain  1.0
domain .  0.3
the unsupervised  0.0006920415224913495
unsupervised ``  0.125
'' -  0.010752688172043012
based approach  0.018518518518518517
approach applies  0.02857142857142857
applies to  0.14285714285714285
any domain  0.03225806451612903
Incorporating diversity  1.0
diversity  1.0
diversity :  0.25
: GRASSHOPPER  0.00980392156862745
GRASSHOPPER  1.0
GRASSHOPPER algorithm  0.3333333333333333
algorithm As  0.03571428571428571
As mentioned  0.16666666666666666
mentioned above  0.16666666666666666
, multi-document  0.0005614823133071309
multi-document extractive  0.25
summarization faces  0.02
faces  1.0
faces a  1.0
a problem  0.0036809815950920245
potential redundancy  0.14285714285714285
redundancy  1.0
redundancy .  0.3333333333333333
would like  0.03773584905660377
like to  0.07142857142857142
extract sentences  0.25
are both  0.008298755186721992
both ``  0.06451612903225806
, contain  0.0005614823133071309
contain the  0.16666666666666666
the main  0.001384083044982699
main ideas  0.125
ideas -RRB-  0.25
`` diverse  0.005291005291005291
diverse  1.0
diverse ''  0.5
they differ  0.025
differ from  0.3333333333333333
one another  0.015384615384615385
another -RRB-  0.07692307692307693
LexRank deals  0.08333333333333333
deals  1.0
deals with  1.0
with diversity  0.00546448087431694
diversity as  0.25
heuristic final  0.3333333333333333
final stage  0.1111111111111111
stage  1.0
stage using  0.2
using CSIS  0.01694915254237288
CSIS ,  0.5
systems have  0.044642857142857144
used similar  0.008849557522123894
similar methods  0.037037037037037035
as Maximal  0.003484320557491289
Maximal  1.0
Maximal Marginal  1.0
Marginal  1.0
Marginal Relevance  1.0
Relevance  1.0
Relevance -LRB-  1.0
-LRB- MMR  0.0027100271002710027
MMR  1.0
MMR -RRB-  1.0
in trying  0.0018726591760299626
to eliminate  0.0026560424966799467
eliminate  1.0
eliminate redundancy  0.5
redundancy in  0.6666666666666666
in information  0.0018726591760299626
retrieval results  0.14285714285714285
We have  0.14285714285714285
have developed  0.009615384615384616
developed a  0.11538461538461539
algorithm like  0.03571428571428571
like Page\/Lex\/TextRank  0.03571428571428571
Page\/Lex\/TextRank  1.0
Page\/Lex\/TextRank that  1.0
that handles  0.0035460992907801418
handles  1.0
handles both  1.0
`` centrality  0.005291005291005291
centrality ''  0.5
`` diversity  0.005291005291005291
diversity ''  0.25
a unified  0.001226993865030675
unified  1.0
unified mathematical  1.0
mathematical framework  0.5
framework  1.0
framework based  0.25
on absorbing  0.0047169811320754715
absorbing  1.0
absorbing Markov  0.3333333333333333
Markov chain  0.05555555555555555
chain  1.0
chain random  1.0
walks .  0.5
An absorbing  0.0625
absorbing random  0.3333333333333333
walk is  0.2
is like  0.0040650406504065045
like a  0.07142857142857142
standard random  0.07142857142857142
walk ,  0.2
, except  0.0005614823133071309
except  1.0
except some  1.0
some states  0.012048192771084338
states  1.0
states are  0.25
are now  0.016597510373443983
now absorbing  0.07692307692307693
absorbing states  0.3333333333333333
states that  0.25
that act  0.0035460992907801418
act as  0.75
`` black  0.005291005291005291
black  1.0
black holes  1.0
holes  1.0
holes ''  1.0
that cause  0.0035460992907801418
cause  1.0
cause the  0.5
the walk  0.0006920415224913495
walk to  0.2
to end  0.0026560424966799467
end abruptly  0.125
abruptly  1.0
abruptly at  1.0
at that  0.014705882352941176
that state  0.0035460992907801418
state  1.0
state .  0.07142857142857142
The algorithm  0.005208333333333333
called GRASSHOPPER  0.05555555555555555
GRASSHOPPER for  0.3333333333333333
for reasons  0.0036101083032490976
reasons  1.0
reasons that  0.5
that should  0.0070921985815602835
should soon  0.05263157894736842
soon  1.0
soon become  0.3333333333333333
become clear  0.25
clear .  0.25
addition to  0.5
to explicitly  0.0026560424966799467
explicitly promoting  0.25
promoting  1.0
promoting diversity  1.0
diversity during  0.25
ranking process  0.14285714285714285
process ,  0.027777777777777776
, GRASSHOPPER  0.0005614823133071309
GRASSHOPPER incorporates  0.3333333333333333
incorporates  1.0
incorporates a  1.0
a prior  0.001226993865030675
prior ranking  0.3333333333333333
ranking -LRB-  0.14285714285714285
on sentence  0.0047169811320754715
summarization -RRB-  0.02
Maximum entropy-based  0.3333333333333333
entropy-based  1.0
entropy-based summarization  1.0
summarization It  0.02
abstractive method  0.16666666666666666
Even though  1.0
though automating  0.1
automating  1.0
automating abstractive  1.0
summarization research  0.02
most practical  0.017241379310344827
practical  1.0
practical systems  0.5
of extractive  0.00089126559714795
Extracted sentences  1.0
can form  0.0055248618784530384
a valid  0.001226993865030675
valid  1.0
valid summary  1.0
summary in  0.047619047619047616
in itself  0.0018726591760299626
itself or  0.2
or form  0.0045045045045045045
basis for  0.3333333333333333
for further  0.010830324909747292
further condensation  0.125
condensation  1.0
condensation operations  1.0
operations  1.0
operations .  1.0
, evaluation  0.0005614823133071309
of extracted  0.00089126559714795
extracted  1.0
extracted summaries  1.0
be automated  0.004219409282700422
automated ,  0.14285714285714285
since it  0.2
a classification  0.001226993865030675
classification task  0.058823529411764705
the DUC  0.0006920415224913495
DUC  1.0
DUC 2001  1.0
2001  1.0
2001 and  0.5
and 2002  0.001445086705202312
2002  1.0
2002 evaluation  0.5
evaluation workshops  0.018518518518518517
workshops  1.0
workshops ,  0.5
, TNO  0.0005614823133071309
TNO  1.0
TNO developed  1.0
sentence extraction  0.020833333333333332
for multi-document  0.0036101083032490976
summarization in  0.04
The system  0.03125
system was  0.053763440860215055
was based  0.012987012987012988
hybrid system  0.5
system using  0.010752688172043012
a naive  0.001226993865030675
naive  1.0
naive Bayes  0.5
Bayes classifier  0.3333333333333333
classifier and  0.14285714285714285
statistical language  0.030303030303030304
for modeling  0.0036101083032490976
modeling salience  0.14285714285714285
salience  1.0
salience .  1.0
system exhibited  0.010752688172043012
exhibited  1.0
exhibited good  1.0
good results  0.07692307692307693
results ,  0.09523809523809523
we wanted  0.022222222222222223
wanted  1.0
wanted to  1.0
to explore  0.0026560424966799467
explore  1.0
explore the  0.25
the effectiveness  0.0006920415224913495
effectiveness  1.0
effectiveness of  0.3333333333333333
a maximum  0.00245398773006135
maximum  1.0
maximum entropy  0.5
entropy  1.0
entropy -LRB-  0.2
-LRB- ME  0.0027100271002710027
ME  1.0
ME -RRB-  0.5
-RRB- classifier  0.0028169014084507044
classifier for  0.14285714285714285
the meeting  0.0006920415224913495
meeting  1.0
meeting summarization  1.0
summarization task  0.02
as ME  0.003484320557491289
ME is  0.5
is known  0.006097560975609756
known to  0.07692307692307693
be robust  0.004219409282700422
robust against  0.25
against  1.0
against feature  0.2
feature dependencies  0.07692307692307693
dependencies  1.0
dependencies .  1.0
Maximum entropy  0.6666666666666666
entropy has  0.2
has also  0.03571428571428571
also been  0.057971014492753624
applied successfully  0.06666666666666667
successfully  1.0
successfully for  0.3333333333333333
for summarization  0.0036101083032490976
the broadcast  0.0006920415224913495
broadcast  1.0
broadcast news  1.0
Aided summarization  0.3333333333333333
summarization Machine  0.02
Machine learning  0.1111111111111111
learning techniques  0.023255813953488372
techniques from  0.043478260869565216
from closely  0.009615384615384616
related fields  0.06666666666666667
fields such  0.16666666666666666
as information  0.003484320557491289
retrieval or  0.14285714285714285
or text  0.009009009009009009
text mining  0.012578616352201259
mining have  0.2
been successfully  0.014705882352941176
successfully adapted  0.3333333333333333
adapted  1.0
adapted to  1.0
to help  0.0026560424966799467
help automatic  0.1111111111111111
Apart from  1.0
from Fully  0.009615384615384616
Fully  1.0
Fully Automated  1.0
Automated Summarizers  0.5
Summarizers  1.0
Summarizers -LRB-  1.0
-LRB- FAS  0.0027100271002710027
FAS  1.0
FAS -RRB-  1.0
are systems  0.012448132780082987
systems that  0.0625
that aid  0.0035460992907801418
aid users  0.25
users with  0.1111111111111111
-LRB- MAHS  0.0027100271002710027
MAHS  1.0
MAHS =  1.0
= Machine  0.1111111111111111
Machine Aided  0.1111111111111111
Aided  0.6666666666666666
Aided Human  0.3333333333333333
Human  0.4
Human Summarization  0.2
Summarization  0.5
Summarization -RRB-  0.5
example by  0.024691358024691357
by highlighting  0.005714285714285714
highlighting  1.0
highlighting candidate  1.0
candidate  1.0
candidate passages  0.3333333333333333
passages  1.0
passages to  0.5
be included  0.004219409282700422
included in  0.125
and there  0.005780346820809248
that depend  0.0035460992907801418
depend  1.0
depend on  1.0
on post-processing  0.0047169811320754715
post-processing by  0.3333333333333333
-LRB- HAMS  0.0027100271002710027
HAMS  1.0
HAMS =  1.0
= Human  0.1111111111111111
Human Aided  0.2
Aided Machine  0.3333333333333333
Machine Summarization  0.1111111111111111
Evaluation An  0.1111111111111111
An ongoing  0.0625
ongoing  1.0
ongoing issue  0.5
issue in  0.125
this field  0.02197802197802198
field is  0.037037037037037035
Evaluation techniques  0.1111111111111111
techniques fall  0.043478260869565216
fall  1.0
fall into  0.5
into intrinsic  0.01282051282051282
intrinsic and  0.25
and extrinsic  0.001445086705202312
extrinsic ,  0.16666666666666666
, inter-texual  0.0005614823133071309
inter-texual  1.0
inter-texual and  0.5
and intra-texual  0.001445086705202312
intra-texual  1.0
intra-texual .  1.0
evaluation tests  0.037037037037037035
tests the  0.5
in of  0.0018726591760299626
of itself  0.00089126559714795
itself while  0.2
while an  0.05
an extrinsic  0.007575757575757576
summarization based  0.02
on how  0.009433962264150943
how it  0.06896551724137931
it affects  0.008547008547008548
affects  1.0
affects the  1.0
the completion  0.0006920415224913495
completion  1.0
completion of  1.0
other task  0.014285714285714285
Intrinsic evaluations  0.3333333333333333
evaluations  1.0
evaluations have  0.16666666666666666
have assessed  0.009615384615384616
assessed  1.0
assessed mainly  1.0
mainly the  0.16666666666666666
the coherence  0.0006920415224913495
coherence  1.0
coherence and  0.6666666666666666
and informativeness  0.001445086705202312
informativeness  1.0
informativeness of  0.6666666666666666
summaries .  0.13953488372093023
Extrinsic evaluations  0.5
evaluations ,  0.16666666666666666
, have  0.0011229646266142617
have tested  0.009615384615384616
tested  1.0
tested the  0.5
the impact  0.001384083044982699
impact  1.0
impact of  0.5
summarization on  0.02
like relevance  0.03571428571428571
relevance assessment  0.3333333333333333
assessment  1.0
assessment ,  1.0
, reading  0.0005614823133071309
reading comprehension  0.25
comprehension ,  0.14285714285714285
Intra-texual methods  1.0
methods assess  0.022727272727272728
assess  1.0
assess the  0.3333333333333333
specific summarization  0.047619047619047616
the inter-texual  0.0006920415224913495
inter-texual ones  0.5
ones focus  0.1
on contrastive  0.0047169811320754715
contrastive  1.0
contrastive analysis  1.0
of outputs  0.00089126559714795
outputs  1.0
outputs of  1.0
several summarization  0.045454545454545456
Human judgement  0.2
judgement  1.0
judgement often  0.3333333333333333
often has  0.045454545454545456
has wide  0.011904761904761904
wide  1.0
wide variance  0.25
variance  1.0
variance on  1.0
is considered  0.0040650406504065045
considered a  0.1111111111111111
`` good  0.005291005291005291
good ''  0.07692307692307693
'' summary  0.005376344086021506
that making  0.0035460992907801418
making the  0.2857142857142857
evaluation process  0.018518518518518517
process automatic  0.027777777777777776
automatic is  0.043478260869565216
particularly difficult  0.2
difficult .  0.07142857142857142
is both  0.0020325203252032522
both time  0.03225806451612903
time and  0.09090909090909091
and labor  0.001445086705202312
labor  1.0
labor intensive  0.5
intensive  1.0
intensive as  1.0
as it  0.003484320557491289
it requires  0.017094017094017096
requires humans  0.0625
humans to  0.08333333333333333
to read  0.0013280212483399733
read  1.0
read not  0.14285714285714285
summaries but  0.023255813953488372
also the  0.028985507246376812
Other issues  0.14285714285714285
issues are  0.2
are those  0.004149377593360996
those concerning  0.045454545454545456
concerning  1.0
concerning coherence  1.0
and coverage  0.001445086705202312
coverage .  0.3333333333333333
One of  0.15384615384615385
the metrics  0.0006920415224913495
metrics used  0.1111111111111111
in NIST  0.0018726591760299626
NIST 's  0.5
's annual  0.0196078431372549
annual  1.0
annual Document  0.5
Document  0.5
Document Understanding  0.25
Understanding Conferences  0.5
Conferences  0.5
Conferences ,  0.5
which research  0.007246376811594203
research groups  0.023809523809523808
groups submit  0.2
submit  1.0
submit their  0.5
their systems  0.058823529411764705
systems for  0.017857142857142856
for both  0.0036101083032490976
both summarization  0.03225806451612903
summarization and  0.02
translation tasks  0.013513513513513514
the ROUGE  0.0006920415224913495
ROUGE metric  0.2
metric -LRB-  0.3333333333333333
It essentially  0.02631578947368421
essentially calculates  0.125
calculates  1.0
calculates n-gram  1.0
n-gram overlaps  0.5
overlaps  1.0
overlaps between  0.5
between automatically  0.02564102564102564
automatically generated  0.14285714285714285
generated summaries  0.06666666666666667
and previously-written  0.001445086705202312
previously-written  1.0
previously-written human  1.0
human summaries  0.021739130434782608
A high  0.02
high level  0.16666666666666666
level of  0.35
of overlap  0.00089126559714795
overlap should  0.25
should indicate  0.05263157894736842
indicate  1.0
indicate a  0.3333333333333333
of shared  0.00089126559714795
shared concepts  0.5
concepts between  0.2
two summaries  0.034482758620689655
that overlap  0.0035460992907801418
overlap metrics  0.25
metrics like  0.1111111111111111
like this  0.03571428571428571
are unable  0.004149377593360996
provide any  0.16666666666666666
any feedback  0.03225806451612903
feedback  1.0
feedback on  0.5
summary 's  0.047619047619047616
's coherence  0.0196078431372549
coherence .  0.3333333333333333
Anaphor resolution  1.0
resolution remains  0.25
remains another  0.25
another problem  0.15384615384615385
problem yet  0.022727272727272728
yet to  0.5
be fully  0.008438818565400843
fully solved  0.16666666666666666
solved .  0.2
Evaluating summaries  1.0
either manually  0.1
manually or  0.25
automatically ,  0.047619047619047616
a hard  0.00245398773006135
hard task  0.16666666666666666
in evaluation  0.0018726591760299626
evaluation comes  0.018518518518518517
comes from  0.4
the impossibility  0.0006920415224913495
impossibility  1.0
impossibility of  1.0
of building  0.00089126559714795
building  1.0
building a  1.0
a fair  0.001226993865030675
fair  1.0
fair gold-standard  1.0
gold-standard  1.0
gold-standard against  1.0
against which  0.2
the results  0.002768166089965398
results of  0.047619047619047616
systems can  0.026785714285714284
be compared  0.004219409282700422
also very  0.028985507246376812
very hard  0.024390243902439025
hard to  0.3333333333333333
determine what  0.043478260869565216
a correct  0.00245398773006135
correct summary  0.06666666666666667
is ,  0.018292682926829267
because there  0.06666666666666667
is always  0.0040650406504065045
always the  0.3333333333333333
good summary  0.15384615384615385
is quite  0.0020325203252032522
quite different  0.375
from any  0.009615384615384616
any human  0.03225806451612903
human summary  0.021739130434782608
summary used  0.023809523809523808
an approximation  0.015151515151515152
approximation  1.0
approximation to  0.3333333333333333
correct output  0.06666666666666667
Current difficulties  0.2
difficulties in  0.5
in evaluating  0.0018726591760299626
evaluating summaries  0.4
summaries automatically  0.023255813953488372
automatically The  0.047619047619047616
evaluate the  0.5
the informativeness  0.0006920415224913495
of automatic  0.0017825311942959
automatic summaries  0.13043478260869565
compare them  0.14285714285714285
them with  0.05263157894736842
with human-made  0.00546448087431694
human-made  1.0
human-made model  0.5
as content  0.003484320557491289
content selection  0.08333333333333333
selection  1.0
selection is  1.0
not a  0.008928571428571428
a deterministic  0.00245398773006135
deterministic  1.0
deterministic problem  0.25
, different  0.0016844469399213925
different people  0.02040816326530612
people would  0.0625
would choose  0.018867924528301886
choose different  0.5
even ,  0.037037037037037035
same person  0.04
person may  0.05263157894736842
may chose  0.019230769230769232
chose  1.0
chose different  1.0
sentences at  0.013157894736842105
at different  0.014705882352941176
different times  0.02040816326530612
times ,  0.2
, showing  0.0005614823133071309
showing evidence  0.5
evidence  1.0
evidence of  0.5
of low  0.00089126559714795
low agreement  0.3333333333333333
agreement among  0.3333333333333333
among humans  0.125
humans as  0.08333333333333333
which sentences  0.007246376811594203
are good  0.004149377593360996
summary sentences  0.023809523809523808
Besides the  1.0
human variability  0.021739130434782608
variability  1.0
variability ,  1.0
the semantic  0.001384083044982699
semantic equivalence  0.047619047619047616
equivalence is  0.5
is another  0.0040650406504065045
because two  0.03333333333333333
two distinct  0.034482758620689655
distinct sentences  0.14285714285714285
same meaning  0.04
meaning but  0.043478260869565216
but not  0.058823529411764705
not using  0.008928571428571428
same words  0.04
This phenomenon  0.031746031746031744
phenomenon is  0.2
as paraphrase  0.003484320557491289
paraphrase  1.0
paraphrase .  1.0
can find  0.0055248618784530384
find an  0.15384615384615385
automatically evaluating  0.047619047619047616
summaries using  0.023255813953488372
using paraphrases  0.01694915254237288
paraphrases  1.0
paraphrases -LRB-  1.0
-LRB- ParaEval  0.0027100271002710027
ParaEval  1.0
ParaEval -RRB-  1.0
Moreover ,  1.0
most summarization  0.017241379310344827
systems perform  0.008928571428571428
perform an  0.09090909090909091
an extractive  0.007575757575757576
extractive approach  0.14285714285714285
, selecting  0.0005614823133071309
selecting and  0.2
and copying  0.001445086705202312
copying  1.0
copying important  1.0
sentences from  0.02631578947368421
Although humans  0.125
humans can  0.08333333333333333
also cut  0.014492753623188406
cut  1.0
cut and  1.0
and paste  0.001445086705202312
paste  1.0
paste relevant  1.0
relevant information  0.14285714285714285
information of  0.021739130434782608
the times  0.0006920415224913495
times they  0.2
they rephrase  0.025
rephrase  1.0
rephrase sentences  1.0
sentences when  0.013157894736842105
when necessary  0.02857142857142857
necessary ,  0.1
or they  0.0045045045045045045
they join  0.025
join  1.0
join different  1.0
different related  0.02040816326530612
related information  0.06666666666666667
into one  0.02564102564102564
summaries qualitatively  0.023255813953488372
qualitatively  1.0
qualitatively The  1.0
main drawback  0.125
drawback  1.0
drawback of  1.0
evaluation systems  0.018518518518518517
systems existing  0.008928571428571428
existing so  0.2
so far  0.03333333333333333
far is  0.125
that we  0.010638297872340425
need at  0.047619047619047616
least one  0.2
one reference  0.015384615384615385
and for  0.001445086705202312
for some  0.018050541516245487
some methods  0.024096385542168676
methods more  0.022727272727272728
compare automatic  0.14285714285714285
with models  0.00546448087431694
hard and  0.16666666666666666
and expensive  0.002890173410404624
expensive task  0.14285714285714285
Much effort  0.3333333333333333
effort has  0.25
have corpus  0.009615384615384616
of texts  0.0017825311942959
texts and  0.058823529411764705
their corresponding  0.029411764705882353
corresponding summaries  0.16666666666666666
methods presented  0.022727272727272728
presented  1.0
presented in  0.5
the previous  0.001384083044982699
previous Section  0.3333333333333333
Section  1.0
Section ,  1.0
, not  0.0039303761931499155
only do  0.02631578947368421
do we  0.038461538461538464
have human-made  0.009615384615384616
human-made summaries  0.5
summaries available  0.023255813953488372
for comparison  0.0036101083032490976
comparison  1.0
comparison ,  0.3333333333333333
also manual  0.014492753623188406
manual annotation  0.5
annotation has  0.25
be performed  0.008438818565400843
in some  0.00749063670411985
them -LRB-  0.05263157894736842
e.g. SCU  0.017857142857142856
SCU  1.0
SCU in  1.0
the Pyramid  0.0006920415224913495
Pyramid  1.0
Pyramid Method  1.0
Method  1.0
Method -RRB-  1.0
In any  0.01904761904761905
, what  0.0011229646266142617
evaluation methods  0.018518518518518517
need as  0.047619047619047616
an input  0.022727272727272728
summaries to  0.046511627906976744
to serve  0.0013280212483399733
as gold  0.003484320557491289
gold standards  0.16666666666666666
standards and  0.2
all perform  0.023255813953488372
perform a  0.2727272727272727
a quantitative  0.00245398773006135
quantitative evaluation  0.5
evaluation with  0.018518518518518517
to different  0.0013280212483399733
different similarity  0.02040816326530612
similarity metrics  0.1
metrics .  0.1111111111111111
To overcome  0.1111111111111111
overcome these  0.5
these problems  0.023809523809523808
we think  0.022222222222222223
think that  0.3333333333333333
the quantitative  0.0006920415224913495
evaluation might  0.018518518518518517
might not  0.07692307692307693
not be  0.10714285714285714
the only  0.0006920415224913495
only way  0.02631578947368421
evaluate summaries  0.25
a qualitative  0.00245398773006135
qualitative  1.0
qualitative automatic  0.5
be also  0.004219409282700422
also important  0.014492753623188406
important .  0.0625
Therefore ,  1.0
second aim  0.1
aim  1.0
aim of  0.5
this paper  0.01098901098901099
paper is  0.09090909090909091
to suggest  0.0013280212483399733
suggest  1.0
suggest a  0.3333333333333333
a novel  0.001226993865030675
novel  1.0
novel proposal  1.0
proposal  1.0
proposal for  1.0
evaluating automatically  0.2
automatically the  0.047619047619047616
qualitative manner  0.5
manner rather  0.25
than in  0.022222222222222223
quantitative one  0.25
Our evaluation  0.3333333333333333
evaluation approach  0.018518518518518517
a preliminary  0.001226993865030675
preliminary approach  0.3333333333333333
be studied  0.004219409282700422
studied  1.0
studied more  1.0
more deeply  0.010526315789473684
deeply  1.0
deeply ,  1.0
and developed  0.002890173410404624
the future  0.001384083044982699
future .  0.3333333333333333
Its main  0.5
main underlying  0.125
underlying  1.0
underlying idea  0.3333333333333333
idea is  0.14285714285714285
to define  0.0026560424966799467
define  1.0
define several  0.5
several quality  0.045454545454545456
quality criteria  0.1
criteria and  0.25
and check  0.001445086705202312
check  1.0
check how  0.5
how a  0.06896551724137931
a generated  0.001226993865030675
generated summary  0.06666666666666667
summary tackles  0.023809523809523808
tackles  1.0
tackles each  1.0
these ,  0.047619047619047616
that a  0.010638297872340425
reference model  0.125
model would  0.1
would not  0.018867924528301886
be necessary  0.008438818565400843
necessary anymore  0.1
anymore  1.0
anymore ,  1.0
, taking  0.0005614823133071309
taking  1.0
taking only  0.2
only into  0.02631578947368421
into consideration  0.01282051282051282
consideration  1.0
consideration the  0.3333333333333333
the automatic  0.001384083044982699
original source  0.07692307692307693
source .  0.041666666666666664
Once performed  0.4
performed ,  0.2
it could  0.008547008547008548
used together  0.008849557522123894
together with  0.125
with any  0.00546448087431694
other automatic  0.014285714285714285
automatic methodology  0.043478260869565216
methodology to  0.5
measure summary  0.09090909090909091
's informativeness  0.0196078431372549
informativeness .  0.3333333333333333
Language Generation  0.08333333333333333
Generation  1.0
Generation -LRB-  0.5
-LRB- NLG  0.008130081300813009
NLG  0.9047619047619048
NLG -RRB-  0.047619047619047616
the natural  0.001384083044982699
processing task  0.018518518518518517
of generating  0.00089126559714795
generating natural  0.2
language from  0.006756756756756757
machine representation  0.02531645569620253
representation system  0.05263157894736842
system such  0.010752688172043012
a knowledge  0.001226993865030675
base or  0.25
a logical  0.001226993865030675
logical form  0.16666666666666666
Psycholinguists prefer  1.0
prefer the  0.5
term language  0.05555555555555555
language production  0.006756756756756757
production when  0.3333333333333333
such formal  0.008130081300813009
representations are  0.25
are interpreted  0.004149377593360996
interpreted  1.0
interpreted as  1.0
as models  0.003484320557491289
for mental  0.0036101083032490976
mental  1.0
mental representations  0.3333333333333333
representations .  0.25
In a  0.01904761904761905
a sense  0.001226993865030675
sense ,  0.125
one can  0.015384615384615385
can say  0.0055248618784530384
say that  0.14285714285714285
that an  0.0035460992907801418
an NLG  0.007575757575757576
NLG system  0.09523809523809523
translator that  0.14285714285714285
that converts  0.0035460992907801418
converts  1.0
converts a  1.0
computer based  0.022727272727272728
based representation  0.018518518518518517
representation into  0.05263157894736842
language representation  0.006756756756756757
the methods  0.0006920415224913495
produce the  0.13636363636363635
final language  0.1111111111111111
language are  0.006756756756756757
are very  0.016597510373443983
very different  0.07317073170731707
from those  0.019230769230769232
those of  0.045454545454545456
a compiler  0.0036809815950920245
compiler  1.0
compiler due  0.3333333333333333
due to  0.4
the inherent  0.0006920415224913495
inherent  1.0
inherent expressivity  1.0
expressivity  1.0
expressivity of  1.0
NLG may  0.047619047619047616
understanding .  0.06060606060606061
The difference  0.005208333333333333
difference can  0.25
be put  0.004219409282700422
put this  0.25
way :  0.041666666666666664
: whereas  0.00980392156862745
whereas in  0.3333333333333333
in natural  0.013108614232209739
understanding the  0.12121212121212122
system needs  0.043010752688172046
needs to  0.4
to disambiguate  0.00398406374501992
disambiguate  1.0
disambiguate the  0.3333333333333333
input sentence  0.024390243902439025
sentence to  0.020833333333333332
the machine  0.0006920415224913495
representation language  0.10526315789473684
in NLG  0.0056179775280898875
NLG the  0.047619047619047616
make decisions  0.05
decisions about  0.2
about how  0.025
put a  0.25
a concept  0.001226993865030675
concept  1.0
concept into  0.25
The simplest  0.005208333333333333
simplest  1.0
simplest -LRB-  1.0
and perhaps  0.001445086705202312
perhaps trivial  0.16666666666666666
trivial -RRB-  0.25
-RRB- examples  0.0028169014084507044
examples are  0.041666666666666664
that generate  0.0035460992907801418
generate form  0.05555555555555555
form letters  0.05
letters .  0.4
Such systems  0.125
systems do  0.008928571428571428
typically involve  0.05555555555555555
involve grammar  0.16666666666666666
grammar rules  0.13513513513513514
but may  0.029411764705882353
may generate  0.019230769230769232
a letter  0.001226993865030675
letter to  0.16666666666666666
a consumer  0.001226993865030675
consumer  1.0
consumer ,  1.0
e.g. stating  0.017857142857142856
stating  1.0
stating that  1.0
a credit  0.00245398773006135
credit  1.0
credit card  1.0
card  1.0
card spending  0.25
spending  1.0
spending limit  1.0
limit is  0.25
is about  0.0020325203252032522
about to  0.025
be reached  0.004219409282700422
reached  1.0
reached .  0.5
More complex  0.1111111111111111
complex NLG  0.041666666666666664
NLG systems  0.23809523809523808
systems dynamically  0.008928571428571428
dynamically  1.0
dynamically create  0.5
create texts  0.058823529411764705
texts to  0.11764705882352941
meet a  0.25
a communicative  0.001226993865030675
communicative goal  0.6666666666666666
goal .  0.14285714285714285
As in  0.2222222222222222
in other  0.009363295880149813
other areas  0.02857142857142857
areas  1.0
areas of  0.3333333333333333
this can  0.01098901098901099
done using  0.09090909090909091
using either  0.01694915254237288
either explicit  0.1
explicit models  0.2
models of  0.038461538461538464
language -LRB-  0.013513513513513514
, grammars  0.0005614823133071309
grammars -RRB-  0.14285714285714285
the domain  0.001384083044982699
models derived  0.038461538461538464
derived by  0.3333333333333333
by analyzing  0.011428571428571429
analyzing human-written  0.2
human-written  1.0
human-written texts  0.5
texts .  0.17647058823529413
NLG is  0.09523809523809523
a fast-evolving  0.001226993865030675
fast-evolving  1.0
fast-evolving field  1.0
The best  0.005208333333333333
best single  0.05555555555555555
source for  0.041666666666666664
for up-to-date  0.0036101083032490976
up-to-date  1.0
up-to-date research  1.0
the area  0.001384083044982699
area is  0.18181818181818182
the SIGGEN  0.0006920415224913495
SIGGEN  1.0
SIGGEN portion  1.0
portion  1.0
portion of  1.0
the ACL  0.0006920415224913495
ACL  1.0
ACL Anthology  0.5
Anthology  1.0
Anthology .  1.0
Perhaps the  1.0
the closest  0.0006920415224913495
closest the  0.5
field comes  0.037037037037037035
comes to  0.2
a specialist  0.001226993865030675
specialist  1.0
specialist textbook  1.0
textbook  1.0
textbook is  0.5
is Reiter  0.0020325203252032522
Reiter  1.0
Reiter and  1.0
and Dale  0.001445086705202312
Dale  1.0
Dale -LRB-  1.0
-LRB- 2000  0.0027100271002710027
2000  1.0
2000 -RRB-  0.3333333333333333
this book  0.01098901098901099
book does  0.125
not describe  0.008928571428571428
describe developments  0.16666666666666666
developments in  0.6666666666666666
field since  0.037037037037037035
since 2000  0.1
2000 .  0.3333333333333333
This system  0.015873015873015872
system takes  0.010752688172043012
takes as  0.3333333333333333
input six  0.024390243902439025
six numbers  0.5
numbers  1.0
numbers ,  0.2857142857142857
which give  0.007246376811594203
give  1.0
give predicted  0.25
predicted  1.0
predicted pollen  0.5
pollen  1.0
pollen levels  0.6923076923076923
levels in  0.045454545454545456
of Scotland  0.0017825311942959
Scotland  1.0
Scotland .  0.4
From these  1.0
these numbers  0.047619047619047616
system generates  0.010752688172043012
generates  1.0
generates a  0.6666666666666666
short textual  0.125
textual summary  0.2
of pollen  0.0017825311942959
levels as  0.045454545454545456
the historical  0.0006920415224913495
historical  1.0
historical data  1.0
data for  0.012987012987012988
for 1-July-2005  0.0036101083032490976
1-July-2005  1.0
1-July-2005 ,  1.0
the software  0.0020761245674740486
software produces  0.037037037037037035
produces Grass  0.25
Grass  1.0
Grass pollen  1.0
levels for  0.13636363636363635
for Friday  0.010830324909747292
Friday  1.0
Friday have  1.0
have increased  0.028846153846153848
increased from  0.6
the moderate  0.0020761245674740486
moderate  1.0
moderate to  0.6
to high  0.00398406374501992
high levels  0.16666666666666666
levels of  0.3181818181818182
of yesterday  0.00267379679144385
yesterday  1.0
yesterday with  0.6666666666666666
with values  0.01639344262295082
values of  0.5
of around  0.0017825311942959
around 6  0.375
6  1.0
6 to  0.75
to 7  0.00398406374501992
7  1.0
7 across  0.42857142857142855
across most  0.6
most parts  0.05172413793103448
the country  0.0020761245674740486
country  1.0
country .  0.5
in Northern  0.0018726591760299626
Northern  1.0
Northern areas  0.3333333333333333
areas ,  0.16666666666666666
, pollen  0.0005614823133071309
levels will  0.09090909090909091
be moderate  0.004219409282700422
moderate with  0.2
of 4  0.0017825311942959
4  1.0
4 .  0.4
the actual  0.0020761245674740486
actual forecast  0.2
forecast  1.0
forecast -LRB-  1.0
-LRB- written  0.0027100271002710027
human meteorologist  0.021739130434782608
meteorologist  1.0
meteorologist -RRB-  1.0
-RRB- from  0.0028169014084507044
from this  0.009615384615384616
this data  0.01098901098901099
data was  0.012987012987012988
was Pollen  0.012987012987012988
Pollen  1.0
Pollen counts  1.0
counts  1.0
counts are  1.0
are expected  0.004149377593360996
expected to  0.2857142857142857
to remain  0.0013280212483399733
remain  1.0
remain high  1.0
high at  0.05555555555555555
at level  0.014705882352941176
level 6  0.05
6 over  0.25
over most  0.08333333333333333
Scotland ,  0.2
even level  0.037037037037037035
level 7  0.05
7 in  0.2857142857142857
the south  0.001384083044982699
south  1.0
south east  1.0
east  1.0
east .  1.0
only relief  0.02631578947368421
relief  1.0
relief is  1.0
the Northern  0.001384083044982699
Northern Isles  0.6666666666666666
Isles  1.0
Isles and  1.0
and far  0.002890173410404624
far northeast  0.25
northeast  1.0
northeast of  1.0
of mainland  0.0017825311942959
mainland  1.0
mainland Scotland  1.0
Scotland with  0.2
with medium  0.00546448087431694
medium  1.0
medium levels  0.3333333333333333
pollen count  0.07692307692307693
count .  0.2
Comparing these  1.0
these two  0.023809523809523808
two illustrates  0.034482758620689655
illustrates  1.0
illustrates some  0.5
the choices  0.0006920415224913495
choices that  0.2
that NLG  0.0070921985815602835
systems must  0.008928571428571428
must make  0.07142857142857142
make ;  0.05
; these  0.0425531914893617
these are  0.047619047619047616
are further  0.004149377593360996
further discussed  0.125
Stages The  1.0
The process  0.015625
generate text  0.05555555555555555
text can  0.006289308176100629
as keeping  0.003484320557491289
keeping  1.0
keeping a  0.5
of canned  0.00089126559714795
canned  1.0
canned text  0.5
text that  0.025157232704402517
is copied  0.0020325203252032522
copied  1.0
copied and  0.5
and pasted  0.001445086705202312
pasted  1.0
pasted ,  1.0
, possibly  0.0005614823133071309
possibly linked  0.5
linked with  0.3333333333333333
some glue  0.012048192771084338
glue  1.0
glue text  1.0
The results  0.005208333333333333
results may  0.047619047619047616
be satisfactory  0.004219409282700422
satisfactory  1.0
satisfactory in  1.0
in simple  0.0018726591760299626
simple domains  0.038461538461538464
domains such  0.125
as horoscope  0.003484320557491289
horoscope  1.0
horoscope machines  1.0
machines  1.0
machines or  0.25
or generators  0.0045045045045045045
generators  1.0
generators of  0.5
of personalised  0.00089126559714795
personalised  1.0
personalised business  1.0
business letters  0.25
a sophisticated  0.001226993865030675
sophisticated  1.0
sophisticated NLG  0.14285714285714285
include stages  0.037037037037037035
stages  1.0
stages of  0.5
of planning  0.00089126559714795
planning  1.0
planning and  0.5
and merging  0.001445086705202312
merging  1.0
merging of  0.5
information to  0.08695652173913043
to enable  0.0013280212483399733
enable  1.0
enable the  1.0
the generation  0.0006920415224913495
generation of  0.1111111111111111
that looks  0.0035460992907801418
looks natural  0.25
natural and  0.02666666666666667
and does  0.001445086705202312
not become  0.008928571428571428
become repetitive  0.25
repetitive  1.0
repetitive .  0.5
Typical stages  0.5
stages are  0.5
are :  0.008298755186721992
: Content  0.00980392156862745
Content  1.0
Content determination  1.0
determination  1.0
determination :  1.0
: Deciding  0.00980392156862745
Deciding  1.0
Deciding what  1.0
what information  0.03125
to mention  0.0013280212483399733
mention in  0.3333333333333333
the pollen  0.0006920415224913495
pollen example  0.07692307692307693
example above  0.012345679012345678
, deciding  0.0022459292532285235
deciding whether  0.3333333333333333
whether to  0.07692307692307693
explicitly mention  0.25
mention that  0.3333333333333333
that pollen  0.0035460992907801418
pollen level  0.15384615384615385
level is  0.05
is 7  0.0020325203252032522
Document structuring  0.25
structuring  1.0
structuring :  1.0
: Overall  0.00980392156862745
Overall  1.0
Overall organization  1.0
convey .  0.3333333333333333
deciding to  0.3333333333333333
to describe  0.0026560424966799467
the areas  0.001384083044982699
areas with  0.3333333333333333
with high  0.01092896174863388
high pollen  0.05555555555555555
levels first  0.045454545454545456
first ,  0.030303030303030304
, instead  0.0005614823133071309
with low  0.00546448087431694
low pollen  0.3333333333333333
levels .  0.045454545454545456
Aggregation :  1.0
: Merging  0.00980392156862745
Merging  1.0
Merging of  1.0
of similar  0.0017825311942959
improve readability  0.07692307692307693
readability  1.0
readability and  1.0
and naturalness  0.001445086705202312
naturalness  1.0
naturalness .  1.0
, merging  0.0005614823133071309
merging the  0.5
sentences Grass  0.013157894736842105
yesterday and  0.3333333333333333
and Grass  0.001445086705202312
be around  0.004219409282700422
country into  0.25
the single  0.0006920415224913495
single sentence  0.07142857142857142
sentence Grass  0.020833333333333332
Lexical choice  0.5
choice :  0.125
: Putting  0.00980392156862745
Putting  1.0
Putting words  1.0
words to  0.009174311926605505
the concepts  0.0006920415224913495
whether medium  0.07692307692307693
medium or  0.3333333333333333
or moderate  0.0045045045045045045
moderate should  0.2
used when  0.017699115044247787
when describing  0.05714285714285714
describing a  0.25
a pollen  0.001226993865030675
Referring expression  1.0
expression generation  0.1
: Creating  0.0196078431372549
Creating  1.0
Creating referring  0.5
referring expressions  0.5
expressions  1.0
expressions that  0.3333333333333333
that identify  0.0035460992907801418
identify objects  0.08333333333333333
objects and  0.2
and regions  0.001445086705202312
regions .  0.5
to use  0.013280212483399735
use in  0.013888888888888888
Scotland to  0.2
to refer  0.0013280212483399733
a certain  0.001226993865030675
certain region  0.14285714285714285
region  1.0
region in  1.0
in Scotland  0.0018726591760299626
This task  0.031746031746031744
task also  0.023809523809523808
also includes  0.014492753623188406
includes making  0.14285714285714285
making decisions  0.14285714285714285
about pronouns  0.025
pronouns and  0.5
other types  0.014285714285714285
of anaphora  0.00089126559714795
anaphora  1.0
anaphora .  1.0
Realisation :  1.0
Creating the  0.5
actual text  0.2
which should  0.007246376811594203
be correct  0.004219409282700422
correct according  0.06666666666666667
rules of  0.09302325581395349
of syntax  0.0017825311942959
, morphology  0.0011229646266142617
and orthography  0.001445086705202312
orthography .  0.5
using will  0.01694915254237288
be for  0.004219409282700422
future tense  0.3333333333333333
tense  1.0
tense of  0.5
of to  0.00089126559714795
be .  0.004219409282700422
Applications The  0.5
The popular  0.005208333333333333
popular media  0.1111111111111111
media has  0.16666666666666666
been especially  0.014705882352941176
especially interested  0.06666666666666667
interested  1.0
interested in  1.0
systems which  0.017857142857142856
which generate  0.021739130434782608
generate jokes  0.05555555555555555
jokes  1.0
jokes -LRB-  1.0
see computational  0.05
computational humor  0.1
humor  1.0
humor -RRB-  1.0
But from  0.16666666666666666
a commercial  0.00245398773006135
commercial  1.0
commercial perspective  0.09090909090909091
perspective ,  0.25
successful NLG  0.1111111111111111
NLG applications  0.047619047619047616
applications have  0.08
been data-to-text  0.014705882352941176
data-to-text  1.0
data-to-text systems  1.0
generate textual  0.05555555555555555
textual summaries  0.2
of databases  0.00089126559714795
databases and  0.125
data sets  0.03896103896103896
sets ;  0.09090909090909091
systems usually  0.017857142857142856
usually perform  0.03125
perform data  0.09090909090909091
data analysis  0.012987012987012988
analysis as  0.015384615384615385
as text  0.003484320557491289
text generation  0.006289308176100629
, several  0.0005614823133071309
several systems  0.045454545454545456
been built  0.014705882352941176
built  1.0
built that  0.3333333333333333
that produce  0.0035460992907801418
produce textual  0.045454545454545456
textual weather  0.2
weather forecasts  0.5714285714285714
forecasts  1.0
forecasts from  0.2
from weather  0.009615384615384616
weather data  0.14285714285714285
The earliest  0.005208333333333333
earliest such  0.5
such system  0.008130081300813009
be deployed  0.004219409282700422
deployed  1.0
deployed was  0.5
was FoG  0.012987012987012988
FoG  1.0
FoG ,  0.5
by Environment  0.005714285714285714
Environment  1.0
Environment Canada  1.0
Canada to  0.16666666666666666
generate weather  0.05555555555555555
forecasts in  0.2
in French  0.0018726591760299626
and English  0.001445086705202312
English in  0.02702702702702703
the early  0.0020761245674740486
early 1990s  0.1
1990s  1.0
1990s .  0.3333333333333333
The success  0.005208333333333333
of FoG  0.00089126559714795
FoG triggered  0.5
triggered  1.0
triggered other  1.0
other work  0.014285714285714285
both research  0.03225806451612903
and commercial  0.001445086705202312
commercial .  0.09090909090909091
this area  0.03296703296703297
area include  0.09090909090909091
include an  0.07407407407407407
an experiment  0.007575757575757576
experiment which  0.2
which showed  0.007246376811594203
that users  0.0035460992907801418
users sometimes  0.1111111111111111
sometimes preferred  0.07692307692307693
preferred  1.0
preferred computer-generated  1.0
computer-generated  1.0
computer-generated weather  1.0
forecasts to  0.2
to human-written  0.0013280212483399733
human-written ones  0.5
in part  0.0018726591760299626
part because  0.037037037037037035
the computer  0.001384083044982699
computer forecasts  0.022727272727272728
forecasts used  0.2
used more  0.008849557522123894
more consistent  0.010526315789473684
consistent  1.0
consistent terminology  1.0
terminology  1.0
terminology ,  1.0
demonstration that  0.2
that statistical  0.0035460992907801418
techniques could  0.043478260869565216
generate high-quality  0.05555555555555555
high-quality  1.0
high-quality weather  1.0
forecasts .  0.2
Recent applications  0.3333333333333333
applications include  0.08
the ARNS  0.0006920415224913495
ARNS  1.0
ARNS system  1.0
system used  0.010752688172043012
to summarise  0.00398406374501992
summarise  1.0
summarise conditions  0.3333333333333333
conditions in  0.2
in US  0.0018726591760299626
US  1.0
US ports  0.14285714285714285
ports  1.0
ports .  1.0
the 1990s  0.001384083044982699
1990s there  0.3333333333333333
was considerable  0.012987012987012988
considerable interest  0.2
in using  0.0056179775280898875
using NLG  0.05084745762711865
NLG to  0.14285714285714285
summarise financial  0.3333333333333333
financial and  0.25
and business  0.001445086705202312
business data  0.25
example the  0.012345679012345678
the SPOTLIGHT  0.0006920415224913495
SPOTLIGHT  1.0
SPOTLIGHT system  1.0
system developed  0.010752688172043012
developed at  0.07692307692307693
at A.C.  0.014705882352941176
A.C.  1.0
A.C. Nielsen  1.0
Nielsen  1.0
Nielsen automatically  1.0
generated readable  0.06666666666666667
readable English  0.3333333333333333
English text  0.02702702702702703
text based  0.006289308176100629
large amounts  0.043478260869565216
of retail  0.00089126559714795
retail  1.0
retail sales  1.0
sales  1.0
sales data  0.3333333333333333
More recently  0.1111111111111111
recently  1.0
recently there  0.3333333333333333
is growing  0.0020325203252032522
growing interest  0.5
summarise electronic  0.3333333333333333
electronic  1.0
electronic medical  0.5
medical records  0.3333333333333333
records  1.0
records .  0.5
Commercial applications  0.5
applications in  0.08
area are  0.09090909090909091
are starting  0.004149377593360996
starting  1.0
starting to  1.0
to appear  0.0026560424966799467
appear ,  0.0625
and researchers  0.001445086705202312
have shown  0.009615384615384616
shown that  0.2
NLG summaries  0.047619047619047616
of medical  0.0017825311942959
medical data  0.3333333333333333
data can  0.025974025974025976
be effective  0.008438818565400843
effective decision-support  0.16666666666666666
decision-support  1.0
decision-support aids  1.0
aids  1.0
aids for  1.0
for medical  0.0036101083032490976
medical professionals  0.16666666666666666
professionals  1.0
professionals .  1.0
There is  0.2727272727272727
also growing  0.014492753623188406
to enhance  0.0013280212483399733
enhance  1.0
enhance accessibility  1.0
accessibility  1.0
accessibility ,  1.0
by describing  0.005714285714285714
describing graphs  0.25
graphs  1.0
graphs and  1.0
sets to  0.09090909090909091
to blind  0.0013280212483399733
blind  1.0
blind people  0.75
people .  0.125
a highly  0.001226993865030675
highly interactive  0.1111111111111111
interactive use  0.25
of NLG  0.00089126559714795
the WYSIWYM  0.0006920415224913495
WYSIWYM  1.0
WYSIWYM framework  1.0
framework .  0.75
It stands  0.02631578947368421
stands  1.0
stands for  1.0
for What  0.0036101083032490976
What you  0.09090909090909091
you see  0.07692307692307693
see is  0.05
what you  0.03125
you meant  0.07692307692307693
meant and  0.5
allows users  0.25
users to  0.2222222222222222
to see  0.0026560424966799467
see and  0.05
and manipulate  0.001445086705202312
manipulate the  0.3333333333333333
the continuously  0.0006920415224913495
continuously  1.0
continuously rendered  1.0
rendered  1.0
rendered view  1.0
view -LRB-  0.3333333333333333
NLG output  0.047619047619047616
output -RRB-  0.038461538461538464
an underlying  0.007575757575757576
underlying formal  0.3333333333333333
formal language  0.2222222222222222
language document  0.006756756756756757
NLG input  0.047619047619047616
input -RRB-  0.024390243902439025
, thereby  0.0005614823133071309
thereby  1.0
thereby editing  1.0
editing  1.0
editing the  0.5
the formal  0.001384083044982699
language without  0.006756756756756757
without having  0.07692307692307693
having  1.0
having to  0.2
learn it  0.07692307692307693
Evaluation As  0.1111111111111111
other scientific  0.014285714285714285
scientific  1.0
scientific fields  0.5
, NLG  0.0005614823133071309
NLG researchers  0.047619047619047616
researchers need  0.1
to test  0.0026560424966799467
test how  0.1
well their  0.03571428571428571
, modules  0.0005614823133071309
modules  1.0
modules ,  0.5
and algorithms  0.001445086705202312
algorithms work  0.02857142857142857
are three  0.004149377593360996
three basic  0.3333333333333333
basic techniques  0.07692307692307693
techniques for  0.08695652173913043
evaluating NLG  0.2
systems :  0.008928571428571428
: task-based  0.00980392156862745
task-based  1.0
task-based -LRB-  0.25
-LRB- extrinsic  0.0027100271002710027
extrinsic -RRB-  0.16666666666666666
-RRB- evaluation  0.0028169014084507044
: give  0.0196078431372549
give the  0.5
the generated  0.001384083044982699
generated text  0.13333333333333333
and assess  0.001445086705202312
assess how  0.6666666666666666
well it  0.07142857142857142
it helps  0.008547008547008548
helps  1.0
helps him  0.5
him  1.0
him perform  0.5
a task  0.0036809815950920245
task -LRB-  0.023809523809523808
or otherwise  0.0045045045045045045
otherwise achieves  0.5
achieves  1.0
achieves its  0.5
its communicative  0.02857142857142857
goal -RRB-  0.14285714285714285
system which  0.010752688172043012
which generates  0.007246376811594203
generates summaries  0.3333333333333333
evaluated by  0.14285714285714285
by giving  0.005714285714285714
giving  1.0
giving these  0.5
to doctors  0.0013280212483399733
doctors  1.0
doctors ,  0.3333333333333333
and assessing  0.001445086705202312
assessing  1.0
assessing whether  1.0
summaries helps  0.023255813953488372
helps doctors  0.5
doctors make  0.3333333333333333
make better  0.05
better decisions  0.1111111111111111
decisions .  0.1
human ratings  0.08695652173913043
ratings :  0.1111111111111111
and ask  0.001445086705202312
ask him  0.25
him or  0.5
or her  0.009009009009009009
her  1.0
her to  0.5
to rate  0.0026560424966799467
rate  1.0
rate the  0.09090909090909091
quality and  0.1
and usefulness  0.001445086705202312
usefulness  1.0
usefulness of  1.0
metrics :  0.1111111111111111
: compare  0.00980392156862745
compare generated  0.14285714285714285
generated texts  0.06666666666666667
to texts  0.0013280212483399733
texts written  0.058823529411764705
by people  0.011428571428571429
people from  0.0625
using an  0.03389830508474576
automatic metric  0.043478260869565216
metric such  0.3333333333333333
as BLEU  0.003484320557491289
BLEU .  0.3333333333333333
Generally speaking  0.4
we ultimately  0.022222222222222223
ultimately  1.0
ultimately want  1.0
to know  0.0013280212483399733
know  1.0
know is  0.5
is how  0.0040650406504065045
how useful  0.034482758620689655
useful NLG  0.07142857142857142
are at  0.008298755186721992
at helping  0.014705882352941176
helping  1.0
helping people  1.0
people ,  0.0625
first of  0.030303030303030304
above techniques  0.07692307692307693
techniques .  0.043478260869565216
, task-based  0.0005614823133071309
task-based evaluations  0.75
evaluations are  0.3333333333333333
are time-consuming  0.004149377593360996
time-consuming and  0.3333333333333333
be difficult  0.004219409282700422
to carry  0.0013280212483399733
carry  1.0
carry out  1.0
out -LRB-  0.07142857142857142
-LRB- especially  0.005420054200542005
especially if  0.06666666666666667
if they  0.03571428571428571
require subjects  0.045454545454545456
subjects  1.0
subjects with  1.0
with specialised  0.00546448087431694
specialised  1.0
specialised expertise  0.5
expertise  1.0
expertise ,  1.0
as doctors  0.003484320557491289
doctors -RRB-  0.3333333333333333
Hence -LRB-  0.5
-RRB- task-based  0.005633802816901409
the exception  0.0006920415224913495
exception  1.0
exception ,  1.0
the norm  0.0006920415224913495
norm  1.0
norm .  1.0
In recent  0.01904761904761905
recent years  0.5
years researchers  0.047619047619047616
have started  0.009615384615384616
started  1.0
started trying  0.25
to assess  0.0013280212483399733
well human-ratings  0.03571428571428571
human-ratings  1.0
human-ratings and  1.0
and metrics  0.001445086705202312
metrics correlate  0.1111111111111111
with -LRB-  0.00546448087431694
-LRB- predict  0.0027100271002710027
predict -RRB-  0.16666666666666666
evaluations .  0.16666666666666666
Much of  0.3333333333333333
this work  0.01098901098901099
is being  0.006097560975609756
being conducted  0.05555555555555555
of Generation  0.00089126559714795
Generation Challenges  0.5
Challenges  1.0
Challenges shared-task  1.0
shared-task  1.0
shared-task events  1.0
events  1.0
events .  1.0
Initial results  1.0
results suggest  0.047619047619047616
suggest that  0.3333333333333333
that human  0.0070921985815602835
ratings are  0.2222222222222222
are much  0.008298755186721992
much better  0.045454545454545456
better than  0.1111111111111111
than metrics  0.022222222222222223
metrics in  0.1111111111111111
this regard  0.01098901098901099
regard .  0.2
In other  0.01904761904761905
other words  0.02857142857142857
ratings usually  0.1111111111111111
usually do  0.03125
do predict  0.038461538461538464
predict task-effectiveness  0.3333333333333333
task-effectiveness  1.0
task-effectiveness at  0.5
least to  0.2
degree -LRB-  0.16666666666666666
-LRB- although  0.005420054200542005
although there  0.16666666666666666
are exceptions  0.004149377593360996
exceptions  1.0
exceptions -RRB-  1.0
while ratings  0.05
ratings produced  0.1111111111111111
by metrics  0.005714285714285714
metrics often  0.1111111111111111
often do  0.022727272727272728
not predict  0.008928571428571428
task-effectiveness well  0.5
well .  0.07142857142857142
These results  0.058823529411764705
results are  0.19047619047619047
very preliminary  0.024390243902439025
preliminary ,  0.3333333333333333
, hopefully  0.0005614823133071309
hopefully  1.0
hopefully better  1.0
better data  0.1111111111111111
data will  0.012987012987012988
be available  0.004219409282700422
available soon  0.058823529411764705
soon .  0.3333333333333333
are currently  0.008298755186721992
currently the  0.14285714285714285
most popular  0.05172413793103448
popular evaluation  0.1111111111111111
evaluation technique  0.018518518518518517
technique in  0.14285714285714285
NLG ;  0.047619047619047616
; this  0.06382978723404255
is contrast  0.0020325203252032522
contrast to  0.25
to machine  0.00398406374501992
where metrics  0.02857142857142857
metrics are  0.1111111111111111
very widely  0.024390243902439025
widely  1.0
widely used  0.875
a subtopic  0.001226993865030675
subtopic  1.0
subtopic of  1.0
processing in  0.037037037037037035
in artificial  0.0018726591760299626
that deals  0.0035460992907801418
with machine  0.00546448087431694
machine reading  0.012658227848101266
comprehension .  0.42857142857142855
of disassembling  0.00089126559714795
disassembling  1.0
disassembling and  1.0
and parsing  0.001445086705202312
parsing input  0.03571428571428571
input is  0.024390243902439025
complex than  0.08333333333333333
than the  0.08888888888888889
the reverse  0.0006920415224913495
reverse  1.0
reverse process  0.5
of assembling  0.00089126559714795
assembling  1.0
assembling output  1.0
output in  0.038461538461538464
generation because  0.1111111111111111
because of  0.2
the occurrence  0.0006920415224913495
occurrence of  0.5
of unknown  0.00089126559714795
unknown  1.0
unknown and  1.0
and unexpected  0.001445086705202312
unexpected  1.0
unexpected features  1.0
features in  0.038461538461538464
input and  0.04878048780487805
appropriate syntactic  0.25
syntactic and  0.15384615384615385
semantic schemes  0.047619047619047616
schemes  1.0
schemes to  0.5
to apply  0.0013280212483399733
to it  0.0013280212483399733
, factors  0.0005614823133071309
factors  1.0
factors which  0.3333333333333333
are pre-determined  0.004149377593360996
pre-determined  1.0
pre-determined when  1.0
when outputting  0.02857142857142857
outputting  1.0
outputting language  0.5
is considerable  0.0020325203252032522
considerable commercial  0.2
commercial interest  0.09090909090909091
field because  0.037037037037037035
its application  0.02857142857142857
application to  0.07142857142857142
to news-gathering  0.0013280212483399733
news-gathering  1.0
news-gathering ,  1.0
, text  0.0016844469399213925
text categorization  0.006289308176100629
categorization  1.0
categorization ,  1.0
, voice-activation  0.0005614823133071309
voice-activation  1.0
voice-activation ,  1.0
, archiving  0.0005614823133071309
archiving  1.0
archiving and  1.0
and large-scale  0.001445086705202312
large-scale  1.0
large-scale content-analysis  1.0
content-analysis  1.0
content-analysis .  1.0
Eight years  1.0
years after  0.047619047619047616
after John  0.08333333333333333
John McCarthy  0.125
McCarthy  1.0
McCarthy coined  1.0
coined  1.0
coined the  1.0
term artificial  0.05555555555555555
intelligence ,  0.125
, Bobrow  0.0005614823133071309
Bobrow  1.0
Bobrow 's  1.0
's dissertation  0.0196078431372549
dissertation -LRB-  0.3333333333333333
-LRB- titled  0.0027100271002710027
titled  1.0
titled Natural  1.0
Language Input  0.08333333333333333
Input  1.0
Input for  0.5
a Computer  0.001226993865030675
Computer  1.0
Computer Problem  0.16666666666666666
Problem  1.0
Problem Solving  1.0
Solving  0.5
Solving System  0.5
System  1.0
System -RRB-  1.0
-RRB- showed  0.0028169014084507044
showed how  0.25
computer can  0.022727272727272728
can understand  0.0055248618784530384
understand simple  0.2857142857142857
simple natural  0.038461538461538464
solve algebra  0.25
algebra word  0.5
word problems  0.016666666666666666
problems .  0.17647058823529413
A year  0.02
year later  0.16666666666666666
later  1.0
later ,  0.5
in 1965  0.0018726591760299626
1965  1.0
1965 ,  0.5
, Joseph  0.0005614823133071309
Weizenbaum at  0.3333333333333333
at MIT  0.029411764705882353
MIT  1.0
MIT wrote  0.5
wrote ELIZA  0.16666666666666666
an interactive  0.007575757575757576
interactive program  0.25
program that  0.09090909090909091
that carried  0.0035460992907801418
carried  1.0
carried on  0.5
a dialogue  0.00245398773006135
dialogue  1.0
dialogue in  0.5
in English  0.009363295880149813
English on  0.02702702702702703
any topic  0.06451612903225806
popular being  0.1111111111111111
being psychotherapy  0.05555555555555555
psychotherapy  1.0
psychotherapy .  1.0
ELIZA worked  0.1111111111111111
worked by  0.2
by simple  0.005714285714285714
simple parsing  0.038461538461538464
parsing and  0.03571428571428571
and substitution  0.001445086705202312
of key  0.00089126559714795
key words  0.16666666666666666
into canned  0.01282051282051282
canned phrases  0.5
and Weizenbaum  0.001445086705202312
Weizenbaum sidestepped  0.3333333333333333
sidestepped  1.0
sidestepped the  1.0
of giving  0.00089126559714795
giving the  0.5
a database  0.0036809815950920245
database  1.0
database of  0.2
of real-world  0.00089126559714795
real-world knowledge  0.16666666666666666
knowledge or  0.037037037037037035
a rich  0.0036809815950920245
rich  1.0
rich lexicon  0.6
lexicon .  0.1111111111111111
Yet ELIZA  1.0
ELIZA gained  0.1111111111111111
gained  1.0
gained surprising  0.5
surprising  1.0
surprising popularity  1.0
popularity  1.0
popularity as  1.0
a toy  0.00245398773006135
toy  1.0
toy project  0.5
project and  0.07692307692307693
be seen  0.012658227848101266
seen as  0.3
very early  0.024390243902439025
early precursor  0.1
precursor  1.0
precursor to  1.0
to current  0.0013280212483399733
current commercial  0.14285714285714285
commercial systems  0.09090909090909091
systems such  0.017857142857142856
as those  0.017421602787456445
those used  0.13636363636363635
by Ask.com  0.005714285714285714
Ask.com  1.0
Ask.com .  1.0
In 1969  0.01904761904761905
1969  1.0
1969 Roger  0.5
Roger Schank  0.75
Schank at  0.2
at Stanford  0.014705882352941176
Stanford  1.0
Stanford University  0.5
University introduced  0.1111111111111111
introduced  1.0
introduced the  1.0
the conceptual  0.0006920415224913495
conceptual dependency  0.5
dependency theory  0.2
theory for  0.07692307692307693
This model  0.031746031746031744
, partially  0.0005614823133071309
partially  1.0
partially influenced  1.0
the work  0.001384083044982699
work of  0.08333333333333333
of Sydney  0.00089126559714795
Sydney  1.0
Sydney Lamb  1.0
Lamb  1.0
Lamb ,  1.0
was extensively  0.012987012987012988
extensively  1.0
extensively used  1.0
by Schank  0.005714285714285714
Schank 's  0.2
's students  0.0196078431372549
students at  0.6666666666666666
at Yale  0.029411764705882353
Yale  1.0
Yale University  0.5
University ,  0.1111111111111111
as Robert  0.003484320557491289
Robert Wilensky  0.25
, Wendy  0.0005614823133071309
Wendy  1.0
Wendy Lehnert  1.0
and Janet  0.001445086705202312
Janet Kolodner  0.5
Kolodner  1.0
Kolodner .  1.0
In 1970  0.009523809523809525
1970 ,  0.3333333333333333
William A.  0.5
A. Woods  0.2
Woods  1.0
Woods introduced  1.0
the augmented  0.0006920415224913495
augmented  1.0
augmented transition  1.0
transition  1.0
transition network  1.0
network  1.0
network -LRB-  0.16666666666666666
-LRB- ATN  0.0027100271002710027
ATN  1.0
ATN -RRB-  1.0
-RRB- to  0.011267605633802818
to represent  0.0013280212483399733
represent natural  0.1111111111111111
input .  0.04878048780487805
of phrase  0.00089126559714795
phrase structure  0.2
structure rules  0.08333333333333333
rules ATNs  0.023255813953488372
ATNs  0.6666666666666666
ATNs used  0.3333333333333333
used an  0.008849557522123894
an equivalent  0.007575757575757576
equivalent set  0.2
of finite  0.00089126559714795
finite  1.0
finite state  0.8
state automata  0.07142857142857142
automata  1.0
automata that  1.0
were called  0.024390243902439025
called recursively  0.05555555555555555
recursively  1.0
recursively .  0.5
ATNs and  0.3333333333333333
their more  0.029411764705882353
more general  0.031578947368421054
general format  0.045454545454545456
format  1.0
format called  0.5
called ``  0.2777777777777778
`` generalized  0.005291005291005291
generalized  1.0
generalized ATNs  1.0
ATNs ''  0.3333333333333333
'' continued  0.005376344086021506
continued  1.0
continued to  0.3333333333333333
of years  0.00089126559714795
years .  0.19047619047619047
In 1971  0.009523809523809525
1971  1.0
1971 Terry  0.3333333333333333
Terry  1.0
Terry Winograd  1.0
Winograd  0.6666666666666666
Winograd finished  0.3333333333333333
finished  1.0
finished writing  0.5
writing SHRDLU  0.1111111111111111
SHRDLU for  0.16666666666666666
for his  0.007220216606498195
his PhD  0.08333333333333333
PhD  1.0
PhD thesis  1.0
thesis  1.0
thesis at  1.0
MIT .  0.5
SHRDLU could  0.16666666666666666
could understand  0.0625
simple English  0.038461538461538464
English sentences  0.02702702702702703
a restricted  0.001226993865030675
restricted world  0.25
world of  0.06666666666666667
of children  0.00089126559714795
children  1.0
children 's  0.5
's blocks  0.0196078431372549
blocks to  0.25
to direct  0.0013280212483399733
direct a  0.16666666666666666
a robotic  0.001226993865030675
robotic  1.0
robotic arm  1.0
arm  1.0
arm to  1.0
to move  0.0013280212483399733
move  1.0
move items  1.0
items .  0.5
The successful  0.005208333333333333
successful demonstration  0.1111111111111111
of SHRDLU  0.00089126559714795
SHRDLU provided  0.16666666666666666
provided significant  0.2
significant momentum  0.1111111111111111
momentum  1.0
momentum for  1.0
for continued  0.0036101083032490976
continued research  0.2222222222222222
Winograd continued  0.3333333333333333
major influence  0.08333333333333333
influence  1.0
influence in  1.0
field with  0.037037037037037035
his book  0.08333333333333333
book Language  0.125
Language as  0.08333333333333333
a Cognitive  0.001226993865030675
Cognitive Process  0.3333333333333333
Process  1.0
Process .  1.0
At Stanford  0.3333333333333333
Stanford ,  0.5
, Winograd  0.0005614823133071309
Winograd was  0.3333333333333333
was later  0.012987012987012988
later the  0.1
the adviser  0.0006920415224913495
adviser  1.0
adviser for  1.0
for Larry  0.0036101083032490976
Larry  1.0
Larry Page  0.5
Page  1.0
Page ,  1.0
, who  0.0005614823133071309
who co-founded  0.1
co-founded  1.0
co-founded Google  1.0
the 1970s  0.0006920415224913495
1970s and  0.6666666666666666
and 1980s  0.002890173410404624
1980s the  0.1111111111111111
processing group  0.018518518518518517
group  1.0
group at  0.25
at SRI  0.014705882352941176
SRI  1.0
SRI International  1.0
International  1.0
International continued  1.0
and development  0.002890173410404624
development in  0.08333333333333333
A number  0.06
of commercial  0.00089126559714795
commercial efforts  0.18181818181818182
efforts  1.0
efforts based  0.14285714285714285
research were  0.023809523809523808
were undertaken  0.024390243902439025
undertaken  1.0
undertaken ,  0.5
in 1982  0.0018726591760299626
1982 Gary  0.3333333333333333
Gary  1.0
Gary Hendrix  1.0
Hendrix  1.0
Hendrix formed  1.0
formed Symantec  0.2
Symantec  1.0
Symantec Corporation  0.5
Corporation  1.0
Corporation originally  0.25
originally  1.0
originally as  0.5
a company  0.001226993865030675
company  1.0
company for  0.3333333333333333
for developing  0.007220216606498195
developing  1.0
developing a  0.25
language interface  0.006756756756756757
interface  1.0
interface for  0.25
for database  0.0036101083032490976
database queries  0.1
queries  1.0
queries on  0.3333333333333333
on personal  0.0047169811320754715
personal  1.0
personal computers  0.25
computers .  0.2222222222222222
the advent  0.0006920415224913495
advent  1.0
advent of  1.0
of mouse  0.00089126559714795
mouse  1.0
mouse driven  1.0
driven  1.0
driven ,  1.0
, graphic  0.0005614823133071309
graphic  1.0
graphic user  1.0
user interfaces  0.14285714285714285
interfaces  1.0
interfaces Symantec  0.5
Symantec changed  0.5
changed  1.0
changed direction  0.5
direction  1.0
direction .  0.3333333333333333
other commercial  0.014285714285714285
efforts were  0.14285714285714285
were started  0.024390243902439025
started around  0.25
, Larry  0.0005614823133071309
Larry R.  0.5
R. Harris  0.16666666666666666
Harris at  0.1111111111111111
the Artificial  0.0006920415224913495
Artificial  0.5
Artificial Intelligence  0.5
Intelligence Corporation  0.3333333333333333
Corporation and  0.25
and Roger  0.001445086705202312
Schank and  0.4
and his  0.001445086705202312
at Cognitive  0.014705882352941176
Cognitive Systems  0.3333333333333333
Systems  0.3333333333333333
Systems corp.  0.08333333333333333
corp.  1.0
corp. .  1.0
In 1983  0.009523809523809525
1983  1.0
1983 ,  1.0
Michael Dyer  0.25
Dyer  1.0
Dyer developed  1.0
developed the  0.15384615384615385
the BORIS  0.0006920415224913495
BORIS  1.0
BORIS system  1.0
system at  0.010752688172043012
Yale which  0.5
which bore  0.007246376811594203
bore  1.0
bore similarities  1.0
similarities  1.0
similarities to  0.5
of Roger  0.00089126559714795
and W.  0.001445086705202312
W.  1.0
W. G.  0.5
G. Lehnart  0.5
Lehnart  1.0
Lehnart .  1.0
Scope and  1.0
The umbrella  0.005208333333333333
umbrella  1.0
umbrella term  1.0
understanding ''  0.06060606060606061
a diverse  0.001226993865030675
diverse set  0.5
computer applications  0.022727272727272728
, ranging  0.0011229646266142617
ranging  1.0
ranging from  1.0
from small  0.019230769230769232
, relatively  0.0005614823133071309
relatively  1.0
relatively simple  1.0
simple tasks  0.038461538461538464
as short  0.003484320557491289
short commands  0.125
commands  1.0
commands issued  0.2
issued  1.0
issued to  1.0
to robots  0.0013280212483399733
robots  1.0
robots ,  1.0
to highly  0.0013280212483399733
highly complex  0.1111111111111111
complex endeavors  0.041666666666666664
endeavors  1.0
endeavors such  1.0
the full  0.0020761245674740486
full comprehension  0.2
of newspaper  0.00089126559714795
newspaper articles  0.3333333333333333
articles or  0.25
or poetry  0.0045045045045045045
poetry  1.0
poetry passages  1.0
passages .  0.5
Many real  0.08333333333333333
world applications  0.06666666666666667
applications fall  0.04
fall between  0.25
two extremes  0.034482758620689655
extremes  1.0
extremes ,  1.0
for instance  0.018050541516245487
instance text  0.07142857142857142
text classification  0.006289308176100629
automatic analysis  0.043478260869565216
of emails  0.00089126559714795
emails  1.0
emails and  0.5
their routing  0.029411764705882353
routing  1.0
routing to  0.3333333333333333
suitable department  0.25
department  1.0
department in  0.5
a corporation  0.001226993865030675
corporation  1.0
corporation does  1.0
not require  0.008928571428571428
require in  0.045454545454545456
in depth  0.0018726591760299626
depth  1.0
depth understanding  0.3333333333333333
but is  0.029411764705882353
is far  0.0040650406504065045
far more  0.25
the management  0.0006920415224913495
management  1.0
management of  0.2857142857142857
of simple  0.0017825311942959
simple queries  0.038461538461538464
queries to  0.3333333333333333
to database  0.0013280212483399733
database tables  0.1
tables  1.0
tables with  0.3333333333333333
with fixed  0.00546448087431694
fixed  1.0
fixed schemata  0.5
schemata  1.0
schemata .  1.0
Throughout the  1.0
the years  0.0006920415224913495
years various  0.047619047619047616
various attempts  0.05555555555555555
at processing  0.014705882352941176
processing natural  0.018518518518518517
language or  0.013513513513513514
or English-like  0.0045045045045045045
English-like  1.0
English-like sentences  0.3333333333333333
sentences presented  0.013157894736842105
presented to  0.16666666666666666
to computers  0.0013280212483399733
computers have  0.1111111111111111
have taken  0.009615384615384616
taken place  0.3333333333333333
place at  0.25
at varying  0.014705882352941176
varying  1.0
varying degrees  1.0
degrees  1.0
degrees of  0.5
of complexity  0.00089126559714795
complexity .  0.08333333333333333
Some attempts  0.047619047619047616
attempts have  0.16666666666666666
not resulted  0.008928571428571428
resulted  1.0
resulted in  1.0
in systems  0.003745318352059925
systems with  0.017857142857142856
with deep  0.00546448087431694
but have  0.029411764705882353
have helped  0.009615384615384616
helped  1.0
helped overall  0.3333333333333333
overall system  0.16666666666666666
system usability  0.010752688172043012
usability  1.0
usability .  1.0
, Wayne  0.0005614823133071309
Wayne  1.0
Wayne Ratliff  1.0
Ratliff  1.0
Ratliff originally  1.0
originally developed  0.5
the Vulcan  0.0006920415224913495
Vulcan  0.5
Vulcan program  0.5
program with  0.045454545454545456
an English-like  0.007575757575757576
English-like syntax  0.3333333333333333
to mimic  0.0013280212483399733
mimic  1.0
mimic the  1.0
the English  0.0020761245674740486
English speaking  0.02702702702702703
speaking computer  0.125
computer in  0.045454545454545456
in Star  0.0018726591760299626
Star  1.0
Star Trek  1.0
Trek  1.0
Trek .  1.0
Vulcan later  0.5
later became  0.1
became the  0.2
the dBase  0.0006920415224913495
dBase  1.0
dBase system  1.0
system whose  0.010752688172043012
whose easy-to-use  0.3333333333333333
easy-to-use  1.0
easy-to-use syntax  1.0
syntax effectively  0.09090909090909091
effectively launched  0.3333333333333333
launched  1.0
launched the  1.0
the personal  0.0006920415224913495
personal computer  0.25
computer database  0.022727272727272728
database industry  0.1
industry  1.0
industry .  0.3333333333333333
Systems with  0.08333333333333333
an easy  0.007575757575757576
easy  1.0
easy to  1.0
or English  0.0045045045045045045
English like  0.02702702702702703
like syntax  0.03571428571428571
syntax are  0.09090909090909091
are ,  0.004149377593360996
, quite  0.0005614823133071309
quite distinct  0.125
from systems  0.009615384615384616
that use  0.0070921985815602835
lexicon and  0.1111111111111111
and include  0.002890173410404624
internal representation  0.6
-LRB- often  0.005420054200542005
as first  0.003484320557491289
first order  0.030303030303030304
order logic  0.07142857142857142
logic -RRB-  0.25
the semantics  0.001384083044982699
semantics of  0.07142857142857142
language sentences  0.006756756756756757
Hence the  0.5
the breadth  0.0006920415224913495
breadth  1.0
breadth and  0.5
and depth  0.001445086705202312
depth of  0.3333333333333333
`` understanding  0.005291005291005291
'' aimed  0.005376344086021506
aimed  1.0
aimed at  1.0
at by  0.014705882352941176
system determine  0.010752688172043012
determine both  0.043478260869565216
both the  0.06451612903225806
the implied  0.0006920415224913495
implied  1.0
implied challenges  1.0
challenges  1.0
challenges -RRB-  0.5
applications it  0.04
can deal  0.0055248618784530384
deal with  0.5
with .  0.00546448087431694
The ``  0.010416666666666666
`` breadth  0.005291005291005291
breadth ''  0.5
'' of  0.005376344086021506
is measured  0.006097560975609756
measured  1.0
measured by  0.5
the sizes  0.0006920415224913495
sizes  1.0
sizes of  0.6666666666666666
its vocabulary  0.02857142857142857
and grammar  0.002890173410404624
grammar .  0.10810810810810811
`` depth  0.005291005291005291
depth ''  0.3333333333333333
the degree  0.0006920415224913495
degree to  0.16666666666666666
which its  0.007246376811594203
its understanding  0.02857142857142857
understanding approximates  0.030303030303030304
approximates  1.0
approximates that  0.5
a fluent  0.001226993865030675
fluent  1.0
fluent native  1.0
At the  0.3333333333333333
the narrowest  0.0006920415224913495
narrowest  1.0
narrowest and  1.0
and shallowest  0.001445086705202312
shallowest  1.0
shallowest ,  1.0
, English-like  0.0005614823133071309
English-like command  0.3333333333333333
command  1.0
command interpreters  0.5
interpreters  1.0
interpreters require  1.0
require minimal  0.045454545454545456
minimal  1.0
minimal complexity  1.0
complexity ,  0.16666666666666666
a small  0.00245398773006135
small range  0.1111111111111111
range  1.0
range of  0.5714285714285714
applications .  0.16
Narrow but  1.0
but deep  0.014705882352941176
deep systems  0.14285714285714285
systems explore  0.008928571428571428
explore and  0.25
and model  0.001445086705202312
model mechanisms  0.03333333333333333
mechanisms  1.0
mechanisms of  0.5
of understanding  0.00089126559714795
but they  0.04411764705882353
they still  0.025
still have  0.06666666666666667
have limited  0.009615384615384616
limited application  0.1
application .  0.07142857142857142
Systems that  0.3333333333333333
that attempt  0.0035460992907801418
to understand  0.00398406374501992
understand the  0.2857142857142857
the contents  0.0006920415224913495
contents  1.0
contents of  1.0
document such  0.027777777777777776
a news  0.00245398773006135
news release  0.07692307692307693
release  1.0
release beyond  0.3333333333333333
beyond simple  0.16666666666666666
simple keyword  0.038461538461538464
keyword  1.0
keyword matching  1.0
matching and  0.2
judge its  0.25
its suitability  0.02857142857142857
suitability  1.0
suitability for  0.5
user are  0.07142857142857142
are broader  0.004149377593360996
broader  1.0
broader and  1.0
require significant  0.045454545454545456
significant complexity  0.1111111111111111
still somewhat  0.06666666666666667
somewhat  1.0
somewhat shallow  0.5
shallow .  0.16666666666666666
both very  0.03225806451612903
very broad  0.04878048780487805
broad  1.0
broad and  0.25
and very  0.001445086705202312
very deep  0.024390243902439025
deep are  0.14285714285714285
are beyond  0.004149377593360996
current state  0.14285714285714285
state of  0.35714285714285715
the art  0.001384083044982699
art  1.0
art .  0.5
Components and  1.0
and architecture  0.001445086705202312
architecture  1.0
architecture Regardless  0.5
Regardless  1.0
Regardless of  1.0
the approach  0.0006920415224913495
approach used  0.02857142857142857
some common  0.012048192771084338
common components  0.04
components  1.0
components can  0.2
identified in  0.2
most natural  0.034482758620689655
understanding systems  0.030303030303030304
needs a  0.2
a lexicon  0.00245398773006135
lexicon of  0.2222222222222222
language and  0.013513513513513514
a parser  0.0036809815950920245
parser and  0.0625
rules to  0.06976744186046512
to break  0.0013280212483399733
break  1.0
break sentences  0.5
The construction  0.005208333333333333
lexicon with  0.1111111111111111
suitable ontology  0.25
ontology requires  0.5
requires significant  0.0625
significant effort  0.1111111111111111
effort ,  0.25
the Wordnet  0.0006920415224913495
Wordnet  1.0
Wordnet lexicon  1.0
lexicon required  0.1111111111111111
required many  0.14285714285714285
many person-years  0.019230769230769232
person-years  1.0
person-years of  1.0
of effort  0.00089126559714795
effort .  0.25
system also  0.010752688172043012
also needs  0.014492753623188406
a semantic  0.001226993865030675
semantic theory  0.09523809523809523
theory to  0.07692307692307693
to guide  0.0013280212483399733
the comprehension  0.0006920415224913495
The interpretation  0.005208333333333333
interpretation  1.0
interpretation capabilities  0.5
capabilities of  0.2
understanding system  0.030303030303030304
system depend  0.010752688172043012
theory it  0.07692307692307693
uses .  0.07142857142857142
Competing semantic  1.0
semantic theories  0.047619047619047616
language have  0.006756756756756757
have specific  0.009615384615384616
specific trade  0.047619047619047616
trade  1.0
trade offs  0.5
offs  1.0
offs in  1.0
in their  0.00749063670411985
their suitability  0.029411764705882353
suitability as  0.5
computer automated  0.022727272727272728
automated semantic  0.14285714285714285
semantic interpretation  0.047619047619047616
interpretation .  0.5
These range  0.058823529411764705
range from  0.2857142857142857
from naive  0.009615384615384616
naive semantics  0.5
semantics or  0.14285714285714285
or stochastic  0.0045045045045045045
stochastic semantic  0.125
semantic analysis  0.09523809523809523
analysis to  0.015384615384615385
of pragmatics  0.00089126559714795
pragmatics to  0.3333333333333333
to derive  0.0013280212483399733
derive  1.0
derive meaning  0.5
meaning from  0.043478260869565216
from context  0.009615384615384616
Advanced applications  0.2
applications of  0.04
understanding also  0.030303030303030304
also attempt  0.014492753623188406
to incorporate  0.0013280212483399733
incorporate  1.0
incorporate logical  1.0
logical inference  0.16666666666666666
inference within  0.25
within their  0.05555555555555555
their framework  0.029411764705882353
is generally  0.0020325203252032522
generally achieved  0.09090909090909091
by mapping  0.005714285714285714
mapping  1.0
mapping the  0.5
the derived  0.0006920415224913495
derived meaning  0.16666666666666666
meaning into  0.043478260869565216
of assertions  0.00089126559714795
assertions  1.0
assertions in  0.5
in predicate  0.0018726591760299626
predicate  1.0
predicate logic  1.0
logic ,  0.25
then using  0.02857142857142857
using logical  0.01694915254237288
logical deduction  0.16666666666666666
deduction  1.0
deduction to  1.0
to arrive  0.0013280212483399733
arrive  1.0
arrive at  1.0
at conclusions  0.014705882352941176
conclusions  1.0
conclusions .  1.0
on functional  0.0047169811320754715
functional  1.0
functional languages  0.5
as Lisp  0.003484320557491289
Lisp  1.0
Lisp hence  1.0
hence  1.0
hence need  0.5
a subsystem  0.001226993865030675
subsystem  1.0
subsystem for  1.0
the representation  0.0006920415224913495
of logical  0.00089126559714795
logical assertions  0.16666666666666666
assertions ,  0.5
while logic  0.05
logic oriented  0.25
oriented  1.0
oriented systems  1.0
those using  0.045454545454545456
language Prolog  0.006756756756756757
Prolog  1.0
Prolog generally  1.0
generally rely  0.09090909090909091
on an  0.014150943396226415
an extension  0.007575757575757576
extension  1.0
extension of  1.0
the built  0.0006920415224913495
built in  0.3333333333333333
in logical  0.0018726591760299626
logical representation  0.16666666666666666
representation framework  0.05263157894736842
The management  0.005208333333333333
of context  0.0017825311942959
context in  0.030303030303030304
understanding can  0.030303030303030304
can present  0.0055248618784530384
present special  0.16666666666666666
special  1.0
special challenges  0.2
challenges .  0.5
A large  0.02
large variety  0.043478260869565216
of examples  0.00089126559714795
and counter  0.001445086705202312
counter  1.0
counter examples  1.0
examples have  0.041666666666666664
have resulted  0.009615384615384616
in multiple  0.0018726591760299626
multiple approaches  0.07692307692307693
formal modeling  0.1111111111111111
modeling of  0.14285714285714285
context ,  0.12121212121212122
each with  0.022222222222222223
with specific  0.00546448087431694
specific strengths  0.047619047619047616
strengths and  0.5
and weaknesses  0.001445086705202312
weaknesses  1.0
weaknesses .  1.0
usually abbreviated  0.03125
abbreviated  1.0
abbreviated to  1.0
to OCR  0.0013280212483399733
OCR ,  0.14285714285714285
the mechanical  0.0006920415224913495
mechanical  1.0
mechanical or  1.0
or electronic  0.0045045045045045045
electronic conversion  0.5
of scanned  0.00089126559714795
scanned  1.0
scanned images  0.3333333333333333
images  1.0
images of  0.3333333333333333
of handwritten  0.00089126559714795
handwritten  1.0
handwritten ,  0.5
, typewritten  0.0011229646266142617
typewritten  1.0
typewritten or  0.2
or printed  0.0045045045045045045
into machine-encoded  0.01282051282051282
machine-encoded  1.0
machine-encoded text  1.0
is widely  0.0040650406504065045
a form  0.001226993865030675
data entry  0.03896103896103896
entry from  0.25
from some  0.009615384615384616
some sort  0.012048192771084338
of original  0.00089126559714795
original paper  0.07692307692307693
paper data  0.09090909090909091
data source  0.012987012987012988
source ,  0.041666666666666664
, whether  0.0005614823133071309
whether documents  0.07692307692307693
, sales  0.0005614823133071309
sales receipts  0.3333333333333333
receipts  1.0
receipts ,  1.0
, mail  0.0005614823133071309
mail  1.0
mail ,  0.5
any number  0.03225806451612903
of printed  0.0017825311942959
printed records  0.08333333333333333
is crucial  0.0020325203252032522
crucial  1.0
crucial to  1.0
the computerization  0.0006920415224913495
computerization  1.0
computerization of  1.0
printed texts  0.08333333333333333
texts so  0.058823529411764705
be electronically  0.004219409282700422
electronically  1.0
electronically searched  1.0
searched  1.0
searched ,  1.0
, stored  0.0005614823133071309
stored  1.0
stored more  1.0
more compactly  0.010526315789473684
compactly  1.0
compactly ,  1.0
, displayed  0.0005614823133071309
displayed  1.0
displayed on-line  0.5
on-line  1.0
on-line ,  0.3333333333333333
and used  0.001445086705202312
machine processes  0.012658227848101266
processes  1.0
processes such  0.2
as machine  0.003484320557491289
mining .  0.2
OCR is  0.061224489795918366
in pattern  0.0018726591760299626
intelligence and  0.125
and computer  0.001445086705202312
computer vision  0.022727272727272728
vision  1.0
vision .  1.0
Early versions  0.5
versions  1.0
versions needed  0.3333333333333333
be programmed  0.008438818565400843
programmed  1.0
programmed with  0.5
with images  0.00546448087431694
each character  0.022222222222222223
character ,  0.09090909090909091
and worked  0.001445086705202312
worked on  0.4
on one  0.009433962264150943
one font  0.015384615384615385
font  1.0
font at  0.3333333333333333
at a  0.058823529411764705
a time  0.00245398773006135
time .  0.12121212121212122
`` Intelligent  0.005291005291005291
Intelligent  1.0
Intelligent ''  0.3333333333333333
'' systems  0.016129032258064516
high degree  0.05555555555555555
of recognition  0.0017825311942959
recognition accuracy  0.05785123966942149
accuracy for  0.06451612903225806
for most  0.007220216606498195
most fonts  0.017241379310344827
fonts  1.0
fonts are  0.3333333333333333
now common  0.07692307692307693
are capable  0.008298755186721992
capable  1.0
capable of  1.0
of reproducing  0.00089126559714795
reproducing  1.0
reproducing formatted  1.0
formatted  1.0
formatted output  1.0
that closely  0.0035460992907801418
closely approximates  0.2
approximates the  0.5
original scanned  0.07692307692307693
scanned page  0.3333333333333333
page including  0.14285714285714285
including images  0.07142857142857142
images ,  0.3333333333333333
, columns  0.0005614823133071309
columns  1.0
columns and  1.0
other non-textual  0.014285714285714285
non-textual  1.0
non-textual components  1.0
components .  0.2
In 1914  0.009523809523809525
1914  1.0
1914 ,  1.0
Emanuel Goldberg  0.5
Goldberg  0.5
Goldberg developed  0.5
machine that  0.012658227848101266
that read  0.0035460992907801418
read characters  0.14285714285714285
characters and  0.0625
and converted  0.001445086705202312
converted  1.0
converted them  0.3333333333333333
them into  0.05263157894736842
into standard  0.01282051282051282
standard telegraph  0.07142857142857142
telegraph  1.0
telegraph code  1.0
code  1.0
code .  0.42857142857142855
-RRB- Around  0.0028169014084507044
Around  1.0
Around the  1.0
, Edmund  0.0005614823133071309
Edmund  1.0
Edmund Fournier  1.0
Fournier  1.0
Fournier d'Albe  1.0
d'Albe  1.0
d'Albe developed  1.0
the Optophone  0.0006920415224913495
Optophone  1.0
Optophone ,  1.0
a handheld  0.001226993865030675
handheld  1.0
handheld scanner  1.0
scanner  1.0
scanner that  0.3333333333333333
that when  0.0035460992907801418
when moved  0.02857142857142857
moved  1.0
moved across  1.0
across a  0.2
a printed  0.001226993865030675
printed page  0.08333333333333333
produced tones  0.1111111111111111
tones  1.0
tones that  1.0
that corresponded  0.0035460992907801418
corresponded  1.0
corresponded to  1.0
to specific  0.0013280212483399733
specific letters  0.047619047619047616
letters or  0.1
or characters  0.0045045045045045045
characters .  0.125
Goldberg continued  0.5
develop OCR  0.2
OCR technology  0.1836734693877551
for data  0.007220216606498195
entry .  0.25
Later ,  1.0
he proposed  0.14285714285714285
proposed photographing  0.1111111111111111
photographing  1.0
photographing data  1.0
data records  0.012987012987012988
records and  0.25
using photocells  0.01694915254237288
photocells  1.0
photocells ,  1.0
, matching  0.0005614823133071309
matching the  0.2
the photos  0.0006920415224913495
photos  1.0
photos against  1.0
against a  0.2
a template  0.00245398773006135
template  1.0
template containing  0.25
containing the  0.375
desired identification  0.2
identification pattern  0.2
pattern .  0.16666666666666666
In 1929  0.009523809523809525
1929  1.0
1929 Gustav  1.0
Gustav  1.0
Gustav Tauschek  1.0
Tauschek  1.0
Tauschek had  0.5
had similar  0.07142857142857142
similar ideas  0.037037037037037035
ideas ,  0.25
and obtained  0.001445086705202312
obtained a  0.2857142857142857
a patent  0.00245398773006135
patent  1.0
patent on  0.75
on OCR  0.0047169811320754715
OCR in  0.04081632653061224
Germany .  0.5
Paul W.  0.2
W. Handel  0.5
Handel  1.0
Handel also  1.0
also obtained  0.014492753623188406
a US  0.00245398773006135
US patent  0.2857142857142857
on such  0.0047169811320754715
such template-matching  0.008130081300813009
template-matching  1.0
template-matching OCR  1.0
technology in  0.045454545454545456
in USA  0.0018726591760299626
USA  1.0
USA in  1.0
in 1933  0.0018726591760299626
1933  1.0
1933 -LRB-  1.0
-LRB- U.S.  0.005420054200542005
U.S.  1.0
U.S. Patent  0.42857142857142855
Patent  1.0
Patent 1,915,993  0.3333333333333333
1,915,993  1.0
1,915,993 -RRB-  1.0
In 1935  0.009523809523809525
1935  1.0
1935 Tauschek  1.0
Tauschek was  0.5
was also  0.025974025974025976
also granted  0.014492753623188406
granted  1.0
granted a  1.0
on his  0.0047169811320754715
his method  0.08333333333333333
method -LRB-  0.0625
Patent 2,026,329  0.3333333333333333
2,026,329  1.0
2,026,329 -RRB-  1.0
In 1949  0.009523809523809525
1949 RCA  0.5
RCA  1.0
RCA engineers  0.2
engineers  1.0
engineers worked  1.0
first primitive  0.030303030303030304
primitive  1.0
primitive computer-type  1.0
computer-type  1.0
computer-type OCR  1.0
OCR to  0.061224489795918366
help blind  0.1111111111111111
people for  0.0625
the US  0.001384083044982699
US Veterans  0.14285714285714285
Veterans  1.0
Veterans Administration  1.0
Administration  1.0
Administration ,  1.0
but instead  0.014705882352941176
of converting  0.0017825311942959
converting  1.0
converting the  0.5
the printed  0.0006920415224913495
printed characters  0.08333333333333333
characters to  0.0625
machine language  0.0379746835443038
, their  0.0005614823133071309
their device  0.029411764705882353
device  1.0
device converted  0.5
converted it  0.3333333333333333
then spoke  0.02857142857142857
spoke  1.0
spoke the  1.0
the letters  0.0006920415224913495
letters :  0.1
: an  0.00980392156862745
an early  0.007575757575757576
early text-to-speech  0.1
text-to-speech technology  0.25
technology .  0.09090909090909091
It proved  0.02631578947368421
proved  1.0
proved far  0.3333333333333333
far too  0.125
too expensive  0.3333333333333333
expensive and  0.14285714285714285
and was  0.001445086705202312
was not  0.025974025974025976
not pursued  0.008928571428571428
pursued  1.0
pursued after  1.0
after testing  0.08333333333333333
testing .  0.2
David H.  0.25
H. Shepard  0.5
Shepard  0.6666666666666666
Shepard ,  0.3333333333333333
a cryptanalyst  0.001226993865030675
cryptanalyst  1.0
cryptanalyst at  1.0
the Armed  0.0006920415224913495
Armed  1.0
Armed Forces  1.0
Forces  1.0
Forces Security  1.0
Security  1.0
Security Agency  1.0
Agency  1.0
Agency in  0.5
States ,  0.14285714285714285
, addressed  0.0005614823133071309
addressed the  0.5
converting printed  0.5
printed messages  0.08333333333333333
messages  1.0
messages into  0.5
into machine  0.01282051282051282
computer processing  0.022727272727272728
processing and  0.018518518518518517
and built  0.001445086705202312
built a  0.3333333333333333
machine to  0.012658227848101266
this ,  0.04395604395604396
, called  0.0005614823133071309
`` Gismo  0.010582010582010581
Gismo  1.0
Gismo .  0.5
. ''  0.4375
He received  0.125
received a  0.5
patent for  0.25
his ``  0.08333333333333333
`` Apparatus  0.005291005291005291
Apparatus  1.0
Apparatus for  1.0
for Reading  0.0036101083032490976
Reading  0.5
Reading ''  0.5
in 1953  0.0018726591760299626
1953  1.0
1953 U.S.  1.0
Patent 2,663,758  0.3333333333333333
2,663,758  1.0
2,663,758 .  1.0
Gismo ''  0.5
'' could  0.005376344086021506
could read  0.0625
read 23  0.14285714285714285
23  1.0
23 letters  1.0
letters of  0.2
English alphabet  0.05405405405405406
alphabet  1.0
alphabet ,  0.6666666666666666
, comprehend  0.0005614823133071309
comprehend  1.0
comprehend Morse  1.0
Morse  1.0
Morse Code  1.0
Code  1.0
Code ,  1.0
, read  0.0011229646266142617
read musical  0.14285714285714285
musical  1.0
musical notations  1.0
notations ,  0.5
read aloud  0.14285714285714285
aloud  1.0
aloud from  1.0
from printed  0.009615384615384616
printed pages  0.08333333333333333
and duplicate  0.001445086705202312
duplicate typewritten  0.5
typewritten pages  0.2
Shepard went  0.3333333333333333
went  1.0
went on  0.2
to found  0.0013280212483399733
found Intelligent  0.07142857142857142
Intelligent Machines  0.3333333333333333
Machines  1.0
Machines Research  1.0
Research Corporation  0.125
Corporation -LRB-  0.5
-LRB- IMR  0.0027100271002710027
IMR  1.0
IMR -RRB-  0.5
which soon  0.007246376811594203
soon developed  0.3333333333333333
the world  0.0020761245674740486
world 's  0.06666666666666667
's first  0.0196078431372549
first commercial  0.06060606060606061
commercial OCR  0.18181818181818182
OCR systems  0.08163265306122448
In 1955  0.009523809523809525
commercial system  0.09090909090909091
was installed  0.012987012987012988
installed at  0.6666666666666666
the Reader  0.0006920415224913495
Reader  1.0
Reader 's  1.0
's Digest  0.058823529411764705
Digest  1.0
Digest ,  0.3333333333333333
which used  0.007246376811594203
used OCR  0.008849557522123894
input sales  0.024390243902439025
sales reports  0.3333333333333333
reports into  0.4
It converted  0.02631578947368421
converted the  0.3333333333333333
the typewritten  0.0006920415224913495
typewritten reports  0.2
into punched  0.01282051282051282
punched  1.0
punched cards  1.0
cards  1.0
cards for  1.0
for input  0.0036101083032490976
input into  0.024390243902439025
the magazine  0.0006920415224913495
magazine  1.0
magazine 's  1.0
's subscription  0.0196078431372549
subscription  1.0
subscription department  1.0
department ,  0.5
for help  0.0036101083032490976
help in  0.1111111111111111
in processing  0.0018726591760299626
processing the  0.018518518518518517
the shipment  0.0006920415224913495
shipment  1.0
shipment of  1.0
of 15-20  0.00089126559714795
15-20  1.0
15-20 million  1.0
million  1.0
million books  0.3333333333333333
books  1.0
books a  1.0
a year  0.001226993865030675
The second  0.005208333333333333
second system  0.1
was sold  0.012987012987012988
sold  1.0
sold to  0.3333333333333333
the Standard  0.0006920415224913495
Standard  1.0
Standard Oil  0.5
Oil  1.0
Oil Company  1.0
Company  1.0
Company for  0.5
for reading  0.007220216606498195
reading credit  0.125
card imprints  0.25
imprints  1.0
imprints for  1.0
for billing  0.0036101083032490976
billing  1.0
billing purposes  1.0
purposes .  0.5
Other systems  0.14285714285714285
systems sold  0.008928571428571428
sold by  0.3333333333333333
by IMR  0.005714285714285714
IMR during  0.5
late 1950s  0.1111111111111111
1950s included  0.25
included a  0.125
a bill  0.001226993865030675
bill  1.0
bill stub  0.5
stub  1.0
stub reader  1.0
reader to  0.1
the Ohio  0.0006920415224913495
Ohio  1.0
Ohio Bell  1.0
Bell  1.0
Bell Telephone  1.0
Telephone  1.0
Telephone Company  1.0
Company and  0.5
a page  0.001226993865030675
page scanner  0.14285714285714285
scanner to  0.3333333333333333
States Air  0.14285714285714285
Air  1.0
Air Force  0.6666666666666666
Force  1.0
Force for  0.5
and transmitting  0.001445086705202312
transmitting  1.0
transmitting by  1.0
by teletype  0.005714285714285714
teletype  1.0
teletype typewritten  1.0
typewritten messages  0.2
messages .  0.5
IBM and  0.3333333333333333
others were  0.08333333333333333
were later  0.024390243902439025
later licensed  0.1
licensed  1.0
licensed on  1.0
on Shepard  0.0047169811320754715
Shepard 's  0.3333333333333333
's OCR  0.0196078431372549
OCR patents  0.02040816326530612
patents  1.0
patents .  1.0
In about  0.009523809523809525
about 1965  0.025
, Reader  0.0005614823133071309
Digest and  0.3333333333333333
and RCA  0.001445086705202312
RCA collaborated  0.2
collaborated  1.0
collaborated to  1.0
to build  0.0013280212483399733
an OCR  0.007575757575757576
OCR Document  0.02040816326530612
Document reader  0.25
reader designed  0.2
to digitize  0.0013280212483399733
digitize  1.0
digitize the  1.0
the serial  0.0006920415224913495
serial  1.0
serial numbers  1.0
numbers on  0.14285714285714285
on Reader  0.0047169811320754715
Digest coupons  0.3333333333333333
coupons  1.0
coupons returned  1.0
from advertisements  0.009615384615384616
advertisements  1.0
advertisements .  1.0
The fonts  0.005208333333333333
fonts used  0.3333333333333333
used on  0.008849557522123894
the documents  0.0020761245674740486
documents were  0.02631578947368421
were printed  0.024390243902439025
printed by  0.08333333333333333
an RCA  0.015151515151515152
RCA Drum  0.2
Drum  1.0
Drum printer  1.0
printer  1.0
printer using  1.0
the OCR-A  0.0006920415224913495
OCR-A  1.0
OCR-A font  1.0
font .  0.6666666666666666
The reader  0.005208333333333333
reader was  0.2
was connected  0.012987012987012988
connected directly  0.2
directly to  0.4
to an  0.0013280212483399733
RCA 301  0.2
301  1.0
301 computer  1.0
computer -LRB-  0.022727272727272728
-LRB- one  0.0027100271002710027
first solid  0.030303030303030304
solid  1.0
solid state  1.0
state computers  0.07142857142857142
computers -RRB-  0.1111111111111111
This reader  0.015873015873015872
was followed  0.012987012987012988
a specialised  0.001226993865030675
specialised document  0.5
document reader  0.027777777777777776
reader installed  0.1
at TWA  0.014705882352941176
TWA  1.0
TWA where  1.0
reader processed  0.1
processed Airline  0.16666666666666666
Airline  1.0
Airline Ticket  1.0
Ticket  1.0
Ticket stock  1.0
stock  1.0
stock .  0.6666666666666666
The readers  0.005208333333333333
readers  1.0
readers processed  0.5
processed documents  0.16666666666666666
documents at  0.02631578947368421
a rate  0.001226993865030675
rate of  0.2727272727272727
of 1,500  0.00089126559714795
1,500  1.0
1,500 documents  1.0
documents per  0.02631578947368421
per  1.0
per minute  0.25
minute  1.0
minute ,  1.0
and checked  0.001445086705202312
checked each  0.5
each document  0.022222222222222223
, rejecting  0.0005614823133071309
rejecting  1.0
rejecting those  0.3333333333333333
those it  0.045454545454545456
not able  0.008928571428571428
process correctly  0.027777777777777776
correctly  1.0
correctly .  1.0
product became  0.14285714285714285
became part  0.2
the RCA  0.0006920415224913495
RCA product  0.2
product line  0.14285714285714285
line as  0.3333333333333333
a reader  0.001226993865030675
process ``  0.027777777777777776
`` Turn  0.005291005291005291
Turn  1.0
Turn around  1.0
around Documents  0.125
Documents  1.0
Documents ''  1.0
'' such  0.005376344086021506
those utility  0.045454545454545456
utility and  0.5
and insurance  0.001445086705202312
insurance  1.0
insurance bills  1.0
bills  1.0
bills returned  1.0
returned with  0.25
with payments  0.00546448087431694
payments  1.0
payments .  1.0
The United  0.005208333333333333
States Postal  0.14285714285714285
Postal  1.0
Postal Service  1.0
Service  1.0
Service has  1.0
been using  0.029411764705882353
using OCR  0.05084745762711865
OCR machines  0.02040816326530612
machines to  0.25
to sort  0.0013280212483399733
sort mail  0.3333333333333333
mail since  0.5
since 1965  0.1
1965 based  0.25
on technology  0.0047169811320754715
technology devised  0.045454545454545456
devised  1.0
devised primarily  0.5
primarily by  0.5
the prolific  0.0006920415224913495
prolific  1.0
prolific inventor  1.0
inventor  1.0
inventor Jacob  1.0
Jacob  1.0
Jacob Rabinow  1.0
Rabinow  1.0
Rabinow .  1.0
first use  0.030303030303030304
of OCR  0.0035650623885918
in Europe  0.003745318352059925
Europe was  0.2
the British  0.0006920415224913495
British  1.0
British General  0.3333333333333333
General  1.0
General Post  1.0
Post  1.0
Post Office  0.5
Office  1.0
Office -LRB-  1.0
-LRB- GPO  0.0027100271002710027
GPO  1.0
GPO -RRB-  1.0
In 1965  0.009523809523809525
1965 it  0.25
it began  0.008547008547008548
began planning  0.14285714285714285
planning an  0.5
an entire  0.007575757575757576
entire banking  0.3333333333333333
banking  1.0
banking system  1.0
the National  0.001384083044982699
National  1.0
National Giro  0.3333333333333333
Giro  1.0
Giro ,  1.0
that revolutionized  0.0035460992907801418
revolutionized  1.0
revolutionized bill  1.0
bill payment  0.5
payment  1.0
payment systems  1.0
systems in  0.008928571428571428
the UK  0.002768166089965398
UK  1.0
UK .  0.5
Canada Post  0.16666666666666666
Post has  0.5
systems since  0.008928571428571428
since 1971  0.1
1971 -LRB-  0.3333333333333333
systems read  0.008928571428571428
read the  0.14285714285714285
the name  0.001384083044982699
name and  0.2
and address  0.001445086705202312
address of  0.25
the addressee  0.0006920415224913495
addressee  1.0
addressee at  1.0
first mechanized  0.030303030303030304
mechanized  1.0
mechanized sorting  1.0
sorting  1.0
sorting center  1.0
center  1.0
center ,  1.0
and print  0.001445086705202312
print  1.0
print a  1.0
a routing  0.001226993865030675
routing bar  0.3333333333333333
bar  1.0
bar code  1.0
code on  0.14285714285714285
the envelope  0.0006920415224913495
envelope  1.0
envelope based  1.0
the postal  0.0006920415224913495
postal  1.0
postal code  1.0
To avoid  0.1111111111111111
avoid  1.0
avoid confusion  1.0
confusion  1.0
confusion with  1.0
the human-readable  0.0006920415224913495
human-readable  1.0
human-readable address  1.0
address field  0.25
field which  0.037037037037037035
be located  0.004219409282700422
located  1.0
located anywhere  1.0
anywhere  1.0
anywhere on  1.0
the letter  0.0006920415224913495
letter ,  0.16666666666666666
, special  0.0005614823133071309
special ink  0.2
ink  1.0
ink -LRB-  1.0
-LRB- orange  0.0027100271002710027
orange  1.0
orange in  1.0
in visible  0.0018726591760299626
visible  1.0
visible light  0.3333333333333333
light  1.0
light -RRB-  0.3333333333333333
used that  0.008849557522123894
is clearly  0.0040650406504065045
clearly  1.0
clearly visible  0.3333333333333333
visible under  0.3333333333333333
under ultraviolet  0.2
ultraviolet  1.0
ultraviolet light  1.0
light .  0.3333333333333333
Envelopes may  1.0
may then  0.019230769230769232
then be  0.02857142857142857
be processed  0.004219409282700422
with equipment  0.00546448087431694
equipment based  0.3333333333333333
on simple  0.0047169811320754715
simple bar  0.038461538461538464
code readers  0.14285714285714285
readers .  0.5
Importance of  1.0
the Blind  0.001384083044982699
Blind  1.0
Blind In  0.5
In 1974  0.009523809523809525
1974  1.0
1974 Ray  1.0
Ray  1.0
Ray Kurzweil  1.0
Kurzweil  1.0
Kurzweil started  0.14285714285714285
started the  0.25
the company  0.0006920415224913495
company Kurzweil  0.3333333333333333
Kurzweil Computer  0.2857142857142857
Computer Products  0.3333333333333333
Products  1.0
Products ,  0.5
, Inc.  0.0011229646266142617
Inc.  1.0
Inc. and  0.5
and continued  0.001445086705202312
continued development  0.1111111111111111
of omni-font  0.00089126559714795
omni-font  1.0
omni-font OCR  1.0
which could  0.007246376811594203
could recognize  0.0625
recognize  1.0
recognize text  0.1111111111111111
text printed  0.006289308176100629
printed in  0.08333333333333333
in virtually  0.0018726591760299626
virtually  1.0
virtually any  0.5
any font  0.03225806451612903
He decided  0.125
decided that  0.3333333333333333
best application  0.05555555555555555
this technology  0.01098901098901099
technology would  0.045454545454545456
be to  0.008438818565400843
a reading  0.001226993865030675
reading machine  0.125
machine for  0.012658227848101266
the blind  0.0006920415224913495
blind ,  0.25
which would  0.014492753623188406
would allow  0.018867924528301886
allow blind  0.2
people to  0.125
computer read  0.022727272727272728
read text  0.14285714285714285
them out  0.05263157894736842
out loud  0.07142857142857142
loud  1.0
loud .  1.0
This device  0.015873015873015872
device required  0.5
required the  0.14285714285714285
the invention  0.0006920415224913495
invention  1.0
invention of  1.0
of two  0.0017825311942959
two enabling  0.034482758620689655
enabling  1.0
enabling technologies  1.0
technologies  1.0
technologies --  0.25
the CCD  0.0006920415224913495
CCD  1.0
CCD flatbed  1.0
flatbed  1.0
flatbed scanner  1.0
scanner and  0.3333333333333333
the text-to-speech  0.0006920415224913495
text-to-speech synthesizer  0.25
synthesizer  1.0
synthesizer .  1.0
On January  0.16666666666666666
January 13  0.25
13 ,  0.5
1976 the  0.5
the successful  0.0006920415224913495
successful finished  0.1111111111111111
finished product  0.5
product was  0.14285714285714285
was unveiled  0.012987012987012988
unveiled  1.0
unveiled during  1.0
a widely-reported  0.001226993865030675
widely-reported  1.0
widely-reported news  1.0
news conference  0.07692307692307693
conference  1.0
conference headed  0.5
headed  1.0
headed by  1.0
by Kurzweil  0.005714285714285714
Kurzweil and  0.14285714285714285
the leaders  0.0006920415224913495
leaders  1.0
leaders of  1.0
National Federation  0.3333333333333333
Federation  1.0
Federation of  1.0
Blind -LRB-  0.5
In 1978  0.009523809523809525
1978 Kurzweil  0.3333333333333333
Products began  0.5
began selling  0.14285714285714285
selling  1.0
selling a  1.0
commercial version  0.09090909090909091
the optical  0.0006920415224913495
optical  1.0
optical character  1.0
recognition computer  0.01652892561983471
LexisNexis was  1.0
was one  0.012987012987012988
first customers  0.030303030303030304
customers  1.0
customers ,  0.5
and bought  0.001445086705202312
bought  1.0
bought the  1.0
to upload  0.0013280212483399733
upload  1.0
upload paper  1.0
paper legal  0.09090909090909091
legal and  0.3333333333333333
and news  0.001445086705202312
news documents  0.07692307692307693
documents onto  0.02631578947368421
onto  1.0
onto its  1.0
its nascent  0.02857142857142857
nascent  1.0
nascent online  1.0
online databases  0.125
databases .  0.5
Two years  0.42857142857142855
years later  0.14285714285714285
, Kurzweil  0.0016844469399213925
Kurzweil sold  0.14285714285714285
sold his  0.3333333333333333
his company  0.08333333333333333
company to  0.3333333333333333
to Xerox  0.0013280212483399733
Xerox  0.5
Xerox ,  0.5
which had  0.007246376811594203
had an  0.07142857142857142
an interest  0.007575757575757576
in further  0.0018726591760299626
further commercializing  0.125
commercializing  1.0
commercializing paper-to-computer  1.0
paper-to-computer  1.0
paper-to-computer text  1.0
text conversion  0.006289308176100629
conversion .  0.3333333333333333
Xerox eventually  0.5
eventually  1.0
eventually spun  1.0
spun  1.0
spun it  1.0
it off  0.008547008547008548
off  1.0
off as  0.5
as Scansoft  0.003484320557491289
Scansoft  1.0
Scansoft ,  1.0
which merged  0.007246376811594203
merged  1.0
merged with  1.0
with Nuance  0.00546448087431694
Nuance  1.0
Nuance Communications  0.6666666666666666
Communications  1.0
Communications -LRB-  1.0
OCR software  0.08163265306122448
software Desktop  0.037037037037037035
Desktop  1.0
Desktop &  1.0
& Server  0.125
Server  1.0
Server OCR  1.0
OCR Software  0.02040816326530612
Software  1.0
Software OCR  0.5
software and  0.037037037037037035
and ICR  0.002890173410404624
ICR  1.0
ICR software  0.3333333333333333
software technology  0.037037037037037035
technology are  0.045454545454545456
are analytical  0.004149377593360996
analytical artificial  0.5
intelligence systems  0.125
that consider  0.0035460992907801418
consider sequences  0.25
of characters  0.0017825311942959
characters rather  0.0625
than whole  0.022222222222222223
whole words  0.1111111111111111
Based on  1.0
of sequential  0.00089126559714795
sequential  1.0
sequential lines  1.0
lines  1.0
lines and  0.3333333333333333
and curves  0.001445086705202312
curves  1.0
curves ,  1.0
, OCR  0.0011229646266142617
OCR and  0.02040816326530612
ICR make  0.3333333333333333
make `  0.05
` best  0.0625
best guesses  0.05555555555555555
guesses  1.0
guesses '  1.0
' at  0.05263157894736842
at characters  0.014705882352941176
characters using  0.0625
using database  0.01694915254237288
database look-up  0.1
look-up  1.0
look-up tables  1.0
tables to  0.3333333333333333
to closely  0.0013280212483399733
closely associate  0.2
associate  1.0
associate or  0.5
or match  0.0045045045045045045
match the  0.3333333333333333
the strings  0.0006920415224913495
strings  1.0
strings of  1.0
characters that  0.0625
that form  0.0035460992907801418
form words  0.05
WebOCR &  0.75
& OnlineOCR  0.25
OnlineOCR  1.0
OnlineOCR With  0.3333333333333333
With  0.2857142857142857
With IT  0.14285714285714285
IT  1.0
IT technology  1.0
technology development  0.045454545454545456
development ,  0.16666666666666666
the platform  0.0006920415224913495
platform  1.0
platform for  0.5
for people  0.007220216606498195
use software  0.013888888888888888
software has  0.037037037037037035
been changed  0.014705882352941176
changed from  0.5
from single  0.009615384615384616
single PC  0.07142857142857142
PC  1.0
PC platform  0.25
platform to  0.5
to multi-platforms  0.0013280212483399733
multi-platforms  1.0
multi-platforms such  1.0
as PC  0.003484320557491289
PC +  0.25
+ Web-based  0.16666666666666666
Web-based  1.0
Web-based +  0.3333333333333333
+ Cloud  0.16666666666666666
Cloud  1.0
Cloud Computing  1.0
Computing +  0.5
+ Mobile  0.16666666666666666
Mobile  1.0
Mobile devices  0.3333333333333333
devices  1.0
devices .  0.5
After 30  0.3333333333333333
30  1.0
30 years  0.6666666666666666
years development  0.09523809523809523
software started  0.037037037037037035
started to  0.25
to adapt  0.0013280212483399733
adapt  1.0
adapt to  1.0
new application  0.041666666666666664
application requirements  0.07142857142857142
requirements  1.0
requirements .  0.5
WebOCR also  0.25
as OnlineOCR  0.003484320557491289
OnlineOCR or  0.3333333333333333
or Web-based  0.0045045045045045045
Web-based OCR  0.6666666666666666
OCR service  0.04081632653061224
service ,  0.4
, has  0.0005614823133071309
been a  0.029411764705882353
new trend  0.041666666666666664
trend  1.0
trend to  1.0
meet larger  0.25
larger volume  0.0625
volume and  0.25
and larger  0.002890173410404624
larger group  0.0625
group of  0.5
of users  0.0017825311942959
users after  0.1111111111111111
after 30  0.08333333333333333
the desktop  0.0006920415224913495
desktop  1.0
desktop OCR  1.0
OCR .  0.02040816326530612
Internet and  0.5
and broadband  0.001445086705202312
broadband  1.0
broadband technologies  1.0
technologies have  0.25
have made  0.009615384615384616
made WebOCR  0.0625
WebOCR  0.5
OnlineOCR practically  0.3333333333333333
practically  1.0
practically available  1.0
available to  0.058823529411764705
to both  0.00398406374501992
both individual  0.03225806451612903
individual users  0.08333333333333333
users and  0.1111111111111111
and enterprise  0.001445086705202312
enterprise  1.0
enterprise customers  1.0
customers .  0.5
Since 2000  0.2
2000 ,  0.3333333333333333
some major  0.012048192771084338
major OCR  0.16666666666666666
OCR vendors  0.02040816326530612
vendors  1.0
vendors began  0.25
began offering  0.14285714285714285
offering  1.0
offering WebOCR  1.0
& Online  0.125
Online software  0.5
software ,  0.037037037037037035
of new  0.00089126559714795
new entrants  0.041666666666666664
entrants  1.0
entrants companies  1.0
companies to  0.5
to seize  0.0013280212483399733
seize  1.0
seize the  1.0
the opportunity  0.0006920415224913495
opportunity  1.0
opportunity to  0.5
develop innovative  0.2
innovative  1.0
innovative Web-based  1.0
are free  0.004149377593360996
free  1.0
free of  0.25
of charge  0.00089126559714795
charge  1.0
charge services  1.0
services  1.0
services .  0.6666666666666666
Application-Oriented OCR  1.0
OCR Since  0.02040816326530612
Since  0.2
Since OCR  0.2
technology has  0.045454545454545456
more widely  0.010526315789473684
widely applied  0.125
to paper-intensive  0.0013280212483399733
paper-intensive  1.0
paper-intensive industry  1.0
industry ,  0.3333333333333333
is facing  0.0020325203252032522
facing  1.0
facing more  1.0
complex images  0.041666666666666664
images environment  0.16666666666666666
environment  1.0
environment in  0.16666666666666666
world .  0.13333333333333333
example :  0.024691358024691357
: complicated  0.00980392156862745
complicated backgrounds  0.3333333333333333
backgrounds  1.0
backgrounds ,  1.0
, degraded-images  0.0005614823133071309
degraded-images  1.0
degraded-images ,  1.0
, heavy-noise  0.0005614823133071309
heavy-noise  1.0
heavy-noise ,  1.0
, paper  0.0005614823133071309
paper skew  0.09090909090909091
skew  1.0
skew ,  1.0
, picture  0.0005614823133071309
picture  1.0
picture distortion  0.25
distortion  1.0
distortion ,  1.0
, low-resolution  0.0005614823133071309
low-resolution  1.0
low-resolution ,  1.0
, disturbed  0.0005614823133071309
disturbed  1.0
disturbed by  1.0
by grid  0.005714285714285714
grid  1.0
grid &  1.0
& lines  0.125
lines ,  0.3333333333333333
text image  0.006289308176100629
image consisting  0.3333333333333333
of special  0.00089126559714795
special fonts  0.2
fonts ,  0.3333333333333333
, symbols  0.0005614823133071309
symbols  1.0
symbols ,  0.3333333333333333
, glossary  0.0011229646266142617
glossary  1.0
glossary words  0.5
and etc.  0.001445086705202312
All the  1.0
the factors  0.0006920415224913495
factors affect  0.3333333333333333
affect  1.0
affect OCR  0.3333333333333333
OCR products  0.02040816326530612
products  1.0
products '  0.25
' stability  0.05263157894736842
stability  1.0
stability in  1.0
in recognition  0.003745318352059925
the major  0.001384083044982699
technology providers  0.045454545454545456
providers  1.0
providers began  1.0
develop dedicated  0.2
dedicated OCR  0.3333333333333333
each for  0.022222222222222223
for special  0.0036101083032490976
special types  0.2
of images  0.00089126559714795
images .  0.16666666666666666
They combine  0.3333333333333333
combine  1.0
combine various  0.6666666666666666
various optimization  0.05555555555555555
optimization  1.0
optimization methods  1.0
methods related  0.022727272727272728
the special  0.0006920415224913495
special image  0.2
image ,  0.3333333333333333
as business  0.003484320557491289
business rules  0.25
standard expression  0.07142857142857142
expression ,  0.2
glossary or  0.5
or dictionary  0.0045045045045045045
dictionary and  0.14285714285714285
and rich  0.001445086705202312
rich information  0.2
information contained  0.021739130434782608
contained  1.0
contained in  1.0
in color  0.0018726591760299626
color  1.0
color images  1.0
improve the  0.07692307692307693
the recognition  0.001384083044982699
Such strategy  0.125
customize OCR  0.5
`` Application-Oriented  0.005291005291005291
Application-Oriented  0.5
OCR ''  0.04081632653061224
`` Customized  0.005291005291005291
Customized  1.0
Customized OCR  1.0
, widely  0.0005614823133071309
the fields  0.0006920415224913495
of Business-card  0.00089126559714795
Business-card  1.0
Business-card OCR  1.0
, Invoice  0.0005614823133071309
Invoice  1.0
Invoice OCR  1.0
, Screenshot  0.0005614823133071309
Screenshot  1.0
Screenshot OCR  1.0
, ID  0.0005614823133071309
ID  1.0
ID card  1.0
card OCR  0.25
, Driver-license  0.0005614823133071309
Driver-license  1.0
Driver-license OCR  1.0
OCR or  0.02040816326530612
or Auto  0.0045045045045045045
Auto  1.0
Auto plant  1.0
plant  1.0
plant OCR  1.0
See also  0.5
also :  0.028985507246376812
: List  0.00980392156862745
List  1.0
List of  1.0
of optical  0.00089126559714795
recognition software  0.024793388429752067
software Current  0.037037037037037035
Current state  0.2
technology This  0.045454545454545456
This section  0.031746031746031744
section needs  0.16666666666666666
needs additional  0.1
additional citations  0.16666666666666666
citations  1.0
citations for  0.3333333333333333
for verification  0.0036101083032490976
verification  1.0
verification .  1.0
help improve  0.1111111111111111
article by  0.034482758620689655
adding citations  0.5
citations to  0.6666666666666666
to reliable  0.0013280212483399733
reliable sources  0.25
sources  1.0
sources .  0.3333333333333333
Unsourced material  1.0
material  1.0
material may  0.5
be challenged  0.004219409282700422
challenged  1.0
challenged and  1.0
and removed  0.001445086705202312
removed  1.0
removed .  1.0
May 2009  0.5
2009 -RRB-  0.3333333333333333
-RRB- Commissioned  0.0028169014084507044
Commissioned  1.0
Commissioned by  1.0
the U.S.  0.002768166089965398
U.S. Department  0.14285714285714285
Department  1.0
Department of  1.0
of Energy  0.00089126559714795
Energy  1.0
Energy -LRB-  1.0
-LRB- DOE  0.0027100271002710027
DOE  1.0
DOE -RRB-  1.0
the Information  0.0006920415224913495
Information Science  0.2
Science  1.0
Science Research  0.5
Research Institute  0.125
Institute  1.0
Institute -LRB-  1.0
-LRB- ISRI  0.0027100271002710027
ISRI  1.0
ISRI -RRB-  1.0
-RRB- had  0.0028169014084507044
had the  0.07142857142857142
the mission  0.0006920415224913495
mission  1.0
mission to  1.0
to foster  0.0013280212483399733
foster  1.0
foster the  1.0
the improvement  0.0006920415224913495
improvement of  0.5
of automated  0.00089126559714795
automated technologies  0.14285714285714285
technologies for  0.25
for understanding  0.0036101083032490976
understanding machine  0.030303030303030304
machine printed  0.012658227848101266
printed documents  0.08333333333333333
it conducted  0.008547008547008548
conducted the  0.2
most authoritative  0.017241379310344827
authoritative  1.0
authoritative of  1.0
the Annual  0.0006920415224913495
Annual  1.0
Annual Test  1.0
Test  1.0
Test of  1.0
OCR Accuracy  0.02040816326530612
Accuracy  0.2857142857142857
Accuracy for  0.14285714285714285
for 5  0.0036101083032490976
5  1.0
5 consecutive  0.5
consecutive  1.0
consecutive years  0.5
years in  0.047619047619047616
the mid-90s  0.0006920415224913495
mid-90s  1.0
mid-90s .  1.0
Recognition of  0.25
of Latin-script  0.00089126559714795
Latin-script  1.0
Latin-script ,  1.0
typewritten text  0.2
text is  0.025157232704402517
is still  0.008130081300813009
still not  0.06666666666666667
not 100  0.008928571428571428
100  1.0
100 %  0.6666666666666666
% accurate  0.05128205128205128
accurate even  0.14285714285714285
even where  0.037037037037037035
where clear  0.02857142857142857
clear imaging  0.25
imaging  1.0
imaging is  1.0
is available  0.0020325203252032522
available .  0.17647058823529413
One study  0.07692307692307693
study based  0.25
on recognition  0.0047169811320754715
of 19th  0.00089126559714795
19th  1.0
19th -  1.0
- and  0.125
and early  0.001445086705202312
early 20th-century  0.1
20th-century  1.0
20th-century newspaper  1.0
newspaper pages  0.3333333333333333
pages concluded  0.14285714285714285
concluded  1.0
concluded that  1.0
that character-by-character  0.0035460992907801418
character-by-character  1.0
character-by-character OCR  1.0
OCR accuracy  0.02040816326530612
for commercial  0.0036101083032490976
software varied  0.037037037037037035
varied  1.0
varied from  1.0
from 71  0.009615384615384616
71  1.0
71 %  1.0
% to  0.05128205128205128
to 98  0.0013280212483399733
98  1.0
98 %  1.0
% ;  0.02564102564102564
; total  0.02127659574468085
total accuracy  0.5
accuracy can  0.03225806451612903
achieved only  0.1
only by  0.05263157894736842
human review  0.021739130434782608
review  1.0
review .  0.3333333333333333
Other areas  0.14285714285714285
areas --  0.16666666666666666
-- including  0.04
including recognition  0.07142857142857142
of hand  0.00089126559714795
hand printing  0.07142857142857142
printing  1.0
printing ,  1.0
, cursive  0.0005614823133071309
cursive  1.0
cursive handwriting  0.2
handwriting  1.0
handwriting ,  0.5
and printed  0.001445086705202312
other scripts  0.014285714285714285
those East  0.045454545454545456
East  1.0
East Asian  1.0
Asian  1.0
Asian language  1.0
language characters  0.006756756756756757
characters which  0.125
which have  0.014492753623188406
many strokes  0.019230769230769232
strokes  1.0
strokes for  1.0
single character  0.07142857142857142
character -RRB-  0.045454545454545456
-RRB- --  0.0028169014084507044
still the  0.06666666666666667
subject of  0.25
of active  0.00089126559714795
active  1.0
active research  0.5
Accuracy rates  0.2857142857142857
rates  1.0
rates can  0.125
be measured  0.004219409282700422
measured in  0.16666666666666666
in several  0.003745318352059925
several ways  0.045454545454545456
ways ,  0.25
and how  0.004335260115606936
how they  0.10344827586206896
are measured  0.004149377593360996
measured can  0.16666666666666666
can greatly  0.0055248618784530384
greatly affect  0.14285714285714285
affect the  0.3333333333333333
the reported  0.0006920415224913495
reported  1.0
reported accuracy  0.2
accuracy rate  0.06451612903225806
rate .  0.09090909090909091
if word  0.03571428571428571
word context  0.016666666666666666
context -LRB-  0.06060606060606061
-LRB- basically  0.0027100271002710027
basically  1.0
basically a  1.0
not used  0.017857142857142856
to correct  0.0013280212483399733
correct software  0.06666666666666667
software finding  0.037037037037037035
finding non-existent  0.2
non-existent  1.0
non-existent words  1.0
a character  0.001226993865030675
character error  0.045454545454545456
error rate  0.4166666666666667
of 1  0.00089126559714795
1 %  0.5
% -LRB-  0.07692307692307693
-LRB- 99  0.0027100271002710027
99  1.0
99 %  1.0
% accuracy  0.10256410256410256
accuracy -RRB-  0.06451612903225806
-RRB- may  0.0028169014084507044
may result  0.019230769230769232
result in  0.09090909090909091
an error  0.007575757575757576
of 5  0.00089126559714795
5 %  0.5
-LRB- 95  0.0027100271002710027
95  1.0
95 %  1.0
or worse  0.0045045045045045045
worse  1.0
worse if  1.0
the measurement  0.0006920415224913495
measurement  1.0
measurement is  0.5
on whether  0.0047169811320754715
whether each  0.07692307692307693
each whole  0.022222222222222223
whole word  0.1111111111111111
word was  0.016666666666666666
was recognized  0.012987012987012988
recognized  1.0
recognized with  0.16666666666666666
with no  0.01092896174863388
no incorrect  0.07692307692307693
incorrect  1.0
incorrect letters  0.3333333333333333
On-line character  0.6666666666666666
recognition is  0.0743801652892562
sometimes confused  0.07692307692307693
with Optical  0.00546448087431694
Optical  0.3333333333333333
Optical Character  0.3333333333333333
Character  1.0
Character Recognition  1.0
Recognition  0.75
Recognition -LRB-  0.125
see Handwriting  0.05
Handwriting  1.0
Handwriting recognition  1.0
recognition -RRB-  0.008264462809917356
an instance  0.007575757575757576
of off-line  0.00089126559714795
off-line  1.0
off-line character  1.0
system recognizes  0.010752688172043012
recognizes  1.0
recognizes the  1.0
the fixed  0.0006920415224913495
fixed static  0.5
static  1.0
static shape  1.0
shape  1.0
shape of  1.0
the character  0.0006920415224913495
while on-line  0.05
on-line character  0.3333333333333333
recognition instead  0.008264462809917356
instead recognizes  0.14285714285714285
the dynamic  0.0006920415224913495
dynamic motion  0.2
motion  1.0
motion during  1.0
during handwriting  0.1
handwriting .  0.5
, on-line  0.0005614823133071309
on-line recognition  0.3333333333333333
as that  0.003484320557491289
that used  0.0035460992907801418
for gestures  0.0036101083032490976
gestures in  0.5
the Penpoint  0.0006920415224913495
Penpoint  1.0
Penpoint OS  1.0
OS  1.0
OS or  0.5
the Tablet  0.0006920415224913495
Tablet  1.0
Tablet PC  1.0
PC can  0.25
can tell  0.0055248618784530384
tell  1.0
tell whether  0.3333333333333333
whether a  0.15384615384615385
a horizontal  0.001226993865030675
horizontal  1.0
horizontal mark  1.0
mark was  0.3333333333333333
was drawn  0.012987012987012988
drawn  1.0
drawn right-to-left  1.0
right-to-left  1.0
right-to-left ,  1.0
or left-to-right  0.0045045045045045045
left-to-right  1.0
left-to-right .  1.0
also referred  0.014492753623188406
by other  0.005714285714285714
other terms  0.014285714285714285
terms such  0.07692307692307693
as dynamic  0.003484320557491289
dynamic character  0.2
, real-time  0.0005614823133071309
real-time character  0.5
and Intelligent  0.001445086705202312
Intelligent Character  0.3333333333333333
Recognition or  0.125
or ICR  0.0045045045045045045
ICR .  0.3333333333333333
On-line systems  0.3333333333333333
for recognizing  0.0036101083032490976
recognizing hand-printed  0.2
hand-printed  1.0
hand-printed text  0.5
text on  0.006289308176100629
the fly  0.0006920415224913495
fly  1.0
fly have  1.0
have become  0.009615384615384616
become well  0.25
well known  0.03571428571428571
as commercial  0.003484320557491289
commercial products  0.09090909090909091
products in  0.25
in recent  0.003745318352059925
years -LRB-  0.047619047619047616
see Tablet  0.05
PC history  0.25
history -RRB-  0.25
Among these  1.0
input devices  0.04878048780487805
devices for  0.25
for personal  0.0036101083032490976
personal digital  0.25
digital assistants  0.14285714285714285
assistants  1.0
assistants such  1.0
those running  0.045454545454545456
running Palm  0.3333333333333333
Palm  1.0
Palm OS  1.0
OS .  0.5
The Apple  0.005208333333333333
Apple  1.0
Apple Newton  1.0
Newton  1.0
Newton pioneered  1.0
pioneered this  0.3333333333333333
this product  0.01098901098901099
product .  0.14285714285714285
algorithms used  0.02857142857142857
these devices  0.023809523809523808
devices take  0.25
the order  0.001384083044982699
, speed  0.0005614823133071309
and direction  0.001445086705202312
direction of  0.3333333333333333
of individual  0.0017825311942959
individual lines  0.08333333333333333
lines segments  0.3333333333333333
segments at  0.2
at input  0.014705882352941176
input are  0.024390243902439025
known .  0.038461538461538464
user can  0.07142857142857142
be retrained  0.004219409282700422
retrained  1.0
retrained to  1.0
only specific  0.02631578947368421
specific letter  0.047619047619047616
letter shapes  0.3333333333333333
shapes  1.0
shapes .  0.3333333333333333
methods can  0.022727272727272728
in software  0.0018726591760299626
software that  0.037037037037037035
that scans  0.0035460992907801418
scans  1.0
scans paper  1.0
paper documents  0.09090909090909091
so accurate  0.03333333333333333
accurate recognition  0.14285714285714285
of hand-printed  0.0017825311942959
hand-printed documents  0.25
documents is  0.02631578947368421
still largely  0.06666666666666667
largely  1.0
largely an  0.2
an open  0.015151515151515152
open problem  0.25
rates of  0.375
of 80  0.00089126559714795
80  1.0
80 %  1.0
to 90  0.0013280212483399733
% on  0.02564102564102564
on neat  0.0047169811320754715
neat  1.0
neat ,  1.0
, clean  0.0005614823133071309
clean hand-printed  0.5
hand-printed characters  0.25
achieved ,  0.2
that accuracy  0.0035460992907801418
rate still  0.09090909090909091
still translates  0.06666666666666667
translates  1.0
translates to  1.0
to dozens  0.0013280212483399733
dozens  1.0
dozens of  1.0
of errors  0.00089126559714795
errors per  0.2
per page  0.25
, making  0.0005614823133071309
the technology  0.0006920415224913495
technology useful  0.045454545454545456
useful only  0.07142857142857142
only in  0.05263157894736842
in very  0.0056179775280898875
limited applications  0.1
of cursive  0.00089126559714795
cursive text  0.2
an active  0.007575757575757576
active area  0.5
with recognition  0.00546448087431694
recognition rates  0.01652892561983471
rates even  0.125
even lower  0.037037037037037035
lower  1.0
lower than  0.2
than that  0.044444444444444446
Higher rates  1.0
of general  0.00089126559714795
general cursive  0.045454545454545456
cursive script  0.4
script  1.0
script will  0.25
likely not  0.0625
be possible  0.008438818565400843
possible without  0.041666666666666664
without the  0.07692307692307693
of contextual  0.00089126559714795
contextual  1.0
contextual or  0.5
or grammatical  0.0045045045045045045
grammatical information  0.09090909090909091
, recognizing  0.0005614823133071309
recognizing entire  0.2
entire words  0.3333333333333333
dictionary is  0.14285714285714285
is easier  0.0040650406504065045
easier than  0.25
than trying  0.022222222222222223
to parse  0.005312084993359893
parse individual  0.1111111111111111
individual characters  0.08333333333333333
characters from  0.0625
from script  0.009615384615384616
script .  0.5
Reading the  0.5
the Amount  0.0006920415224913495
Amount  1.0
Amount line  1.0
line of  0.3333333333333333
a cheque  0.001226993865030675
cheque  1.0
cheque -LRB-  1.0
always a  0.3333333333333333
a written-out  0.001226993865030675
written-out  1.0
written-out number  1.0
number -RRB-  0.046511627906976744
example where  0.012345679012345678
where using  0.02857142857142857
a smaller  0.001226993865030675
smaller  1.0
smaller dictionary  0.14285714285714285
dictionary can  0.14285714285714285
can increase  0.0055248618784530384
increase recognition  0.25
rates greatly  0.125
greatly .  0.2857142857142857
Knowledge of  0.5
grammar of  0.05405405405405406
being scanned  0.05555555555555555
scanned can  0.3333333333333333
also help  0.014492753623188406
help determine  0.1111111111111111
if a  0.03571428571428571
word is  0.06666666666666667
a verb  0.007361963190184049
, allowing  0.0005614823133071309
allowing greater  0.3333333333333333
greater accuracy  0.3333333333333333
The shapes  0.005208333333333333
shapes of  0.6666666666666666
individual cursive  0.08333333333333333
cursive characters  0.2
characters themselves  0.0625
themselves  1.0
themselves simply  0.25
simply do  0.08333333333333333
not contain  0.008928571428571428
contain enough  0.08333333333333333
enough information  0.2
to accurately  0.0013280212483399733
accurately -LRB-  0.5
-LRB- greater  0.0027100271002710027
greater than  0.3333333333333333
than 98  0.022222222222222223
% -RRB-  0.02564102564102564
-RRB- recognize  0.0028169014084507044
recognize all  0.1111111111111111
all handwritten  0.023255813953488372
handwritten cursive  0.5
is necessary  0.0040650406504065045
necessary to  0.2
understand that  0.14285714285714285
that OCR  0.0035460992907801418
basic technology  0.07692307692307693
technology also  0.045454545454545456
in advanced  0.0018726591760299626
advanced scanning  0.4
scanning  1.0
scanning applications  0.5
Due to  1.0
an advanced  0.007575757575757576
scanning solution  0.5
solution  1.0
solution can  1.0
be unique  0.004219409282700422
unique  1.0
unique and  1.0
and patented  0.001445086705202312
patented  1.0
patented and  1.0
not easily  0.026785714285714284
easily copied  0.1111111111111111
copied despite  0.5
despite being  0.3333333333333333
being based  0.05555555555555555
this basic  0.01098901098901099
basic OCR  0.07692307692307693
For more  0.03278688524590164
complex recognition  0.041666666666666664
recognition problems  0.008264462809917356
, intelligent  0.0005614823133071309
intelligent  1.0
intelligent character  1.0
generally used  0.09090909090909091
as artificial  0.003484320557491289
artificial neural  0.09090909090909091
neural  1.0
neural networks  0.5333333333333333
networks can  0.07142857142857142
made indifferent  0.0625
indifferent  1.0
indifferent to  1.0
both affine  0.03225806451612903
affine  1.0
affine and  1.0
and non-linear  0.001445086705202312
non-linear  1.0
non-linear transformations  1.0
transformations .  0.5
A technique  0.02
technique which  0.14285714285714285
is having  0.0020325203252032522
having considerable  0.2
considerable success  0.2
success in  0.2
recognizing difficult  0.2
difficult words  0.03571428571428571
and character  0.001445086705202312
character groups  0.045454545454545456
groups within  0.2
within documents  0.05555555555555555
documents generally  0.02631578947368421
generally amenable  0.09090909090909091
amenable  1.0
amenable to  1.0
to computer  0.0013280212483399733
computer OCR  0.022727272727272728
to submit  0.0013280212483399733
submit them  0.5
them automatically  0.05263157894736842
automatically to  0.047619047619047616
to humans  0.0013280212483399733
humans in  0.08333333333333333
the reCAPTCHA  0.0006920415224913495
reCAPTCHA  1.0
reCAPTCHA system  1.0
In corpus  0.009523809523809525
, part-of-speech  0.0011229646266142617
part-of-speech tagging  0.4666666666666667
tagging -LRB-  0.04
POS tagging  0.38461538461538464
tagging or  0.08
or POST  0.0045045045045045045
POST  1.0
POST -RRB-  1.0
called grammatical  0.05555555555555555
grammatical tagging  0.18181818181818182
or word-category  0.0045045045045045045
word-category  1.0
word-category disambiguation  1.0
disambiguation ,  0.1
of marking  0.00089126559714795
marking up  0.5
-LRB- corpus  0.0027100271002710027
as corresponding  0.003484320557491289
particular part  0.07692307692307693
speech ,  0.07236842105263158
on both  0.0047169811320754715
both its  0.03225806451612903
its definition  0.02857142857142857
definition ,  0.4
its context  0.02857142857142857
context --  0.030303030303030304
-- i.e.  0.04
i.e. relationship  0.05263157894736842
relationship with  0.16666666666666666
with adjacent  0.00546448087431694
adjacent and  0.16666666666666666
related words  0.06666666666666667
phrase ,  0.1
, sentence  0.0005614823133071309
or paragraph  0.0045045045045045045
paragraph .  0.3333333333333333
A simplified  0.02
simplified  1.0
simplified form  0.5
is commonly  0.0040650406504065045
commonly taught  0.125
taught to  0.3333333333333333
to school-age  0.0013280212483399733
school-age  1.0
school-age children  1.0
children ,  0.5
words as  0.009174311926605505
as nouns  0.003484320557491289
, verbs  0.0011229646266142617
verbs  1.0
verbs ,  0.6
, adjectives  0.0005614823133071309
adjectives ,  0.3333333333333333
, adverbs  0.0005614823133071309
adverbs  1.0
adverbs ,  1.0
, POS  0.0005614823133071309
tagging is  0.08
now done  0.07692307692307693
using algorithms  0.01694915254237288
algorithms which  0.02857142857142857
which associate  0.007246376811594203
associate discrete  0.5
discrete terms  0.3333333333333333
terms ,  0.07692307692307693
as hidden  0.003484320557491289
hidden  1.0
hidden parts  0.125
in accordance  0.0018726591760299626
accordance  1.0
accordance with  1.0
of descriptive  0.00089126559714795
descriptive tags  0.3333333333333333
POS-tagging algorithms  1.0
algorithms fall  0.02857142857142857
into two  0.01282051282051282
two distinctive  0.034482758620689655
distinctive  1.0
distinctive groups  0.5
groups :  0.2
: rule-based  0.00980392156862745
rule-based and  0.14285714285714285
and stochastic  0.001445086705202312
stochastic .  0.125
E. Brill  0.25
Brill  1.0
Brill 's  0.3333333333333333
's tagger  0.0196078431372549
first and  0.030303030303030304
and widely  0.001445086705202312
used English  0.008849557522123894
English POS-taggers  0.02702702702702703
POS-taggers  1.0
POS-taggers ,  1.0
, employs  0.0005614823133071309
employs  1.0
employs rule-based  0.5
rule-based algorithms  0.14285714285714285
not rare  0.008928571428571428
rare --  0.25
-- in  0.04
as opposed  0.003484320557491289
opposed  1.0
opposed to  1.0
many artificial  0.019230769230769232
artificial languages  0.09090909090909091
languages -RRB-  0.04
large percentage  0.043478260869565216
percentage  1.0
percentage of  1.0
of word-forms  0.00089126559714795
word-forms  1.0
word-forms are  1.0
are ambiguous  0.004149377593360996
ambiguous .  0.25
, even  0.0039303761931499155
even ``  0.037037037037037035
`` dogs  0.021164021164021163
dogs  1.0
dogs ''  0.5714285714285714
usually thought  0.03125
thought of  0.6666666666666666
of as  0.0017825311942959
as just  0.003484320557491289
just a  0.2222222222222222
a plural  0.001226993865030675
plural noun  0.4
verb :  0.07692307692307693
The sailor  0.005208333333333333
sailor  1.0
sailor dogs  0.2
dogs the  0.14285714285714285
the barmaid  0.0006920415224913495
barmaid  1.0
barmaid .  0.16666666666666666
Performing grammatical  1.0
tagging will  0.04
will indicate  0.02857142857142857
indicate that  0.3333333333333333
verb ,  0.38461538461538464
the more  0.0034602076124567475
more common  0.010526315789473684
common plural  0.04
since one  0.1
words must  0.009174311926605505
main verb  0.125
the noun  0.0006920415224913495
noun reading  0.07142857142857142
reading is  0.125
is less  0.0020325203252032522
less likely  0.16666666666666666
likely following  0.0625
following ``  0.06666666666666667
`` sailor  0.010582010582010581
sailor ''  0.4
-LRB- sailor  0.005420054200542005
sailor !  0.2
 dogs  0.3333333333333333
dogs -RRB-  0.14285714285714285
Semantic analysis  0.3333333333333333
analysis can  0.03076923076923077
can then  0.0055248618784530384
then extrapolate  0.02857142857142857
extrapolate  1.0
extrapolate that  1.0
`` barmaid  0.010582010582010581
barmaid ''  0.3333333333333333
'' implicate  0.005376344086021506
implicate  1.0
implicate ``  1.0
as 1  0.003484320557491289
1 -RRB-  0.25
the nautical  0.0006920415224913495
nautical  1.0
nautical context  0.5
sailor   0.2
  0.6666666666666666
 <verb>  0.3333333333333333
<verb>  1.0
<verb>   1.0
  1.0
 barmaid  1.0
barmaid -RRB-  0.5
and 2  0.001445086705202312
2 -RRB-  0.2
-RRB- an  0.0028169014084507044
an action  0.007575757575757576
action applied  0.2
the object  0.0006920415224913495
object  1.0
object ``  0.5
-LRB- -LRB-  0.0027100271002710027
-LRB- subject  0.0027100271002710027
subject -RRB-  0.125
-RRB- dogs  0.0028169014084507044
dogs   0.14285714285714285
 barmaid  0.3333333333333333
this context  0.02197802197802198
a nautical  0.001226993865030675
nautical term  0.5
term meaning  0.05555555555555555
meaning ``  0.043478260869565216
`` fastens  0.005291005291005291
fastens  1.0
fastens -LRB-  1.0
-LRB- a  0.013550135501355014
a watertight  0.001226993865030675
watertight  1.0
watertight barmaid  1.0
-RRB- securely  0.0028169014084507044
securely  1.0
securely ;  1.0
; applies  0.02127659574468085
a dog  0.001226993865030675
dog  1.0
dog to  0.3333333333333333
to ''  0.0026560424966799467
`` Dogged  0.005291005291005291
Dogged  1.0
Dogged ''  1.0
be either  0.004219409282700422
either an  0.1
an adjective  0.03787878787878788
adjective or  0.42857142857142855
a past-tense  0.001226993865030675
past-tense  1.0
past-tense verb  1.0
verb .  0.15384615384615385
Just which  1.0
which parts  0.007246376811594203
speech a  0.006578947368421052
can represent  0.011049723756906077
represent varies  0.1111111111111111
varies  1.0
varies greatly  1.0
Trained linguists  1.0
linguists can  0.3333333333333333
can identify  0.0055248618784530384
grammatical parts  0.09090909090909091
speech to  0.019736842105263157
to various  0.0013280212483399733
various fine  0.05555555555555555
fine  1.0
fine degrees  0.5
degrees depending  0.5
the tagging  0.001384083044982699
tagging system  0.04
Schools commonly  1.0
commonly teach  0.125
teach  1.0
teach that  1.0
are 9  0.004149377593360996
9  1.0
9 parts  1.0
speech in  0.013157894736842105
English :  0.02702702702702703
: noun  0.00980392156862745
, article  0.0016844469399213925
, adjective  0.0005614823133071309
adjective ,  0.14285714285714285
, preposition  0.0005614823133071309
preposition  1.0
preposition ,  1.0
, pronoun  0.0005614823133071309
pronoun  1.0
pronoun ,  1.0
, adverb  0.0005614823133071309
adverb  1.0
adverb ,  1.0
, conjunction  0.0005614823133071309
conjunction  1.0
conjunction ,  0.3333333333333333
and interjection  0.001445086705202312
interjection  1.0
interjection .  1.0
are clearly  0.004149377593360996
clearly many  0.3333333333333333
many more  0.019230769230769232
more categories  0.010526315789473684
categories  1.0
categories and  0.2222222222222222
and sub-categories  0.001445086705202312
sub-categories  1.0
sub-categories .  1.0
For nouns  0.01639344262295082
, plural  0.0005614823133071309
, possessive  0.0005614823133071309
possessive  1.0
possessive ,  1.0
and singular  0.001445086705202312
singular  1.0
singular forms  0.25
forms can  0.16666666666666666
be distinguished  0.004219409282700422
distinguished  1.0
distinguished .  1.0
many languages  0.019230769230769232
languages words  0.02
also marked  0.014492753623188406
marked for  0.6666666666666666
for their  0.007220216606498195
their ``  0.029411764705882353
`` case  0.005291005291005291
case ''  0.058823529411764705
-LRB- role  0.0027100271002710027
role as  0.25
as subject  0.003484320557491289
, object  0.0005614823133071309
object ,  0.5
, grammatical  0.0005614823133071309
grammatical gender  0.09090909090909091
gender  1.0
gender ,  1.0
on ;  0.0047169811320754715
; while  0.02127659574468085
while verbs  0.05
verbs are  0.2
are marked  0.004149377593360996
for tense  0.0036101083032490976
tense ,  0.5
, aspect  0.0005614823133071309
aspect  1.0
aspect ,  0.5
things .  0.3333333333333333
In part-of-speech  0.009523809523809525
tagging by  0.04
computer ,  0.022727272727272728
is typical  0.0020325203252032522
typical to  0.1111111111111111
distinguish from  0.2
from 50  0.009615384615384616
50  1.0
50 to  0.3333333333333333
to 150  0.0013280212483399733
150  1.0
150 separate  0.5
separate parts  0.1
for English  0.0036101083032490976
, NN  0.0005614823133071309
NN  1.0
NN for  1.0
for singular  0.007220216606498195
singular common  0.25
common nouns  0.08
, NNS  0.0005614823133071309
NNS  1.0
NNS for  1.0
for plural  0.0036101083032490976
plural common  0.2
, NP  0.0005614823133071309
NP  1.0
NP for  1.0
singular proper  0.25
proper nouns  0.14285714285714285
nouns -LRB-  0.1111111111111111
POS tags  0.15384615384615385
tags used  0.3333333333333333
Corpus -RRB-  0.1875
Work on  0.5
on stochastic  0.0047169811320754715
stochastic methods  0.125
methods for  0.045454545454545456
for tagging  0.0036101083032490976
tagging Koine  0.04
Koine  1.0
Koine Greek  1.0
Greek  1.0
Greek -LRB-  0.3333333333333333
-LRB- DeRose  0.0027100271002710027
DeRose  0.6
DeRose 1990  0.2
1990  1.0
1990 -RRB-  0.6666666666666666
has used  0.011904761904761904
used over  0.008849557522123894
over 1,000  0.08333333333333333
1,000  1.0
1,000 parts  0.5
and found  0.001445086705202312
that about  0.0035460992907801418
about as  0.025
as many  0.006968641114982578
many words  0.038461538461538464
words were  0.01834862385321101
were ambiguous  0.024390243902439025
ambiguous there  0.08333333333333333
there as  0.025
A morphosyntactic  0.02
morphosyntactic  1.0
morphosyntactic descriptor  1.0
descriptor  1.0
descriptor in  1.0
of morphologically  0.00089126559714795
morphologically  1.0
morphologically rich  1.0
rich languages  0.2
languages can  0.04
be expressed  0.012658227848101266
expressed  1.0
expressed like  0.16666666666666666
like Ncmsan  0.03571428571428571
Ncmsan  1.0
Ncmsan ,  1.0
means Category  0.16666666666666666
Category  1.0
Category =  0.5
= Noun  0.1111111111111111
Noun  1.0
Noun ,  1.0
, Type  0.0005614823133071309
Type  1.0
Type =  1.0
= common  0.1111111111111111
common ,  0.08
, Gender  0.0005614823133071309
Gender  1.0
Gender =  1.0
= masculine  0.1111111111111111
masculine  1.0
masculine ,  1.0
, Number  0.0005614823133071309
Number  1.0
Number =  1.0
= singular  0.1111111111111111
singular ,  0.25
, Case  0.0005614823133071309
Case  1.0
Case =  1.0
= accusative  0.1111111111111111
accusative  1.0
accusative ,  1.0
, Animate  0.0005614823133071309
Animate  1.0
Animate =  1.0
= no.  0.1111111111111111
no.  1.0
no. .  1.0
History The  0.5
The Brown  0.015625
Corpus Research  0.0625
Research on  0.125
on part-of-speech  0.0047169811320754715
tagging has  0.04
been closely  0.014705882352941176
closely tied  0.2
tied  1.0
tied to  1.0
to corpus  0.0013280212483399733
linguistics .  0.05
first major  0.06060606060606061
major corpus  0.08333333333333333
English for  0.02702702702702703
computer analysis  0.022727272727272728
analysis was  0.03076923076923077
was the  0.05194805194805195
Corpus developed  0.0625
at Brown  0.029411764705882353
Brown University  0.14285714285714285
University by  0.1111111111111111
by Henry  0.005714285714285714
Henry Kucera  0.5
Kucera  1.0
Kucera and  1.0
and Nelson  0.001445086705202312
Nelson  1.0
Nelson Francis  1.0
Francis  1.0
Francis ,  1.0
the mid-1960s  0.0006920415224913495
mid-1960s  1.0
mid-1960s .  1.0
It consists  0.02631578947368421
of about  0.00089126559714795
about 1,000,000  0.025
1,000,000  1.0
1,000,000 words  1.0
of running  0.00089126559714795
running English  0.3333333333333333
English prose  0.02702702702702703
prose  1.0
prose text  1.0
, made  0.0011229646266142617
made up  0.0625
up of  0.045454545454545456
500 samples  0.5
samples  1.0
samples from  0.5
from randomly  0.009615384615384616
randomly  1.0
randomly chosen  1.0
chosen publications  0.2
publications  1.0
publications .  1.0
Each sample  0.16666666666666666
sample is  0.3333333333333333
is 2,000  0.0020325203252032522
2,000  1.0
2,000 or  0.5
more words  0.010526315789473684
-LRB- ending  0.0027100271002710027
ending  1.0
ending at  1.0
first sentence-end  0.030303030303030304
sentence-end  1.0
sentence-end after  1.0
after 2,000  0.08333333333333333
2,000 words  0.5
the corpus  0.0006920415224913495
corpus contains  0.03225806451612903
contains only  0.1
only complete  0.02631578947368421
complete  1.0
complete sentences  1.0
Corpus was  0.125
was painstakingly  0.012987012987012988
painstakingly  1.0
painstakingly ``  1.0
`` tagged  0.010582010582010581
tagged  1.0
tagged ''  0.6666666666666666
with part-of-speech  0.00546448087431694
part-of-speech markers  0.06666666666666667
markers  1.0
markers over  0.3333333333333333
over many  0.08333333333333333
many years  0.019230769230769232
A first  0.02
first approximation  0.030303030303030304
approximation was  0.16666666666666666
was done  0.012987012987012988
done with  0.18181818181818182
a program  0.0049079754601227
program by  0.045454545454545456
by Greene  0.005714285714285714
Greene  1.0
Greene and  1.0
and Rubin  0.001445086705202312
Rubin  1.0
Rubin ,  1.0
which consisted  0.007246376811594203
consisted  1.0
consisted of  1.0
a huge  0.001226993865030675
huge  1.0
huge handmade  1.0
handmade  1.0
handmade list  1.0
what categories  0.03125
categories could  0.1111111111111111
could co-occur  0.0625
co-occur at  0.5
all .  0.023255813953488372
article then  0.034482758620689655
then noun  0.02857142857142857
noun can  0.07142857142857142
can occur  0.0055248618784530384
occur  1.0
occur ,  0.2
but article  0.014705882352941176
article verb  0.034482758620689655
-LRB- arguably  0.0027100271002710027
arguably  1.0
arguably -RRB-  0.5
-RRB- can  0.008450704225352112
not .  0.017857142857142856
The program  0.005208333333333333
program got  0.045454545454545456
got  1.0
got about  1.0
about 70  0.05
70 %  0.75
% correct  0.02564102564102564
Its results  0.5
results were  0.047619047619047616
were repeatedly  0.024390243902439025
repeatedly  1.0
repeatedly reviewed  1.0
reviewed  1.0
reviewed and  1.0
and corrected  0.001445086705202312
corrected  1.0
corrected by  1.0
and later  0.001445086705202312
later users  0.1
users sent  0.1111111111111111
sent  1.0
sent in  1.0
in errata  0.0018726591760299626
errata  1.0
errata ,  1.0
that by  0.0035460992907801418
late 70s  0.1111111111111111
70s  1.0
70s the  1.0
tagging was  0.08
was nearly  0.012987012987012988
nearly  1.0
nearly perfect  0.5
perfect  1.0
perfect -LRB-  1.0
-LRB- allowing  0.0027100271002710027
allowing for  0.3333333333333333
cases on  0.05555555555555555
which even  0.007246376811594203
even human  0.037037037037037035
human speakers  0.021739130434782608
speakers might  0.25
not agree  0.008928571428571428
agree  1.0
agree -RRB-  0.3333333333333333
corpus has  0.03225806451612903
for innumerable  0.0036101083032490976
innumerable  1.0
innumerable studies  1.0
studies of  0.25
of word-frequency  0.00089126559714795
word-frequency  1.0
word-frequency and  1.0
and of  0.001445086705202312
part-of-speech ,  0.06666666666666667
and inspired  0.001445086705202312
inspired  1.0
inspired the  1.0
similar ``  0.037037037037037035
'' corpora  0.005376344086021506
corpora in  0.09090909090909091
Statistics derived  0.3333333333333333
analyzing it  0.2
it formed  0.008547008547008548
formed the  0.2
most later  0.017241379310344827
later part-of-speech  0.1
tagging systems  0.04
as CLAWS  0.003484320557491289
CLAWS  0.5
CLAWS -LRB-  0.25
and VOLSUNGA  0.001445086705202312
VOLSUNGA  1.0
VOLSUNGA .  1.0
by this  0.005714285714285714
time -LRB-  0.06060606060606061
-LRB- 2005  0.0027100271002710027
2005  1.0
2005 -RRB-  1.0
-RRB- it  0.0028169014084507044
been superseded  0.014705882352941176
superseded  1.0
superseded by  1.0
by larger  0.005714285714285714
larger corpora  0.0625
corpora such  0.09090909090909091
the 100  0.0006920415224913495
100 million  0.3333333333333333
million word  0.3333333333333333
word British  0.016666666666666666
British National  0.3333333333333333
National Corpus  0.3333333333333333
Corpus .  0.0625
For some  0.01639344262295082
some time  0.024096385542168676
was considered  0.012987012987012988
considered an  0.1111111111111111
an inseparable  0.007575757575757576
inseparable  1.0
inseparable part  1.0
are certain  0.004149377593360996
certain cases  0.14285714285714285
cases where  0.16666666666666666
speech can  0.013157894736842105
be decided  0.004219409282700422
decided without  0.3333333333333333
without understanding  0.07692307692307693
or even  0.02252252252252252
even the  0.07407407407407407
the pragmatics  0.0006920415224913495
pragmatics of  0.3333333333333333
extremely expensive  0.25
especially because  0.06666666666666667
because analyzing  0.03333333333333333
analyzing the  0.2
the higher  0.0006920415224913495
higher levels  0.2857142857142857
levels is  0.045454545454545456
much harder  0.045454545454545456
harder when  0.14285714285714285
when multiple  0.02857142857142857
multiple part-of-speech  0.07692307692307693
part-of-speech possibilities  0.06666666666666667
possibilities  1.0
possibilities must  0.2
considered for  0.1111111111111111
Use of  0.5
of Hidden  0.00089126559714795
Hidden  0.5
Hidden Markov  1.0
Markov Models  0.16666666666666666
Models  1.0
Models In  0.3333333333333333
the mid  0.0006920415224913495
mid  1.0
mid 1980s  1.0
, researchers  0.0011229646266142617
researchers in  0.1
Europe began  0.2
use hidden  0.013888888888888888
hidden Markov  0.875
models -LRB-  0.11538461538461539
-LRB- HMMs  0.005420054200542005
HMMs  0.625
HMMs -RRB-  0.25
disambiguate parts  0.3333333333333333
when working  0.02857142857142857
working to  0.14285714285714285
to tag  0.0013280212483399733
tag the  0.0625
the Lancaster-Oslo-Bergen  0.0006920415224913495
Lancaster-Oslo-Bergen  1.0
Lancaster-Oslo-Bergen Corpus  1.0
Corpus of  0.0625
of British  0.00089126559714795
British English  0.3333333333333333
HMMs involve  0.125
involve counting  0.16666666666666666
counting  1.0
counting cases  1.0
cases -LRB-  0.05555555555555555
as from  0.003484320557491289
and making  0.002890173410404624
making a  0.14285714285714285
a table  0.0036809815950920245
table of  0.42857142857142855
the probabilities  0.002768166089965398
probabilities of  0.2727272727272727
of certain  0.00089126559714795
certain sequences  0.14285714285714285
sequences .  0.2222222222222222
, once  0.0005614823133071309
once  1.0
once you  1.0
you 've  0.15384615384615385
've  1.0
've seen  0.5
seen an  0.2
an article  0.015151515151515152
article such  0.034482758620689655
as `  0.003484320557491289
` the  0.0625
the '  0.0006920415224913495
perhaps the  0.16666666666666666
next word  0.2857142857142857
noun 40  0.07142857142857142
40  1.0
40 %  1.0
adjective 40  0.14285714285714285
% ,  0.05128205128205128
number 20  0.023255813953488372
20  1.0
20 %  1.0
Knowing this  1.0
program can  0.045454545454545456
can decide  0.0055248618784530384
decide that  0.25
`` can  0.005291005291005291
can ''  0.011049723756906077
in ``  0.0056179775280898875
the can  0.0006920415224913495
more likely  0.010526315789473684
noun than  0.07142857142857142
a modal  0.001226993865030675
modal  1.0
modal .  1.0
The same  0.010416666666666666
same method  0.04
method can  0.0625
can of  0.0055248618784530384
of course  0.0017825311942959
course  1.0
course be  0.3333333333333333
to benefit  0.0013280212483399733
benefit  1.0
benefit from  1.0
from knowledge  0.009615384615384616
about following  0.025
following words  0.06666666666666667
More advanced  0.1111111111111111
advanced -LRB-  0.2
`` higher  0.005291005291005291
higher order  0.14285714285714285
order ''  0.07142857142857142
-RRB- HMMs  0.0028169014084507044
HMMs learn  0.125
learn the  0.07692307692307693
probabilities not  0.09090909090909091
only of  0.02631578947368421
of pairs  0.0017825311942959
pairs  1.0
pairs ,  1.0
but triples  0.014705882352941176
triples  1.0
triples or  0.3333333333333333
even larger  0.037037037037037035
larger sequences  0.0625
So ,  0.3333333333333333
've just  0.5
just seen  0.1111111111111111
next item  0.14285714285714285
item  1.0
item may  1.0
be very  0.012658227848101266
very likely  0.024390243902439025
likely a  0.0625
a preposition  0.001226993865030675
or noun  0.0045045045045045045
but much  0.014705882352941176
much less  0.045454545454545456
likely another  0.0625
another verb  0.07692307692307693
When several  0.14285714285714285
several ambiguous  0.045454545454545456
ambiguous words  0.16666666666666666
words occur  0.009174311926605505
occur together  0.2
together ,  0.125
the possibilities  0.0006920415224913495
possibilities multiply  0.2
multiply  1.0
multiply .  1.0
is easy  0.0020325203252032522
to enumerate  0.0013280212483399733
enumerate  1.0
enumerate every  1.0
every  1.0
every combination  0.3333333333333333
combination and  0.2
a relative  0.001226993865030675
relative probability  0.3333333333333333
probability to  0.14285714285714285
by multiplying  0.005714285714285714
multiplying  1.0
multiplying together  1.0
together the  0.125
each choice  0.022222222222222223
choice in  0.125
turn .  0.16666666666666666
The combination  0.005208333333333333
highest probability  0.3333333333333333
probability is  0.14285714285714285
then chosen  0.02857142857142857
chosen .  0.2
The European  0.005208333333333333
European group  0.3333333333333333
group developed  0.25
developed CLAWS  0.038461538461538464
CLAWS ,  0.5
a tagging  0.001226993865030675
tagging program  0.04
that did  0.0070921985815602835
did  1.0
did exactly  0.2
exactly this  0.3333333333333333
and achieved  0.001445086705202312
achieved accuracy  0.2
accuracy in  0.03225806451612903
the 93-95  0.0006920415224913495
93-95  1.0
93-95 %  1.0
% range  0.02564102564102564
range .  0.14285714285714285
worth remembering  0.5
remembering  1.0
remembering ,  1.0
as Eugene  0.003484320557491289
Eugene  1.0
Eugene Charniak  1.0
Charniak  1.0
Charniak points  1.0
points out  0.5
out in  0.14285714285714285
in Statistical  0.0018726591760299626
Statistical techniques  0.1111111111111111
language parsing  0.013513513513513514
parsing ,  0.07142857142857142
that merely  0.0035460992907801418
merely assigning  0.5
assigning  1.0
assigning the  1.0
common tag  0.04
tag to  0.0625
each known  0.022222222222222223
known word  0.038461538461538464
word and  0.016666666666666666
the tag  0.001384083044982699
tag ``  0.0625
`` proper  0.005291005291005291
proper noun  0.14285714285714285
noun ''  0.07142857142857142
all unknowns  0.023255813953488372
unknowns  1.0
unknowns ,  1.0
, will  0.0011229646266142617
will approach  0.02857142857142857
approach 90  0.02857142857142857
accuracy because  0.03225806451612903
because many  0.03333333333333333
are unambiguous  0.004149377593360996
unambiguous  1.0
unambiguous .  0.5
CLAWS pioneered  0.25
pioneered the  0.3333333333333333
of HMM-based  0.00089126559714795
HMM-based  1.0
HMM-based part  0.3333333333333333
but was  0.014705882352941176
was quite  0.012987012987012988
quite expensive  0.125
expensive since  0.14285714285714285
it enumerated  0.008547008547008548
enumerated  1.0
enumerated all  1.0
all possibilities  0.023255813953488372
possibilities .  0.2
It sometimes  0.02631578947368421
sometimes had  0.07692307692307693
had to  0.07142857142857142
to resort  0.0013280212483399733
resort  1.0
resort to  1.0
to backup  0.0013280212483399733
backup  1.0
backup methods  1.0
methods when  0.022727272727272728
when there  0.02857142857142857
there were  0.075
were simply  0.024390243902439025
simply too  0.08333333333333333
many -LRB-  0.019230769230769232
Corpus contains  0.0625
contains a  0.1
a case  0.001226993865030675
case with  0.058823529411764705
with 17  0.00546448087431694
17  1.0
17 ambiguous  1.0
a row  0.001226993865030675
row  1.0
row ,  1.0
are words  0.004149377593360996
words such  0.01834862385321101
`` still  0.005291005291005291
still ''  0.06666666666666667
represent as  0.1111111111111111
many as  0.019230769230769232
as 7  0.003484320557491289
7 distinct  0.14285714285714285
distinct parts  0.14285714285714285
speech -RRB-  0.02631578947368421
HMMs underlie  0.125
underlie  1.0
underlie the  1.0
the functioning  0.0006920415224913495
functioning  1.0
functioning of  0.3333333333333333
of stochastic  0.00089126559714795
stochastic taggers  0.125
taggers and  0.2857142857142857
in various  0.0056179775280898875
various algorithms  0.05555555555555555
algorithms one  0.02857142857142857
most widely  0.017241379310344827
used being  0.008849557522123894
being the  0.05555555555555555
the bi-directional  0.0006920415224913495
bi-directional  1.0
bi-directional inference  1.0
inference algorithm  0.25
Dynamic Programming  0.2
Programming  0.3333333333333333
Programming methods  0.3333333333333333
methods In  0.022727272727272728
In 1987  0.009523809523809525
1987 ,  0.6666666666666666
, Steven  0.0005614823133071309
Steven  1.0
Steven DeRose  1.0
DeRose and  0.2
and Ken  0.001445086705202312
Ken  1.0
Ken Church  1.0
Church  1.0
Church independently  0.3333333333333333
independently  1.0
independently developed  1.0
developed dynamic  0.038461538461538464
dynamic programming  0.2
programming  1.0
programming algorithms  0.2
solve the  0.25
same problem  0.04
problem in  0.09090909090909091
in vastly  0.0018726591760299626
vastly  1.0
vastly less  1.0
less time  0.08333333333333333
Their methods  0.5
were similar  0.024390243902439025
the Viterbi  0.002768166089965398
Viterbi  1.0
Viterbi algorithm  1.0
algorithm known  0.03571428571428571
known for  0.038461538461538464
time in  0.030303030303030304
other fields  0.014285714285714285
fields .  0.16666666666666666
DeRose used  0.2
while Church  0.05
Church used  0.3333333333333333
of triples  0.00089126559714795
triples and  0.3333333333333333
of estimating  0.00089126559714795
estimating  1.0
estimating the  1.0
the values  0.0006920415224913495
values for  0.125
for triples  0.0036101083032490976
triples that  0.3333333333333333
were rare  0.024390243902439025
rare or  0.25
or nonexistent  0.0045045045045045045
nonexistent  1.0
nonexistent in  1.0
Corpus -LRB-  0.0625
-LRB- actual  0.0027100271002710027
actual measurement  0.2
measurement of  0.5
of triple  0.00089126559714795
triple  1.0
triple probabilities  1.0
probabilities would  0.09090909090909091
much larger  0.09090909090909091
Both methods  0.3333333333333333
methods achieved  0.022727272727272728
accuracy over  0.03225806451612903
over 95  0.08333333333333333
DeRose 's  0.4
's 1990  0.0196078431372549
1990 dissertation  0.3333333333333333
dissertation at  0.3333333333333333
University included  0.1111111111111111
included analyses  0.125
analyses of  0.2
specific error  0.047619047619047616
error types  0.08333333333333333
types ,  0.07142857142857142
, probabilities  0.0005614823133071309
probabilities ,  0.09090909090909091
other related  0.014285714285714285
related data  0.06666666666666667
and replicated  0.001445086705202312
replicated  1.0
replicated his  1.0
his work  0.08333333333333333
work for  0.041666666666666664
for Greek  0.0036101083032490976
Greek ,  0.3333333333333333
where it  0.02857142857142857
it proved  0.008547008547008548
proved similarly  0.3333333333333333
similarly  1.0
similarly effective  1.0
effective .  0.3333333333333333
These findings  0.058823529411764705
findings  1.0
findings were  1.0
were surprisingly  0.024390243902439025
surprisingly disruptive  0.3333333333333333
disruptive  1.0
disruptive to  1.0
The accuracy  0.010416666666666666
accuracy reported  0.03225806451612903
reported was  0.2
was higher  0.012987012987012988
higher than  0.14285714285714285
the typical  0.0006920415224913495
typical accuracy  0.1111111111111111
of very  0.0017825311942959
very sophisticated  0.024390243902439025
sophisticated algorithms  0.2857142857142857
that integrated  0.0035460992907801418
integrated part  0.3333333333333333
speech choice  0.006578947368421052
choice with  0.125
with many  0.00546448087431694
many higher  0.019230769230769232
linguistic analysis  0.0625
: syntax  0.00980392156862745
, DeRose  0.0005614823133071309
's and  0.0196078431372549
and Church  0.001445086705202312
Church 's  0.3333333333333333
's methods  0.0196078431372549
methods did  0.022727272727272728
did fail  0.2
fail for  0.3333333333333333
known cases  0.038461538461538464
where semantics  0.02857142857142857
semantics is  0.07142857142857142
required ,  0.14285714285714285
but those  0.014705882352941176
those proved  0.045454545454545456
proved negligibly  0.3333333333333333
negligibly  1.0
negligibly rare  1.0
This convinced  0.015873015873015872
convinced  1.0
convinced many  1.0
many in  0.019230769230769232
that part-of-speech  0.0035460992907801418
tagging could  0.04
could usefully  0.0625
usefully  1.0
usefully be  1.0
be separated  0.004219409282700422
separated out  0.3333333333333333
out from  0.07142857142857142
other levels  0.014285714285714285
of processing  0.00089126559714795
processing ;  0.018518518518518517
this in  0.01098901098901099
turn simplified  0.16666666666666666
simplified the  0.5
theory and  0.07692307692307693
and practice  0.001445086705202312
practice  1.0
practice of  0.5
of computerized  0.0017825311942959
computerized  1.0
computerized language  0.5
language analysis  0.006756756756756757
and encouraged  0.001445086705202312
encouraged  1.0
encouraged researchers  1.0
researchers to  0.1
find ways  0.07692307692307693
ways to  0.125
to separate  0.0013280212483399733
separate out  0.1
out other  0.07142857142857142
other pieces  0.014285714285714285
pieces  1.0
pieces as  1.0
Models are  0.3333333333333333
now the  0.07692307692307693
the standard  0.001384083044982699
standard method  0.07142857142857142
method for  0.125
for part-of-speech  0.007220216606498195
part-of-speech assignment  0.06666666666666667
assignment  1.0
assignment .  0.5
Unsupervised taggers  0.16666666666666666
taggers The  0.14285714285714285
methods already  0.022727272727272728
already discussed  0.2
discussed involve  0.14285714285714285
involve working  0.16666666666666666
working from  0.14285714285714285
a pre-existing  0.001226993865030675
pre-existing corpus  0.5
corpus to  0.03225806451612903
learn tag  0.07692307692307693
tag probabilities  0.0625
to bootstrap  0.0013280212483399733
bootstrap  1.0
bootstrap using  1.0
using ``  0.01694915254237288
`` unsupervised  0.005291005291005291
unsupervised ''  0.125
'' tagging  0.005376344086021506
tagging .  0.08
Unsupervised tagging  0.16666666666666666
tagging techniques  0.04
techniques use  0.043478260869565216
use an  0.013888888888888888
an untagged  0.007575757575757576
untagged  1.0
untagged corpus  1.0
corpus for  0.03225806451612903
their training  0.029411764705882353
the tagset  0.0006920415224913495
tagset  1.0
tagset by  1.0
by induction  0.005714285714285714
That is  1.0
they observe  0.025
observe  1.0
observe patterns  1.0
patterns in  0.2
in word  0.0018726591760299626
word use  0.016666666666666666
and derive  0.001445086705202312
derive part-of-speech  0.5
part-of-speech categories  0.06666666666666667
categories themselves  0.1111111111111111
themselves .  0.25
, statistics  0.0011229646266142617
statistics readily  0.125
readily reveal  0.3333333333333333
reveal  1.0
reveal that  1.0
the ''  0.0006920415224913495
`` a  0.005291005291005291
a ''  0.001226993865030675
`` an  0.005291005291005291
an ''  0.007575757575757576
'' occur  0.005376344086021506
occur in  0.4
in similar  0.0018726591760299626
similar contexts  0.037037037037037035
contexts ,  0.2857142857142857
while ``  0.05
`` eat  0.005291005291005291
eat  1.0
eat ''  1.0
'' occurs  0.005376344086021506
occurs  1.0
occurs in  1.0
different ones  0.02040816326530612
ones .  0.2
With sufficient  0.14285714285714285
sufficient iteration  0.2
iteration  1.0
iteration ,  1.0
, similarity  0.0005614823133071309
similarity classes  0.1
words emerge  0.009174311926605505
emerge  1.0
emerge that  1.0
are remarkably  0.004149377593360996
remarkably  1.0
remarkably similar  1.0
to those  0.0026560424966799467
those human  0.045454545454545456
human linguists  0.021739130434782608
linguists would  0.3333333333333333
would expect  0.018867924528301886
expect  1.0
expect ;  0.3333333333333333
the differences  0.0006920415224913495
differences themselves  0.3333333333333333
themselves sometimes  0.25
sometimes suggest  0.07692307692307693
suggest valuable  0.3333333333333333
valuable  1.0
valuable new  0.5
new insights  0.041666666666666664
insights  1.0
insights .  1.0
These two  0.058823529411764705
two categories  0.034482758620689655
categories can  0.1111111111111111
be further  0.004219409282700422
further subdivided  0.125
subdivided  1.0
subdivided into  1.0
into rule-based  0.01282051282051282
rule-based ,  0.14285714285714285
, stochastic  0.0005614823133071309
and neural  0.002890173410404624
neural approaches  0.06666666666666667
Other taggers  0.14285714285714285
and methods  0.001445086705202312
methods Some  0.022727272727272728
current major  0.14285714285714285
major algorithms  0.08333333333333333
tagging include  0.04
, Brill  0.0005614823133071309
Brill Tagger  0.3333333333333333
Tagger  1.0
Tagger ,  1.0
, Constraint  0.0005614823133071309
Constraint  1.0
Constraint Grammar  1.0
Grammar  1.0
Grammar ,  1.0
the Baum-Welch  0.0006920415224913495
Baum-Welch  1.0
Baum-Welch algorithm  1.0
algorithm -LRB-  0.03571428571428571
the forward-backward  0.0006920415224913495
forward-backward  1.0
forward-backward algorithm  1.0
algorithm -RRB-  0.03571428571428571
Markov model  0.4444444444444444
model and  0.03333333333333333
and visible  0.001445086705202312
visible Markov  0.3333333333333333
model taggers  0.03333333333333333
taggers can  0.14285714285714285
can both  0.0055248618784530384
both be  0.03225806451612903
be implemented  0.008438818565400843
implemented using  0.2
The Brill  0.005208333333333333
Brill tagger  0.3333333333333333
tagger is  0.1111111111111111
is unusual  0.0020325203252032522
unusual  1.0
unusual in  1.0
in that  0.003745318352059925
it learns  0.008547008547008548
learns  1.0
learns a  1.0
of patterns  0.00089126559714795
patterns ,  0.2
then applies  0.02857142857142857
applies those  0.14285714285714285
those patterns  0.045454545454545456
patterns rather  0.2
than optimizing  0.022222222222222223
optimizing  1.0
optimizing a  1.0
statistical quantity  0.030303030303030304
quantity  1.0
quantity .  0.3333333333333333
Many machine  0.08333333333333333
learning methods  0.023255813953488372
have also  0.009615384615384616
of POS  0.0017825311942959
Methods such  0.25
as SVM  0.003484320557491289
SVM  1.0
SVM ,  1.0
, Maximum  0.0005614823133071309
Maximum  0.3333333333333333
entropy classifier  0.2
classifier ,  0.14285714285714285
, Perceptron  0.0005614823133071309
Perceptron  1.0
Perceptron ,  1.0
and Nearest-neighbor  0.001445086705202312
Nearest-neighbor  1.0
Nearest-neighbor have  1.0
have all  0.009615384615384616
all been  0.023255813953488372
been tried  0.029411764705882353
tried ,  0.3333333333333333
and most  0.001445086705202312
most can  0.017241379310344827
can achieve  0.0055248618784530384
achieve  1.0
achieve accuracy  0.5
accuracy above  0.03225806451612903
above 95  0.07692307692307693
A direct  0.02
direct comparison  0.16666666666666666
comparison of  0.3333333333333333
several methods  0.045454545454545456
methods is  0.022727272727272728
is reported  0.0020325203252032522
reported -LRB-  0.2
-LRB- with  0.008130081300813009
with references  0.00546448087431694
-RRB- at  0.0028169014084507044
at .  0.014705882352941176
This comparison  0.015873015873015872
comparison uses  0.3333333333333333
uses the  0.07142857142857142
Penn tag  0.2222222222222222
tag set  0.3125
set on  0.02564102564102564
Treebank data  0.16666666666666666
are directly  0.004149377593360996
directly comparable  0.2
comparable  1.0
comparable .  1.0
many significant  0.019230769230769232
significant taggers  0.1111111111111111
taggers are  0.14285714285714285
not included  0.008928571428571428
included -LRB-  0.125
-LRB- perhaps  0.0027100271002710027
perhaps because  0.16666666666666666
the labor  0.0006920415224913495
labor involved  0.5
involved in  0.16666666666666666
in reconfiguring  0.0018726591760299626
reconfiguring  1.0
reconfiguring them  1.0
them for  0.05263157894736842
this particular  0.01098901098901099
particular dataset  0.07692307692307693
dataset  1.0
dataset -RRB-  1.0
it should  0.008547008547008548
should not  0.05263157894736842
be assumed  0.004219409282700422
assumed  1.0
assumed that  1.0
results reported  0.047619047619047616
reported there  0.2
best that  0.1111111111111111
achieved with  0.2
given approach  0.08333333333333333
approach ;  0.02857142857142857
; nor  0.02127659574468085
nor  1.0
nor even  1.0
been achieved  0.029411764705882353
approach .  0.05714285714285714
Issues While  0.5
While there  0.2
is broad  0.0020325203252032522
broad agreement  0.25
agreement about  0.3333333333333333
about basic  0.025
basic categories  0.07692307692307693
categories ,  0.1111111111111111
of edge  0.00089126559714795
edge cases  0.3333333333333333
cases make  0.05555555555555555
make it  0.1
to settle  0.0013280212483399733
settle  1.0
settle on  1.0
single ``  0.07142857142857142
`` correct  0.005291005291005291
correct ''  0.06666666666666667
'' set  0.005376344086021506
of tags  0.00089126559714795
tags ,  0.3333333333333333
even in  0.07407407407407407
single language  0.07142857142857142
language such  0.006756756756756757
is hard  0.0020325203252032522
to say  0.00398406374501992
say whether  0.14285714285714285
whether ``  0.07692307692307693
`` fire  0.005291005291005291
fire  1.0
fire ''  0.5
is functioning  0.0020325203252032522
functioning as  0.6666666666666666
noun in  0.07142857142857142
the big  0.0006920415224913495
big  1.0
big green  0.5
green  1.0
green fire  1.0
fire truck  0.5
truck  1.0
truck A  1.0
A second  0.02
second important  0.1
important example  0.0625
example is  0.012345679012345678
the use\/mention  0.0006920415224913495
use\/mention  1.0
use\/mention distinction  1.0
distinction ,  0.2
following example  0.13333333333333333
where ``  0.02857142857142857
`` blue  0.010582010582010581
blue  1.0
blue ''  1.0
clearly not  0.3333333333333333
not functioning  0.008928571428571428
adjective -LRB-  0.14285714285714285
Corpus tag  0.125
set appends  0.02564102564102564
appends  1.0
appends the  1.0
the suffix  0.0006920415224913495
suffix  1.0
suffix ''  1.0
- NC  0.0625
NC  1.0
NC ''  1.0
such cases  0.016260162601626018
cases -RRB-  0.05555555555555555
: the  0.0196078431372549
word ``  0.016666666666666666
'' has  0.010752688172043012
has 4  0.011904761904761904
4 letters  0.2
Words in  0.25
language other  0.006756756756756757
other than  0.014285714285714285
`` main  0.005291005291005291
main ''  0.125
'' text  0.005376344086021506
, are  0.0011229646266142617
are commonly  0.004149377593360996
commonly tagged  0.125
tagged as  0.3333333333333333
`` foreign  0.005291005291005291
foreign  1.0
foreign ''  0.5
usually in  0.09375
in addition  0.0056179775280898875
a tag  0.001226993865030675
tag for  0.0625
the role  0.001384083044982699
role the  0.25
the foreign  0.0006920415224913495
foreign word  0.5
is actually  0.0040650406504065045
actually playing  0.3333333333333333
playing  1.0
playing in  1.0
also many  0.014492753623188406
where POS  0.02857142857142857
POS categories  0.07692307692307693
`` words  0.005291005291005291
words ''  0.01834862385321101
'' do  0.005376344086021506
not map  0.008928571428571428
map one  0.5
to one  0.0026560424966799467
: David  0.00980392156862745
David 's  0.25
's gonna  0.0196078431372549
gonna  1.0
gonna do  1.0
do n't  0.07692307692307693
n't  1.0
n't vice  0.25
vice  1.0
vice versa  1.0
versa  1.0
versa first-cut  1.0
first-cut  1.0
first-cut can  1.0
not pre  0.008928571428571428
pre  1.0
pre -  1.0
and post-secondary  0.001445086705202312
post-secondary  1.0
post-secondary look  1.0
look -LRB-  0.2
word -RRB-  0.016666666666666666
-RRB- up  0.0028169014084507044
up In  0.045454545454545456
the last  0.0020761245674740486
last example  0.2
`` look  0.005291005291005291
look ''  0.2
`` up  0.005291005291005291
up ''  0.045454545454545456
'' arguably  0.005376344086021506
arguably function  0.5
function as  0.125
single verbal  0.07142857142857142
verbal  1.0
verbal unit  1.0
unit ,  0.3333333333333333
, despite  0.0005614823133071309
despite the  0.3333333333333333
words coming  0.009174311926605505
coming  1.0
coming between  1.0
between them  0.05128205128205128
them .  0.10526315789473684
Some tag  0.047619047619047616
tag sets  0.25
sets -LRB-  0.09090909090909091
as Penn  0.003484320557491289
Penn -RRB-  0.1111111111111111
-RRB- break  0.0028169014084507044
break hyphenated  0.5
hyphenated  1.0
hyphenated words  1.0
, contractions  0.0005614823133071309
contractions  1.0
contractions ,  0.5
and possessives  0.001445086705202312
possessives  1.0
possessives into  1.0
separate tokens  0.1
tokens ,  0.14285714285714285
, thus  0.0011229646266142617
thus avoiding  0.1
avoiding  1.0
avoiding some  0.5
some but  0.012048192771084338
but far  0.014705882352941176
far from  0.125
from all  0.009615384615384616
all such  0.023255813953488372
such problems  0.008130081300813009
is unclear  0.0020325203252032522
unclear  1.0
unclear whether  1.0
whether it  0.07692307692307693
is best  0.0020325203252032522
to treat  0.0013280212483399733
treat  1.0
treat words  0.5
`` be  0.010582010582010581
be ''  0.008438818565400843
`` have  0.005291005291005291
have ''  0.009615384615384616
`` do  0.005291005291005291
do ''  0.038461538461538464
as categories  0.003484320557491289
categories in  0.1111111111111111
their own  0.029411764705882353
own right  0.16666666666666666
right -LRB-  0.1
or as  0.009009009009009009
as simply  0.003484320557491289
simply verbs  0.08333333333333333
verbs -LRB-  0.2
the LOB  0.0006920415224913495
LOB  1.0
LOB Corpus  1.0
Corpus and  0.125
Treebank -RRB-  0.16666666666666666
has more  0.011904761904761904
more forms  0.010526315789473684
forms than  0.16666666666666666
than other  0.022222222222222223
other English  0.014285714285714285
English verbs  0.02702702702702703
and occurs  0.001445086705202312
in quite  0.0018726591760299626
different grammatical  0.02040816326530612
grammatical contexts  0.09090909090909091
, complicating  0.0005614823133071309
complicating  1.0
complicating the  1.0
issue .  0.25
popular ``  0.1111111111111111
'' for  0.005376344086021506
for POS  0.0036101083032490976
tagging for  0.04
for American  0.0036101083032490976
American English  0.2
English is  0.02702702702702703
is probably  0.0020325203252032522
probably the  0.25
set ,  0.05128205128205128
, developed  0.0005614823133071309
Treebank project  0.16666666666666666
project .  0.07692307692307693
is largely  0.0020325203252032522
largely similar  0.2
the earlier  0.0006920415224913495
earlier  1.0
earlier Brown  0.25
and LOB  0.001445086705202312
sets ,  0.09090909090909091
though much  0.1
much smaller  0.045454545454545456
smaller .  0.14285714285714285
, tag  0.0005614823133071309
sets from  0.09090909090909091
the Eagles  0.0006920415224913495
Eagles  1.0
Eagles Guidelines  1.0
Guidelines  1.0
Guidelines see  0.5
see wide  0.05
wide use  0.25
include versions  0.037037037037037035
versions for  0.3333333333333333
for multiple  0.0036101083032490976
multiple languages  0.07692307692307693
tagging work  0.04
of languages  0.00089126559714795
the set  0.001384083044982699
used varies  0.008849557522123894
greatly with  0.14285714285714285
with language  0.00546448087431694
Tags usually  1.0
usually are  0.03125
are designed  0.004149377593360996
include overt  0.037037037037037035
overt  1.0
overt morphological  1.0
morphological distinctions  0.3333333333333333
distinctions -LRB-  0.5
-LRB- this  0.0027100271002710027
this makes  0.01098901098901099
sets for  0.09090909090909091
for heavily  0.0036101083032490976
heavily  1.0
heavily inflected  1.0
inflected  1.0
inflected languages  1.0
as Greek  0.003484320557491289
Greek and  0.3333333333333333
and Latin  0.001445086705202312
Latin  1.0
Latin very  0.25
very large  0.024390243902439025
large ;  0.043478260869565216
and makes  0.001445086705202312
makes tagging  0.125
tagging words  0.04
in agglutinative  0.0018726591760299626
agglutinative  1.0
agglutinative languages  1.0
an Inuit  0.007575757575757576
Inuit  1.0
Inuit virtually  1.0
virtually impossible  0.5
impossible .  0.5
, Petrov  0.0005614823133071309
Petrov  1.0
Petrov ,  1.0
, D.  0.0005614823133071309
D. Das  0.2
Das  1.0
Das ,  1.0
and R.  0.001445086705202312
R. McDonald  0.16666666666666666
McDonald  1.0
McDonald -LRB-  1.0
`` A  0.005291005291005291
A Universal  0.02
Universal  1.0
Universal Part-of-Speech  1.0
Part-of-Speech  1.0
Part-of-Speech Tagset  1.0
Tagset  1.0
Tagset ''  1.0
'' http:\/\/arxiv.org\/abs\/1104.2086  0.005376344086021506
http:\/\/arxiv.org\/abs\/1104.2086  1.0
http:\/\/arxiv.org\/abs\/1104.2086 -RRB-  1.0
have proposed  0.009615384615384616
universal ''  0.3333333333333333
'' tag  0.005376344086021506
with 12  0.00546448087431694
12 categories  0.2
categories -LRB-  0.1111111111111111
, no  0.0016844469399213925
no subtypes  0.07692307692307693
subtypes  1.0
subtypes of  1.0
of nouns  0.00089126559714795
, punctuation  0.0005614823133071309
punctuation ,  0.2857142857142857
etc. ;  0.045454545454545456
; no  0.02127659574468085
no distinction  0.07692307692307693
distinction of  0.2
an infinitive  0.007575757575757576
infinitive  1.0
infinitive marker  1.0
marker  1.0
marker vs.  1.0
vs. preposition  0.08333333333333333
Whether a  0.5
small set  0.1111111111111111
broad tags  0.25
larger set  0.0625
more precise  0.010526315789473684
precise ones  0.3333333333333333
is preferable  0.0020325203252032522
preferable  1.0
preferable ,  1.0
, depends  0.0005614823133071309
purpose at  0.2
at hand  0.014705882352941176
hand .  0.07142857142857142
Automatic tagging  0.1111111111111111
easier on  0.125
on smaller  0.0047169811320754715
smaller tag-sets  0.14285714285714285
tag-sets  1.0
tag-sets .  1.0
A different  0.04
different issue  0.02040816326530612
issue is  0.125
cases are  0.05555555555555555
in fact  0.0018726591760299626
fact ambiguous  0.09090909090909091
Beatrice Santorini  1.0
Santorini  1.0
Santorini gives  1.0
gives  1.0
gives examples  0.5
examples in  0.041666666666666664
`` Part-of-speech  0.005291005291005291
Part-of-speech  0.5
Part-of-speech Tagging  0.5
Tagging  1.0
Tagging Guidelines  1.0
Guidelines for  0.5
Treebank Project  0.16666666666666666
Project  1.0
Project ,  1.0
, ''  0.0016844469399213925
-LRB- 3rd  0.0027100271002710027
3rd  1.0
3rd rev  1.0
rev  1.0
rev ,  1.0
, June  0.0005614823133071309
June  1.0
June 1990  1.0
including the  0.07142857142857142
following -LRB-  0.06666666666666667
-LRB- p.  0.0027100271002710027
p.  1.0
p. 32  1.0
32  1.0
32 -RRB-  1.0
-RRB- case  0.0028169014084507044
case in  0.058823529411764705
which entertaining  0.007246376811594203
entertaining  1.0
entertaining can  0.5
can function  0.0055248618784530384
function either  0.125
is no  0.0020325203252032522
no evident  0.07692307692307693
evident  1.0
evident way  0.5
decide :  0.25
The Duchess  0.005208333333333333
Duchess  1.0
Duchess was  1.0
was entertaining  0.012987012987012988
entertaining last  0.5
last night  0.2
night  1.0
night .  1.0
In computer  0.009523809523809525
science and  0.1
, parsing  0.0016844469399213925
or ,  0.0045045045045045045
more formally  0.010526315789473684
formally  1.0
formally ,  0.5
syntactic analysis  0.15384615384615385
of analyzing  0.00089126559714795
analyzing a  0.2
a sequence  0.007361963190184049
sequence  1.0
sequence of  0.875
tokens -LRB-  0.14285714285714285
, words  0.0011229646266142617
its grammatical  0.02857142857142857
grammatical structure  0.09090909090909091
structure with  0.08333333333333333
given -LRB-  0.041666666666666664
less -RRB-  0.08333333333333333
-RRB- formal  0.0028169014084507044
formal grammar  0.2222222222222222
Parsing can  0.2
linguistic term  0.0625
term ,  0.05555555555555555
instance when  0.07142857142857142
when discussing  0.05714285714285714
discussing  1.0
discussing how  0.5
how phrases  0.034482758620689655
phrases are  0.0625
are divided  0.004149377593360996
divided up  0.3333333333333333
in garden  0.0018726591760299626
garden  1.0
garden path  1.0
path  1.0
path sentences  0.5
Parsing is  0.4
also an  0.014492753623188406
an earlier  0.007575757575757576
earlier term  0.25
the diagramming  0.001384083044982699
diagramming  1.0
diagramming of  1.0
still used  0.06666666666666667
of inflected  0.00089126559714795
the Romance  0.0006920415224913495
Romance  1.0
Romance languages  1.0
languages or  0.02
or Latin  0.0045045045045045045
Latin .  0.25
The term  0.020833333333333332
term parsing  0.05555555555555555
parsing comes  0.03571428571428571
from Latin  0.009615384615384616
Latin pars  0.25
pars  1.0
pars -LRB-  1.0
-LRB- rtinis  0.0027100271002710027
rtinis  1.0
rtinis -RRB-  1.0
, meaning  0.0005614823133071309
meaning part  0.043478260869565216
part -LRB-  0.037037037037037035
-LRB- of  0.005420054200542005
a common  0.00245398773006135
common term  0.04
term used  0.1111111111111111
in psycholinguistics  0.0018726591760299626
psycholinguistics when  0.5
describing language  0.25
language comprehension  0.006756756756756757
parsing refers  0.03571428571428571
refers  1.0
refers to  1.0
the way  0.002768166089965398
human beings  0.021739130434782608
beings  1.0
beings ,  1.0
, rather  0.0011229646266142617
than computers  0.022222222222222223
computers ,  0.2222222222222222
, analyze  0.0005614823133071309
analyze a  0.25
or phrase  0.0045045045045045045
phrase -LRB-  0.1
-LRB- in  0.005420054200542005
in spoken  0.003745318352059925
spoken language  0.14285714285714285
text -RRB-  0.006289308176100629
-RRB- ``  0.0028169014084507044
of grammatical  0.00089126559714795
grammatical constituents  0.09090909090909091
constituents  1.0
constituents ,  0.5
, identifying  0.0016844469399213925
the parts  0.0006920415224913495
syntactic relations  0.07692307692307693
etc. ''  0.045454545454545456
'' This  0.005376344086021506
This term  0.015873015873015872
term is  0.05555555555555555
common when  0.04
discussing what  0.5
what linguistic  0.03125
linguistic cues  0.0625
cues  1.0
cues help  1.0
help speakers  0.1111111111111111
speakers to  0.25
parse garden-path  0.1111111111111111
garden-path  1.0
garden-path sentences  1.0
The parser  0.005208333333333333
parser often  0.0625
often uses  0.022727272727272728
separate lexical  0.1
lexical analyser  0.07692307692307693
analyser  1.0
analyser to  1.0
create tokens  0.058823529411764705
tokens from  0.14285714285714285
the sequence  0.0006920415224913495
input characters  0.024390243902439025
Parsers may  0.5
programmed by  0.5
hand or  0.14285714285714285
or may  0.009009009009009009
be -LRB-  0.004219409282700422
-LRB- semi  0.0027100271002710027
semi  1.0
semi -  1.0
- -RRB-  0.0625
-RRB- automatically  0.0028169014084507044
generated -LRB-  0.06666666666666667
some programming  0.012048192771084338
programming languages  0.6
-RRB- by  0.0028169014084507044
tool .  0.5
Human languages  0.2
languages See  0.02
See  0.8333333333333334
: Category  0.00980392156862745
Category :  0.5
: Natural  0.00980392156862745
parsing In  0.03571428571428571
some machine  0.012048192771084338
and natural  0.005780346820809248
human languages  0.021739130434782608
languages are  0.02
are parsed  0.004149377593360996
parsed  1.0
parsed by  0.75
Human sentences  0.2
easily parsed  0.1111111111111111
by programs  0.005714285714285714
programs ,  0.18181818181818182
as there  0.003484320557491289
is substantial  0.0020325203252032522
substantial ambiguity  0.2
ambiguity in  0.125
whose usage  0.3333333333333333
usage  1.0
usage is  1.0
convey meaning  0.3333333333333333
meaning -LRB-  0.043478260869565216
or semantics  0.0045045045045045045
semantics -RRB-  0.07142857142857142
-RRB- amongst  0.0028169014084507044
amongst  1.0
amongst a  1.0
a potentially  0.001226993865030675
potentially unlimited  0.3333333333333333
unlimited  1.0
unlimited range  1.0
of possibilities  0.00089126559714795
possibilities but  0.2
but only  0.014705882352941176
are germane  0.004149377593360996
germane  1.0
germane to  1.0
particular case  0.07692307692307693
So an  0.3333333333333333
an utterance  0.015151515151515152
utterance  1.0
utterance ``  0.3333333333333333
`` Man  0.010582010582010581
Man  1.0
Man bites  0.5
bites  1.0
bites dog  0.3333333333333333
dog ''  0.3333333333333333
'' versus  0.005376344086021506
versus  1.0
versus ``  1.0
`` Dog  0.005291005291005291
Dog  1.0
Dog bites  1.0
bites man  0.3333333333333333
man  1.0
man ''  1.0
is definite  0.0020325203252032522
definite  1.0
definite on  1.0
one detail  0.015384615384615385
detail  1.0
detail but  0.5
but in  0.029411764705882353
language might  0.006756756756756757
might appear  0.038461538461538464
appear as  0.0625
Man dog  0.5
dog bites  0.3333333333333333
bites ''  0.3333333333333333
a reliance  0.001226993865030675
reliance  1.0
reliance on  1.0
the larger  0.0006920415224913495
larger context  0.0625
context to  0.030303030303030304
between those  0.02564102564102564
those two  0.045454545454545456
two possibilities  0.034482758620689655
possibilities ,  0.2
if indeed  0.03571428571428571
indeed  1.0
indeed that  0.3333333333333333
that difference  0.0035460992907801418
difference was  0.25
was of  0.012987012987012988
of concern  0.00089126559714795
concern  1.0
concern .  1.0
is difficult  0.008130081300813009
to prepare  0.0013280212483399733
prepare  1.0
prepare formal  1.0
formal rules  0.1111111111111111
describe informal  0.16666666666666666
informal behavior  0.5
behavior  1.0
behavior even  0.5
even though  0.07407407407407407
though it  0.2
is clear  0.0020325203252032522
clear that  0.25
some rules  0.012048192771084338
rules are  0.023255813953488372
are being  0.008298755186721992
being followed  0.05555555555555555
followed .  0.25
In order  0.01904761904761905
parse natural  0.1111111111111111
language data  0.006756756756756757
researchers must  0.1
must first  0.07142857142857142
first agree  0.030303030303030304
agree on  0.3333333333333333
grammar to  0.02702702702702703
The choice  0.005208333333333333
choice of  0.25
syntax is  0.09090909090909091
is affected  0.0020325203252032522
affected  1.0
affected by  1.0
by both  0.005714285714285714
both linguistic  0.03225806451612903
linguistic and  0.0625
and computational  0.001445086705202312
computational concerns  0.1
concerns ;  0.5
; for  0.0425531914893617
instance some  0.07142857142857142
some parsing  0.012048192771084338
parsing systems  0.03571428571428571
systems use  0.05357142857142857
use lexical  0.027777777777777776
lexical functional  0.07692307692307693
functional grammar  0.5
parsing for  0.03571428571428571
for grammars  0.0036101083032490976
grammars of  0.07142857142857142
type is  0.14285714285714285
be NP-complete  0.004219409282700422
NP-complete  1.0
NP-complete .  1.0
Head-driven phrase  1.0
structure grammar  0.08333333333333333
grammar is  0.05405405405405406
another linguistic  0.07692307692307693
linguistic formalism  0.0625
formalism  1.0
formalism which  1.0
been popular  0.014705882352941176
popular in  0.1111111111111111
parsing community  0.03571428571428571
community  1.0
community ,  1.0
but other  0.014705882352941176
other research  0.014285714285714285
research efforts  0.023809523809523808
efforts have  0.5714285714285714
have focused  0.019230769230769232
on less  0.0047169811320754715
less complex  0.08333333333333333
complex formalisms  0.041666666666666664
formalisms  1.0
formalisms such  0.5
the one  0.0006920415224913495
one used  0.015384615384615385
Shallow parsing  0.5
parsing aims  0.03571428571428571
aims to  0.6666666666666666
find only  0.07692307692307693
the boundaries  0.0020761245674740486
boundaries of  0.09090909090909091
of major  0.00089126559714795
major constituents  0.08333333333333333
constituents such  0.5
as noun  0.003484320557491289
noun phrases  0.07142857142857142
Another popular  0.07692307692307693
popular strategy  0.1111111111111111
strategy for  0.2
for avoiding  0.0036101083032490976
avoiding linguistic  0.5
linguistic controversy  0.0625
controversy  1.0
controversy is  1.0
is dependency  0.0020325203252032522
dependency grammar  0.2
grammar parsing  0.02702702702702703
parsing .  0.10714285714285714
Most modern  0.5
modern parsers  0.2
parsers are  0.15384615384615385
least partly  0.2
partly  1.0
partly statistical  1.0
statistical ;  0.030303030303030304
; that  0.02127659574468085
they rely  0.025
data which  0.012987012987012988
has already  0.011904761904761904
been annotated  0.014705882352941176
annotated -LRB-  0.5
-LRB- parsed  0.0027100271002710027
hand -RRB-  0.07142857142857142
approach allows  0.02857142857142857
to gather  0.0013280212483399733
gather  1.0
gather information  1.0
the frequency  0.0006920415224913495
frequency  1.0
frequency with  0.5
with which  0.00546448087431694
which various  0.007246376811594203
various constructions  0.05555555555555555
constructions  1.0
constructions occur  1.0
in specific  0.003745318352059925
specific contexts  0.047619047619047616
-LRB- See  0.01084010840108401
See machine  0.16666666666666666
Approaches which  0.3333333333333333
used include  0.008849557522123894
include straightforward  0.037037037037037035
straightforward  1.0
straightforward PCFGs  1.0
PCFGs  1.0
PCFGs -LRB-  1.0
-LRB- probabilistic  0.0027100271002710027
probabilistic context-free  0.14285714285714285
context-free  1.0
context-free grammars  0.36363636363636365
, maximum  0.0005614823133071309
entropy ,  0.2
neural nets  0.06666666666666667
nets  1.0
nets .  1.0
Most of  0.5
successful systems  0.1111111111111111
lexical statistics  0.07692307692307693
statistics -LRB-  0.125
-LRB- that  0.01084010840108401
they consider  0.025
the identities  0.0006920415224913495
identities  1.0
identities of  1.0
words involved  0.009174311926605505
involved ,  0.16666666666666666
as their  0.006968641114982578
their part  0.029411764705882353
However such  0.02702702702702703
such systems  0.008130081300813009
are vulnerable  0.004149377593360996
vulnerable  1.0
vulnerable to  1.0
to overfitting  0.0013280212483399733
overfitting and  0.5
require some  0.045454545454545456
of smoothing  0.00089126559714795
smoothing  1.0
smoothing to  1.0
-RRB- Parsing  0.0028169014084507044
Parsing  0.2
Parsing algorithms  0.2
language can  0.006756756756756757
grammar having  0.02702702702702703
having `  0.2
` nice  0.0625
nice '  0.25
' properties  0.05263157894736842
properties as  0.25
as with  0.006968641114982578
with manually  0.00546448087431694
manually designed  0.25
designed grammars  0.14285714285714285
grammars for  0.07142857142857142
for programming  0.0036101083032490976
mentioned earlier  0.3333333333333333
earlier some  0.25
some grammar  0.012048192771084338
grammar formalisms  0.02702702702702703
formalisms are  0.5
parse computationally  0.1111111111111111
computationally  1.0
computationally ;  0.5
; in  0.02127659574468085
even if  0.1111111111111111
desired structure  0.2
structure is  0.08333333333333333
not context-free  0.008928571428571428
context-free ,  0.09090909090909091
of context-free  0.00089126559714795
context-free approximation  0.09090909090909091
first pass  0.030303030303030304
pass  1.0
pass .  1.0
Algorithms which  0.5
which use  0.007246376811594203
use context-free  0.013888888888888888
grammars often  0.07142857142857142
often rely  0.022727272727272728
some variant  0.012048192771084338
variant  1.0
variant of  1.0
the CKY  0.0006920415224913495
CKY  1.0
CKY algorithm  1.0
usually with  0.0625
some heuristic  0.012048192771084338
heuristic to  0.3333333333333333
to prune  0.0013280212483399733
prune  1.0
prune away  1.0
away  1.0
away unlikely  0.5
unlikely  1.0
unlikely analyses  1.0
analyses to  0.2
to save  0.0013280212483399733
save  1.0
save time  1.0
See chart  0.16666666666666666
chart  1.0
chart parsing  1.0
However some  0.02702702702702703
systems trade  0.008928571428571428
trade speed  0.5
speed for  0.14285714285714285
for accuracy  0.0036101083032490976
accuracy using  0.03225806451612903
using ,  0.01694915254237288
, linear-time  0.0005614823133071309
linear-time  1.0
linear-time versions  1.0
versions of  0.3333333333333333
the shift-reduce  0.0006920415224913495
shift-reduce  1.0
shift-reduce algorithm  1.0
A somewhat  0.02
somewhat recent  0.5
recent development  0.125
development has  0.08333333333333333
been parse  0.014705882352941176
parse reranking  0.1111111111111111
reranking  1.0
reranking in  1.0
parser proposes  0.0625
proposes  1.0
proposes some  1.0
some large  0.012048192771084338
of analyses  0.00089126559714795
analyses ,  0.2
system selects  0.010752688172043012
selects the  0.5
best option  0.05555555555555555
option  1.0
option .  1.0
Programming languages  0.6666666666666666
languages The  0.02
parser is  0.1875
is as  0.0020325203252032522
compiler or  0.3333333333333333
or interpreter  0.009009009009009009
interpreter  1.0
interpreter .  0.5
This parses  0.015873015873015872
parses the  0.5
source code  0.041666666666666664
code of  0.14285714285714285
computer programming  0.022727272727272728
programming language  0.2
create some  0.058823529411764705
of internal  0.00089126559714795
languages tend  0.02
tend  1.0
tend to  1.0
be specified  0.004219409282700422
specified  1.0
specified in  1.0
a context-free  0.0036809815950920245
context-free grammar  0.45454545454545453
grammar because  0.02702702702702703
because fast  0.03333333333333333
fast  1.0
fast and  1.0
and efficient  0.002890173410404624
efficient parsers  0.3333333333333333
parsers can  0.07692307692307693
be written  0.004219409282700422
written for  0.038461538461538464
Parsers are  0.5
are written  0.004149377593360996
or generated  0.0045045045045045045
generated by  0.06666666666666667
by parser  0.005714285714285714
parser generators  0.0625
generators .  0.5
Context-free grammars  1.0
grammars are  0.07142857142857142
are limited  0.004149377593360996
limited in  0.1
extent to  0.25
which they  0.014492753623188406
express all  0.2
the requirements  0.0006920415224913495
requirements of  0.5
Informally ,  1.0
the reason  0.0006920415224913495
reason  1.0
reason is  0.25
the memory  0.0006920415224913495
memory of  0.5
is limited  0.0020325203252032522
limited .  0.2
grammar can  0.02702702702702703
not remember  0.008928571428571428
remember  1.0
remember the  1.0
the presence  0.0006920415224913495
presence  1.0
presence of  1.0
a construct  0.001226993865030675
construct over  0.3333333333333333
over an  0.08333333333333333
an arbitrarily  0.007575757575757576
arbitrarily  1.0
arbitrarily long  1.0
long input  0.5
input ;  0.024390243902439025
language in  0.006756756756756757
which ,  0.007246376811594203
a name  0.00245398773006135
name must  0.2
be declared  0.004219409282700422
declared before  0.5
before it  0.16666666666666666
it may  0.017094017094017096
be referenced  0.004219409282700422
referenced  1.0
referenced .  1.0
More powerful  0.1111111111111111
powerful  1.0
powerful grammars  1.0
grammars that  0.07142857142857142
express this  0.2
this constraint  0.01098901098901099
constraint  1.0
constraint ,  1.0
be parsed  0.004219409282700422
parsed efficiently  0.25
efficiently  1.0
efficiently .  1.0
common strategy  0.04
a relaxed  0.001226993865030675
relaxed  1.0
relaxed parser  1.0
parser for  0.0625
grammar which  0.05405405405405406
which accepts  0.007246376811594203
accepts  1.0
accepts a  0.5
a superset  0.001226993865030675
superset  1.0
superset of  1.0
desired language  0.2
language constructs  0.006756756756756757
constructs  1.0
constructs -LRB-  0.3333333333333333
it accepts  0.008547008547008548
accepts some  0.5
some invalid  0.012048192771084338
invalid  1.0
invalid constructs  1.0
constructs -RRB-  0.3333333333333333
; later  0.02127659574468085
the unwanted  0.0006920415224913495
unwanted  1.0
unwanted constructs  1.0
constructs can  0.3333333333333333
filtered out  0.3333333333333333
out .  0.07142857142857142
of process  0.00089126559714795
process Flow  0.027777777777777776
Flow  1.0
Flow of  1.0
typical parser  0.1111111111111111
parser The  0.125
example demonstrates  0.012345679012345678
demonstrates  1.0
demonstrates the  1.0
the common  0.0006920415224913495
common case  0.04
of parsing  0.0017825311942959
parsing a  0.03571428571428571
computer language  0.022727272727272728
language with  0.006756756756756757
with two  0.01092896174863388
two levels  0.034482758620689655
of grammar  0.0017825311942959
grammar :  0.02702702702702703
: lexical  0.00980392156862745
lexical and  0.15384615384615385
and syntactic  0.002890173410404624
syntactic .  0.07692307692307693
first stage  0.030303030303030304
stage is  0.4
the token  0.0006920415224913495
token  1.0
token generation  0.25
generation ,  0.1111111111111111
lexical analysis  0.07692307692307693
by which  0.005714285714285714
input character  0.024390243902439025
character stream  0.045454545454545456
stream is  0.5
is split  0.0040650406504065045
split  1.0
split into  0.5
into meaningful  0.02564102564102564
meaningful symbols  0.125
symbols defined  0.3333333333333333
defined by  0.16666666666666666
a grammar  0.00245398773006135
of regular  0.00089126559714795
regular  1.0
regular expressions  1.0
expressions .  0.6666666666666666
a calculator  0.00245398773006135
calculator  1.0
calculator program  0.5
program would  0.045454545454545456
at an  0.014705882352941176
input such  0.024390243902439025
`` 12  0.010582010582010581
12 \*  0.4
\*  1.0
\* -LRB-  0.25
-LRB- 3  0.005420054200542005
3  1.0
3 +4  0.2
+4  1.0
+4 -RRB-  1.0
-RRB- ^  0.0028169014084507044
^  1.0
^ 2  0.3333333333333333
2 ''  0.2
and split  0.001445086705202312
split it  0.25
the tokens  0.001384083044982699
tokens 12  0.14285714285714285
12 ,  0.2
, \*  0.0005614823133071309
\* ,  0.5
-LRB- ,  0.0027100271002710027
, 3  0.0005614823133071309
3 ,  0.2
, +  0.0011229646266142617
+ ,  0.3333333333333333
, 4  0.0005614823133071309
4 ,  0.2
, -RRB-  0.0005614823133071309
, ^  0.0011229646266142617
^ ,  0.6666666666666666
, 2  0.0005614823133071309
2 ,  0.2
meaningful symbol  0.125
symbol in  0.25
an arithmetic  0.007575757575757576
arithmetic  1.0
arithmetic expression  1.0
expression .  0.2
The lexer  0.005208333333333333
lexer  1.0
lexer would  1.0
would contain  0.018867924528301886
contain rules  0.08333333333333333
to tell  0.0013280212483399733
tell it  0.3333333333333333
it that  0.008547008547008548
the characters  0.0006920415224913495
characters \*  0.0625
and -RRB-  0.001445086705202312
-RRB- mark  0.0028169014084507044
mark the  0.3333333333333333
new token  0.041666666666666664
token ,  0.25
so meaningless  0.03333333333333333
meaningless  1.0
meaningless tokens  1.0
tokens like  0.14285714285714285
like ``  0.10714285714285714
\* ''  0.25
or ''  0.0045045045045045045
3 ''  0.2
'' will  0.005376344086021506
be generated  0.004219409282700422
The next  0.005208333333333333
next stage  0.2857142857142857
is parsing  0.0020325203252032522
parsing or  0.07142857142857142
or syntactic  0.0045045045045045045
is checking  0.0020325203252032522
checking  1.0
checking that  1.0
tokens form  0.14285714285714285
form an  0.05
an allowable  0.007575757575757576
allowable expression  0.5
usually done  0.0625
with reference  0.00546448087431694
which recursively  0.007246376811594203
recursively defines  0.5
defines components  0.5
components that  0.2
up an  0.045454545454545456
an expression  0.007575757575757576
order in  0.07142857142857142
they must  0.025
must appear  0.07142857142857142
appear .  0.0625
not all  0.017857142857142856
all rules  0.023255813953488372
rules defining  0.023255813953488372
defining  1.0
defining programming  1.0
expressed by  0.16666666666666666
by context-free  0.005714285714285714
grammars alone  0.07142857142857142
alone ,  0.25
example type  0.012345679012345678
type validity  0.07142857142857142
validity  1.0
validity and  1.0
and proper  0.001445086705202312
proper declaration  0.14285714285714285
declaration  1.0
declaration of  1.0
of identifiers  0.00089126559714795
identifiers  1.0
identifiers .  1.0
These rules  0.058823529411764705
be formally  0.004219409282700422
formally expressed  0.5
expressed with  0.16666666666666666
with attribute  0.00546448087431694
attribute  1.0
attribute grammars  0.5
The final  0.005208333333333333
final phase  0.1111111111111111
phase  1.0
phase is  1.0
is semantic  0.0020325203252032522
semantic parsing  0.047619047619047616
or analysis  0.0045045045045045045
working out  0.14285714285714285
out the  0.21428571428571427
the implications  0.0006920415224913495
implications  1.0
implications of  1.0
the expression  0.001384083044982699
expression just  0.1
just validated  0.1111111111111111
validated  1.0
validated and  1.0
and taking  0.001445086705202312
taking the  0.6
appropriate action  0.25
action .  0.2
calculator or  0.5
interpreter ,  0.5
the action  0.0006920415224913495
action is  0.2
expression or  0.1
or program  0.0045045045045045045
program ,  0.045454545454545456
compiler ,  0.3333333333333333
, would  0.0016844469399213925
would generate  0.018867924528301886
generate some  0.05555555555555555
of code  0.00089126559714795
Attribute grammars  1.0
grammars can  0.07142857142857142
define these  0.5
these actions  0.023809523809523808
actions  1.0
actions .  1.0
of parser  0.0017825311942959
essentially to  0.125
if and  0.03571428571428571
how the  0.034482758620689655
input can  0.024390243902439025
start symbol  0.2857142857142857
symbol of  0.25
This can  0.015873015873015872
in essentially  0.0018726591760299626
essentially two  0.125
two ways  0.034482758620689655
: Top-down  0.00980392156862745
Top-down  1.0
Top-down parsing  1.0
parsing -  0.07142857142857142
- Top-down  0.0625
parsing can  0.07142857142857142
find left-most  0.07692307692307693
left-most  1.0
left-most derivations  0.5
derivations  1.0
derivations of  1.0
an input-stream  0.007575757575757576
input-stream  1.0
input-stream by  1.0
by searching  0.005714285714285714
searching for  0.3333333333333333
for parse  0.0036101083032490976
parse trees  0.2222222222222222
trees using  0.16666666666666666
a top-down  0.001226993865030675
top-down  1.0
top-down expansion  0.25
expansion of  0.3333333333333333
the given  0.001384083044982699
given formal  0.041666666666666664
Tokens are  1.0
are consumed  0.004149377593360996
consumed  1.0
consumed from  1.0
from left  0.009615384615384616
left to  0.16666666666666666
to right  0.0013280212483399733
right .  0.3
Inclusive choice  1.0
to accommodate  0.0026560424966799467
accommodate  1.0
accommodate ambiguity  0.4
ambiguity by  0.125
by expanding  0.005714285714285714
expanding  1.0
expanding all  1.0
all alternative  0.023255813953488372
alternative right-hand-sides  0.3333333333333333
right-hand-sides  1.0
right-hand-sides of  1.0
Bottom-up parsing  1.0
- A  0.0625
A parser  0.02
parser can  0.0625
can start  0.0055248618784530384
start with  0.14285714285714285
and attempt  0.001445086705202312
to rewrite  0.0013280212483399733
rewrite  1.0
rewrite it  1.0
Intuitively ,  1.0
parser attempts  0.0625
attempts to  0.5
to locate  0.0013280212483399733
locate  1.0
locate the  1.0
most basic  0.017241379310344827
basic elements  0.07692307692307693
elements  1.0
elements ,  0.25
then the  0.05714285714285714
the elements  0.0006920415224913495
elements containing  0.25
containing these  0.125
LR parsers  1.0
of bottom-up  0.00089126559714795
bottom-up  1.0
bottom-up parsers  1.0
parsers .  0.07692307692307693
Another term  0.07692307692307693
is Shift-Reduce  0.0020325203252032522
Shift-Reduce  1.0
Shift-Reduce parsing  1.0
LL parsers  1.0
parsers and  0.07692307692307693
and recursive-descent  0.001445086705202312
recursive-descent  1.0
recursive-descent parser  1.0
parser are  0.0625
of top-down  0.0017825311942959
top-down parsers  0.25
parsers which  0.07692307692307693
not accommodate  0.017857142857142856
accommodate left  0.2
left recursive  0.16666666666666666
recursive  1.0
recursive productions  1.0
productions  1.0
productions .  1.0
Although it  0.125
been believed  0.014705882352941176
believed  1.0
believed that  1.0
that simple  0.0035460992907801418
simple implementations  0.038461538461538464
top-down parsing  0.5
accommodate direct  0.2
direct and  0.16666666666666666
and indirect  0.001445086705202312
indirect  1.0
indirect left-recursion  1.0
left-recursion  1.0
left-recursion and  1.0
and may  0.002890173410404624
may require  0.038461538461538464
require exponential  0.045454545454545456
exponential  1.0
exponential time  0.5
and space  0.001445086705202312
space  1.0
space complexity  0.2
complexity while  0.08333333333333333
while parsing  0.05
parsing ambiguous  0.03571428571428571
ambiguous context-free  0.08333333333333333
more sophisticated  0.021052631578947368
for top-down  0.0036101083032490976
parsing have  0.03571428571428571
by Frost  0.005714285714285714
Frost  1.0
Frost ,  1.0
, Hafiz  0.0005614823133071309
Hafiz  1.0
Hafiz ,  1.0
and Callaghan  0.001445086705202312
Callaghan  1.0
Callaghan which  1.0
which accommodate  0.007246376811594203
ambiguity and  0.125
and left  0.001445086705202312
left recursion  0.16666666666666666
recursion  1.0
recursion in  1.0
in polynomial  0.0018726591760299626
polynomial  1.0
polynomial time  1.0
and which  0.001445086705202312
generate polynomial-size  0.05555555555555555
polynomial-size  1.0
polynomial-size representations  1.0
representations of  0.25
the potentially  0.0006920415224913495
potentially exponential  0.3333333333333333
exponential number  0.5
of parse  0.00089126559714795
Their algorithm  0.5
is able  0.0020325203252032522
produce both  0.045454545454545456
both left-most  0.03225806451612903
left-most and  0.5
and right-most  0.001445086705202312
right-most  1.0
right-most derivations  1.0
input with  0.024390243902439025
given CFG  0.041666666666666664
CFG  1.0
CFG -LRB-  1.0
-LRB- context-free  0.0027100271002710027
distinction with  0.2
to parsers  0.0013280212483399733
parsers is  0.07692307692307693
is whether  0.0020325203252032522
parser generates  0.0625
a leftmost  0.00245398773006135
leftmost  1.0
leftmost derivation  1.0
derivation  1.0
derivation or  0.25
a rightmost  0.00245398773006135
rightmost  1.0
rightmost derivation  1.0
derivation -LRB-  0.5
see context-free  0.05
parsers will  0.15384615384615385
derivation and  0.25
and LR  0.001445086705202312
LR  0.5
although usually  0.16666666666666666
in reverse  0.0018726591760299626
reverse -RRB-  0.5
In information  0.009523809523809525
retrieval and  0.2857142857142857
, question  0.0011229646266142617
question answering  0.21428571428571427
answering -LRB-  0.08333333333333333
-LRB- QA  0.0027100271002710027
QA  0.9523809523809523
QA -RRB-  0.047619047619047616
automatically answering  0.047619047619047616
answering a  0.08333333333333333
a question  0.008588957055214725
question posed  0.047619047619047616
posed  1.0
posed in  0.6666666666666666
To find  0.1111111111111111
the answer  0.009688581314878892
answer to  0.16666666666666666
a QA  0.0049079754601227
QA computer  0.047619047619047616
program may  0.045454545454545456
may use  0.019230769230769232
use either  0.013888888888888888
either a  0.2
a pre-structured  0.001226993865030675
pre-structured  1.0
pre-structured database  1.0
database or  0.2
language documents  0.006756756756756757
corpus such  0.03225806451612903
Web or  0.1111111111111111
or some  0.013513513513513514
some local  0.012048192771084338
local collection  0.3333333333333333
collection -RRB-  0.2
QA research  0.047619047619047616
research attempts  0.047619047619047616
to deal  0.0013280212483399733
a wide  0.00245398773006135
wide range  0.5
of question  0.00267379679144385
question types  0.023809523809523808
types including  0.07142857142857142
including :  0.14285714285714285
: fact  0.00980392156862745
, list  0.0005614823133071309
list ,  0.09090909090909091
, definition  0.0005614823133071309
, How  0.0005614823133071309
How  0.42857142857142855
How ,  0.14285714285714285
, Why  0.0005614823133071309
Why ,  0.14285714285714285
, hypothetical  0.0005614823133071309
hypothetical  1.0
hypothetical ,  1.0
, semantically  0.0005614823133071309
semantically  1.0
semantically constrained  1.0
constrained  1.0
constrained ,  1.0
and cross-lingual  0.001445086705202312
cross-lingual  1.0
cross-lingual questions  0.5
questions .  0.07692307692307693
Search collections  0.5
collections  1.0
collections vary  0.25
vary from  0.16666666666666666
small local  0.1111111111111111
local document  0.3333333333333333
document collections  0.027777777777777776
collections ,  0.5
to internal  0.0013280212483399733
internal organization  0.2
organization documents  0.2
to compiled  0.0013280212483399733
compiled  1.0
compiled newswire  1.0
newswire  1.0
newswire reports  1.0
reports ,  0.2
Web .  0.2222222222222222
Closed-domain question  1.0
answering deals  0.16666666666666666
with questions  0.01092896174863388
questions under  0.038461538461538464
under a  0.2
domain -LRB-  0.05
, medicine  0.0005614823133071309
medicine  1.0
medicine or  1.0
or automotive  0.0045045045045045045
automotive  1.0
automotive maintenance  1.0
maintenance  1.0
maintenance -RRB-  1.0
an easier  0.007575757575757576
easier task  0.125
task because  0.023809523809523808
because NLP  0.03333333333333333
can exploit  0.0055248618784530384
exploit  1.0
exploit domain-specific  1.0
domain-specific knowledge  0.5
knowledge frequently  0.037037037037037035
frequently  1.0
frequently formalized  0.5
formalized  1.0
formalized in  1.0
in ontologies  0.0018726591760299626
ontologies .  0.5
Alternatively ,  1.0
, closed-domain  0.0005614823133071309
closed-domain  1.0
closed-domain might  1.0
might refer  0.038461538461538464
a situation  0.001226993865030675
situation  1.0
situation where  0.5
where only  0.02857142857142857
only a  0.05263157894736842
limited type  0.1
of questions  0.004456327985739751
are accepted  0.004149377593360996
accepted  1.0
accepted ,  1.0
as questions  0.003484320557491289
questions asking  0.038461538461538464
asking  1.0
asking for  0.5
for descriptive  0.0036101083032490976
descriptive rather  0.3333333333333333
than procedural  0.022222222222222223
procedural  1.0
procedural information  1.0
Open-domain question  1.0
questions about  0.15384615384615385
about nearly  0.025
nearly anything  0.5
anything  1.0
anything ,  1.0
only rely  0.02631578947368421
on general  0.0047169811320754715
general ontologies  0.045454545454545456
ontologies and  0.16666666666666666
and world  0.001445086705202312
world knowledge  0.13333333333333333
knowledge .  0.037037037037037035
, these  0.0011229646266142617
usually have  0.03125
have much  0.009615384615384616
available from  0.058823529411764705
which to  0.007246376811594203
extract the  0.25
, current  0.0005614823133071309
current QA  0.14285714285714285
QA systems  0.2857142857142857
use text  0.013888888888888888
text documents  0.006289308176100629
documents as  0.02631578947368421
their underlying  0.029411764705882353
underlying knowledge  0.3333333333333333
knowledge source  0.037037037037037035
source and  0.041666666666666664
and combine  0.001445086705202312
various natural  0.05555555555555555
processing techniques  0.037037037037037035
to search  0.0013280212483399733
search for  0.09090909090909091
the answers  0.0006920415224913495
answers .  0.08333333333333333
Current QA  0.2
systems typically  0.008928571428571428
typically include  0.05555555555555555
question classifier  0.023809523809523808
classifier module  0.14285714285714285
module  1.0
module that  0.3333333333333333
question and  0.023809523809523808
of answer  0.00089126559714795
After the  0.3333333333333333
the question  0.011072664359861591
question is  0.09523809523809523
analyzed ,  0.2
system typically  0.010752688172043012
typically uses  0.05555555555555555
uses several  0.07142857142857142
several modules  0.045454545454545456
modules that  0.5
that apply  0.0035460992907801418
apply increasingly  0.2
increasingly complex  0.3333333333333333
complex NLP  0.08333333333333333
NLP techniques  0.0425531914893617
techniques on  0.043478260869565216
a gradually  0.001226993865030675
gradually  1.0
gradually reduced  1.0
reduced amount  0.25
document retrieval  0.027777777777777776
retrieval module  0.14285714285714285
module uses  0.3333333333333333
uses search  0.07142857142857142
engines to  0.3333333333333333
documents or  0.02631578947368421
paragraphs in  0.25
document set  0.027777777777777776
set that  0.02564102564102564
to contain  0.0013280212483399733
Subsequently a  1.0
a filter  0.001226993865030675
filter  1.0
filter preselects  0.5
preselects  1.0
preselects small  1.0
small text  0.1111111111111111
text fragments  0.006289308176100629
fragments  1.0
fragments that  1.0
that contain  0.010638297872340425
contain strings  0.08333333333333333
same type  0.04
type as  0.07142857142857142
the expected  0.001384083044982699
expected answer  0.14285714285714285
is ``  0.0040650406504065045
`` Who  0.010582010582010581
Who  1.0
Who invented  0.5
invented Penicillin  0.5
Penicillin  1.0
Penicillin ''  1.0
'' the  0.005376344086021506
the filter  0.0006920415224913495
filter returns  0.5
returns  1.0
returns text  1.0
contain names  0.08333333333333333
names of  0.14285714285714285
of people  0.00089126559714795
Finally ,  1.0
an answer  0.015151515151515152
answer extraction  0.06666666666666667
extraction module  0.03225806451612903
module looks  0.3333333333333333
looks for  0.25
further clues  0.125
clues  1.0
clues in  0.3333333333333333
answer candidate  0.03333333333333333
candidate can  0.3333333333333333
can indeed  0.0055248618784530384
indeed answer  0.3333333333333333
answer the  0.03333333333333333
question .  0.047619047619047616
answering methods  0.08333333333333333
methods QA  0.022727272727272728
QA is  0.047619047619047616
very dependent  0.024390243902439025
dependent  1.0
dependent on  0.6666666666666666
good search  0.07692307692307693
search corpus  0.09090909090909091
corpus -  0.03225806451612903
- for  0.0625
for without  0.0036101083032490976
without documents  0.07692307692307693
documents containing  0.02631578947368421
answer ,  0.03333333333333333
is little  0.0020325203252032522
little any  0.3333333333333333
any QA  0.03225806451612903
QA system  0.23809523809523808
system can  0.010752688172043012
do .  0.038461538461538464
It thus  0.02631578947368421
thus makes  0.1
makes sense  0.125
sense that  0.125
that larger  0.0035460992907801418
larger collection  0.0625
collection sizes  0.2
sizes generally  0.3333333333333333
generally lend  0.09090909090909091
lend  1.0
lend well  1.0
well to  0.03571428571428571
better QA  0.1111111111111111
QA performance  0.047619047619047616
performance ,  0.1111111111111111
, unless  0.0005614823133071309
unless  1.0
unless the  1.0
question domain  0.023809523809523808
domain is  0.05
is orthogonal  0.0020325203252032522
orthogonal  1.0
orthogonal to  1.0
the collection  0.0006920415224913495
collection .  0.2
The notion  0.010416666666666666
data redundancy  0.012987012987012988
in massive  0.0018726591760299626
massive  1.0
massive collections  1.0
the web  0.001384083044982699
web ,  0.125
, means  0.0005614823133071309
that nuggets  0.0035460992907801418
nuggets  1.0
nuggets of  1.0
information are  0.021739130434782608
be phrased  0.004219409282700422
phrased  1.0
phrased in  1.0
different ways  0.02040816326530612
ways in  0.125
in differing  0.0018726591760299626
differing  1.0
differing contexts  1.0
contexts and  0.14285714285714285
and documents  0.001445086705202312
, leading  0.0005614823133071309
to two  0.0013280212483399733
two benefits  0.034482758620689655
benefits  1.0
benefits :  0.5
: By  0.00980392156862745
By  0.6666666666666666
By having  0.3333333333333333
having the  0.2
right information  0.1
information appear  0.021739130434782608
many forms  0.019230769230769232
forms ,  0.16666666666666666
the burden  0.0006920415224913495
burden  1.0
burden on  1.0
the QA  0.0006920415224913495
perform complex  0.09090909090909091
is lessened  0.0020325203252032522
lessened  1.0
lessened .  1.0
Correct answers  1.0
answers can  0.08333333333333333
filtered from  0.3333333333333333
from false  0.009615384615384616
false  1.0
false positives  0.5
positives  1.0
positives by  1.0
by relying  0.005714285714285714
relying  1.0
relying on  1.0
correct answer  0.06666666666666667
appear more  0.0625
more times  0.010526315789473684
times in  0.2
documents than  0.02631578947368421
than instances  0.022222222222222223
of incorrect  0.00089126559714795
incorrect ones  0.3333333333333333
Issues In  0.5
In 2002  0.009523809523809525
2002 a  0.5
a group  0.001226993865030675
of researchers  0.00089126559714795
researchers wrote  0.1
wrote a  0.16666666666666666
a roadmap  0.001226993865030675
roadmap  1.0
roadmap of  1.0
answering .  0.16666666666666666
following issues  0.06666666666666667
issues were  0.2
were identified  0.024390243902439025
identified .  0.2
Question classes  0.2857142857142857
classes Different  0.2
questions -LRB-  0.038461538461538464
of Lichtenstein  0.00089126559714795
Lichtenstein  1.0
Lichtenstein ?  1.0
vs. ``  0.16666666666666666
a rainbow  0.001226993865030675
rainbow  1.0
rainbow form  1.0
form ?  0.05
`` Did  0.005291005291005291
Did  1.0
Did Marilyn  1.0
Marilyn  1.0
Marilyn Monroe  1.0
Monroe  1.0
Monroe and  1.0
and Cary  0.001445086705202312
Cary  1.0
Cary Grant  1.0
Grant  1.0
Grant ever  1.0
ever  1.0
ever appear  1.0
a movie  0.00245398773006135
movie  1.0
movie together  0.3333333333333333
together ?  0.125
of different  0.0017825311942959
different strategies  0.02040816326530612
strategies to  0.5
classes are  0.2
are arranged  0.004149377593360996
arranged  1.0
arranged hierarchically  1.0
hierarchically  1.0
hierarchically in  1.0
in taxonomies  0.0018726591760299626
taxonomies  1.0
taxonomies .  1.0
Question processing  0.14285714285714285
processing The  0.018518518518518517
same information  0.04
information request  0.021739130434782608
request  1.0
request can  1.0
expressed in  0.16666666666666666
various ways  0.1111111111111111
some interrogative  0.012048192771084338
interrogative  1.0
interrogative -LRB-  1.0
Who is  0.5
the president  0.001384083044982699
president  1.0
president of  1.0
States ?  0.14285714285714285
and some  0.002890173410404624
some assertive  0.012048192771084338
assertive  1.0
assertive -LRB-  1.0
`` Tell  0.005291005291005291
Tell  1.0
Tell me  1.0
me  1.0
me the  1.0
name of  0.2
A semantic  0.02
semantic model  0.047619047619047616
model of  0.03333333333333333
question understanding  0.023809523809523808
understanding and  0.030303030303030304
and processing  0.001445086705202312
processing would  0.018518518518518517
would recognize  0.018867924528301886
recognize equivalent  0.1111111111111111
equivalent questions  0.2
questions ,  0.3076923076923077
of how  0.00089126559714795
are presented  0.004149377593360996
presented .  0.16666666666666666
would enable  0.018867924528301886
complex question  0.041666666666666664
question into  0.023809523809523808
of simpler  0.0017825311942959
simpler  1.0
simpler questions  0.3333333333333333
would identify  0.018867924528301886
identify ambiguities  0.08333333333333333
ambiguities and  0.25
and treat  0.001445086705202312
treat them  0.5
them in  0.05263157894736842
context or  0.030303030303030304
or by  0.0045045045045045045
by interactive  0.005714285714285714
interactive clarification  0.25
clarification  1.0
clarification .  0.3333333333333333
Context and  1.0
and QA  0.001445086705202312
QA Questions  0.047619047619047616
Questions  1.0
Questions are  1.0
usually asked  0.03125
asked  1.0
asked within  0.3333333333333333
a context  0.00245398773006135
context and  0.12121212121212122
and answers  0.001445086705202312
answers are  0.08333333333333333
are provided  0.004149377593360996
provided within  0.2
within that  0.05555555555555555
that specific  0.0035460992907801418
specific context  0.047619047619047616
The context  0.005208333333333333
context can  0.030303030303030304
to clarify  0.0013280212483399733
clarify  1.0
clarify a  1.0
, resolve  0.0005614823133071309
ambiguities or  0.25
or keep  0.0045045045045045045
keep track  0.3333333333333333
track  1.0
track of  1.0
an investigation  0.007575757575757576
investigation  1.0
investigation performed  1.0
performed through  0.1
through a  0.25
-LRB- For  0.005420054200542005
For  0.06557377049180328
Why did  0.14285714285714285
did Joe  0.2
Joe  1.0
Joe Biden  1.0
Biden  1.0
Biden visit  0.3333333333333333
visit  1.0
visit Iraq  0.5
Iraq  1.0
Iraq in  0.5
in January  0.003745318352059925
January 2010  0.5
2010 ?  0.3333333333333333
be asking  0.004219409282700422
asking why  0.5
why Vice  0.14285714285714285
Vice  1.0
Vice President  1.0
President Biden  0.25
Biden visited  0.3333333333333333
visited  1.0
visited and  1.0
not President  0.008928571428571428
President Obama  0.25
Obama  1.0
Obama ,  1.0
, why  0.0011229646266142617
why he  0.2857142857142857
he went  0.2857142857142857
went to  0.4
to Iraq  0.0013280212483399733
Iraq and  0.5
not Afghanistan  0.008928571428571428
Afghanistan  1.0
Afghanistan or  1.0
other country  0.014285714285714285
country ,  0.25
went in  0.2
2010 and  0.3333333333333333
not before  0.008928571428571428
before or  0.16666666666666666
or after  0.0045045045045045045
after ,  0.08333333333333333
or what  0.009009009009009009
what Biden  0.03125
Biden was  0.3333333333333333
was hoping  0.012987012987012988
hoping  1.0
hoping to  1.0
to accomplish  0.0013280212483399733
accomplish  1.0
accomplish with  1.0
with his  0.00546448087431694
his visit  0.08333333333333333
visit .  0.5
If the  0.4
related questions  0.06666666666666667
previous questions  0.3333333333333333
questions and  0.038461538461538464
their answers  0.029411764705882353
answers might  0.08333333333333333
might shed  0.038461538461538464
shed  1.0
shed light  1.0
light on  0.3333333333333333
the questioner  0.002768166089965398
questioner  1.0
questioner 's  0.25
's intent  0.0196078431372549
intent  1.0
intent .  1.0
Data sources  1.0
sources for  0.16666666666666666
for QA  0.010830324909747292
QA Before  0.047619047619047616
Before  0.5
Before a  0.5
question can  0.023809523809523808
be answered  0.004219409282700422
answered  1.0
answered ,  0.2
it must  0.008547008547008548
be known  0.004219409282700422
known what  0.038461538461538464
what knowledge  0.03125
knowledge sources  0.037037037037037035
sources are  0.16666666666666666
available and  0.058823529411764705
and relevant  0.001445086705202312
relevant .  0.14285714285714285
not present  0.026785714285714284
data sources  0.025974025974025976
sources ,  0.16666666666666666
no matter  0.07692307692307693
matter how  0.3333333333333333
well the  0.03571428571428571
question processing  0.07142857142857142
and answer  0.001445086705202312
extraction is  0.06451612903225806
correct result  0.06666666666666667
result will  0.09090909090909091
be obtained  0.004219409282700422
obtained .  0.14285714285714285
Answer extraction  0.6666666666666666
extraction Answer  0.03225806451612903
Answer  0.3333333333333333
extraction depends  0.03225806451612903
answer type  0.06666666666666667
type provided  0.07142857142857142
provided by  0.4
by question  0.005714285714285714
actual data  0.2
data where  0.012987012987012988
answer is  0.06666666666666667
is searched  0.0020325203252032522
the search  0.0006920415224913495
search method  0.09090909090909091
method and  0.0625
and on  0.002890173410404624
question focus  0.023809523809523808
focus and  0.14285714285714285
Answer formulation  0.3333333333333333
formulation  1.0
formulation The  1.0
The result  0.005208333333333333
system should  0.010752688172043012
be presented  0.004219409282700422
way as  0.041666666666666664
as natural  0.003484320557491289
natural as  0.013333333333333334
, simple  0.0011229646266142617
simple extraction  0.038461538461538464
is sufficient  0.0040650406504065045
question classification  0.023809523809523808
classification indicates  0.058823529411764705
indicates  1.0
indicates that  1.0
name -LRB-  0.2
organization ,  0.2
, shop  0.0005614823133071309
shop  1.0
shop or  1.0
or disease  0.0045045045045045045
disease  1.0
disease ,  1.0
a quantity  0.001226993865030675
quantity -LRB-  0.3333333333333333
-LRB- monetary  0.0027100271002710027
monetary  1.0
monetary value  1.0
value  1.0
value ,  0.3333333333333333
, length  0.0005614823133071309
, size  0.0005614823133071309
size ,  0.16666666666666666
, distance  0.0005614823133071309
distance  1.0
distance ,  0.6666666666666666
a date  0.001226993865030675
date  1.0
date -LRB-  0.6666666666666666
`` On  0.005291005291005291
On  0.16666666666666666
On what  0.16666666666666666
what day  0.03125
day  1.0
day did  1.0
did Christmas  0.2
Christmas  1.0
Christmas fall  1.0
fall in  0.25
in 1989  0.0018726591760299626
1989 ?  0.5
single datum  0.07142857142857142
datum  1.0
datum is  1.0
For other  0.01639344262295082
other cases  0.02857142857142857
the presentation  0.0006920415224913495
presentation  1.0
presentation of  1.0
answer may  0.03333333333333333
of fusion  0.00089126559714795
fusion  1.0
fusion techniques  1.0
techniques that  0.08695652173913043
that combine  0.0035460992907801418
combine the  0.3333333333333333
the partial  0.0006920415224913495
partial  1.0
partial answers  1.0
answers from  0.16666666666666666
from multiple  0.009615384615384616
Real time  0.5
time question  0.030303030303030304
answering There  0.08333333333333333
is need  0.0020325203252032522
developing Q&A  0.25
Q&A  1.0
Q&A systems  1.0
of extracting  0.00089126559714795
extracting answers  0.2
from large  0.009615384615384616
large data  0.043478260869565216
sets in  0.09090909090909091
several seconds  0.045454545454545456
seconds  1.0
seconds ,  1.0
size and  0.3333333333333333
and multitude  0.001445086705202312
multitude  1.0
multitude of  1.0
sources or  0.16666666666666666
the ambiguity  0.0006920415224913495
ambiguity of  0.125
Multilingual -LRB-  1.0
or cross-lingual  0.0045045045045045045
cross-lingual -RRB-  0.5
-RRB- question  0.0028169014084507044
answering The  0.08333333333333333
The ability  0.005208333333333333
to answer  0.00398406374501992
answer a  0.03333333333333333
language using  0.006756756756756757
answer corpus  0.03333333333333333
even several  0.037037037037037035
several -RRB-  0.045454545454545456
This allows  0.031746031746031744
to consult  0.0013280212483399733
consult  1.0
consult information  1.0
information that  0.021739130434782608
not use  0.017857142857142856
use directly  0.013888888888888888
directly .  0.2
also Machine  0.014492753623188406
Interactive QA  0.5
QA It  0.047619047619047616
often the  0.045454545454545456
case that  0.058823529411764705
information need  0.021739130434782608
need is  0.047619047619047616
not well  0.008928571428571428
well captured  0.03571428571428571
captured  1.0
captured by  1.0
processing part  0.018518518518518517
part may  0.037037037037037035
may fail  0.019230769230769232
fail to  0.3333333333333333
to classify  0.0013280212483399733
classify  1.0
classify properly  0.5
properly the  0.5
question or  0.023809523809523808
information needed  0.021739130434782608
for extracting  0.0036101083032490976
extracting and  0.2
and generating  0.001445086705202312
generating the  0.2
easily retrieved  0.1111111111111111
retrieved  1.0
retrieved .  1.0
In such  0.009523809523809525
questioner might  0.25
might want  0.038461538461538464
want not  0.16666666666666666
only to  0.02631578947368421
to reformulate  0.0013280212483399733
reformulate  1.0
reformulate the  1.0
but to  0.014705882352941176
dialogue with  0.5
system might  0.010752688172043012
might ask  0.038461538461538464
ask for  0.25
a clarification  0.001226993865030675
clarification of  0.6666666666666666
what sense  0.03125
sense a  0.125
being used  0.05555555555555555
what type  0.03125
information is  0.043478260869565216
being asked  0.05555555555555555
asked for  0.3333333333333333
for .  0.0036101083032490976
Advanced reasoning  0.2
reasoning for  0.14285714285714285
QA More  0.047619047619047616
More  0.1111111111111111
More sophisticated  0.3333333333333333
sophisticated questioners  0.14285714285714285
questioners  1.0
questioners expect  1.0
expect answers  0.3333333333333333
answers that  0.08333333333333333
are outside  0.004149377593360996
outside the  0.5
of written  0.0035650623885918
texts or  0.058823529411764705
or structured  0.0045045045045045045
structured databases  0.16666666666666666
To upgrade  0.1111111111111111
upgrade  1.0
upgrade a  1.0
system with  0.010752688172043012
with such  0.00546448087431694
such capabilities  0.008130081300813009
capabilities ,  0.2
to integrate  0.0013280212483399733
integrate  1.0
integrate reasoning  1.0
reasoning components  0.14285714285714285
components operating  0.2
operating  1.0
operating on  0.5
knowledge bases  0.037037037037037035
bases  1.0
bases ,  1.0
, encoding  0.0005614823133071309
encoding  1.0
encoding world  1.0
knowledge and  0.07407407407407407
and common-sense  0.001445086705202312
common-sense  1.0
common-sense reasoning  1.0
reasoning mechanisms  0.14285714285714285
mechanisms ,  0.5
as knowledge  0.003484320557491289
knowledge specific  0.037037037037037035
specific to  0.047619047619047616
of domains  0.00089126559714795
domains .  0.25
User profiling  0.5
profiling  1.0
profiling for  1.0
QA The  0.047619047619047616
The user  0.005208333333333333
user profile  0.07142857142857142
profile  1.0
profile captures  0.3333333333333333
captures  1.0
captures data  1.0
data about  0.012987012987012988
questioner ,  0.5
, comprising  0.0005614823133071309
comprising context  0.5
context data  0.030303030303030304
, domain  0.0005614823133071309
domain of  0.1
interest ,  0.09090909090909091
, reasoning  0.0005614823133071309
reasoning schemes  0.14285714285714285
schemes frequently  0.5
frequently used  0.5
, common  0.0005614823133071309
common ground  0.04
ground  1.0
ground established  1.0
established  1.0
established within  1.0
within different  0.05555555555555555
different dialogues  0.02040816326530612
dialogues  1.0
dialogues between  1.0
user ,  0.07142857142857142
so forth  0.03333333333333333
forth  1.0
forth .  1.0
The profile  0.005208333333333333
profile may  0.3333333333333333
be represented  0.008438818565400843
represented  1.0
represented as  0.3333333333333333
a predefined  0.001226993865030675
predefined  1.0
predefined template  1.0
template ,  0.25
where each  0.02857142857142857
each template  0.022222222222222223
template slot  0.25
slot  1.0
slot represents  1.0
represents  1.0
represents a  0.75
different profile  0.02040816326530612
profile feature  0.3333333333333333
Profile templates  1.0
templates  1.0
templates may  1.0
be nested  0.004219409282700422
nested  1.0
nested one  1.0
one within  0.015384615384615385
within another  0.05555555555555555
History Some  0.5
early AI  0.2
AI systems  0.6666666666666666
were question  0.024390243902439025
answering systems  0.08333333333333333
Two of  0.2857142857142857
most famous  0.034482758620689655
famous QA  0.3333333333333333
of that  0.0017825311942959
that time  0.0035460992907801418
time are  0.030303030303030304
are BASEBALL  0.004149377593360996
BASEBALL  0.5
BASEBALL and  0.5
and LUNAR  0.001445086705202312
LUNAR  0.6666666666666666
LUNAR ,  0.6666666666666666
1960s .  0.3333333333333333
BASEBALL answered  0.5
answered questions  0.6
US baseball  0.14285714285714285
baseball  1.0
baseball league  1.0
league  1.0
league over  1.0
over a  0.08333333333333333
a period  0.00245398773006135
period  1.0
period of  0.5
one year  0.015384615384615385
turn ,  0.16666666666666666
, answered  0.0005614823133071309
the geological  0.0006920415224913495
geological  1.0
geological analysis  1.0
of rocks  0.00089126559714795
rocks  1.0
rocks returned  1.0
returned by  0.25
the Apollo  0.0006920415224913495
Apollo  1.0
Apollo moon  1.0
moon  1.0
moon missions  1.0
missions  1.0
missions .  1.0
Both QA  0.3333333333333333
were very  0.04878048780487805
very effective  0.024390243902439025
their chosen  0.029411764705882353
chosen domains  0.2
, LUNAR  0.0005614823133071309
LUNAR was  0.3333333333333333
was demonstrated  0.012987012987012988
demonstrated  1.0
demonstrated at  1.0
a lunar  0.001226993865030675
lunar  1.0
lunar science  1.0
science convention  0.1
convention  1.0
convention in  1.0
in 1971  0.0018726591760299626
1971 and  0.3333333333333333
answer 90  0.03333333333333333
the questions  0.0006920415224913495
questions in  0.038461538461538464
its domain  0.05714285714285714
domain posed  0.05
posed by  0.3333333333333333
people untrained  0.0625
untrained  1.0
untrained on  1.0
Further restricted-domain  0.3333333333333333
restricted-domain  1.0
restricted-domain QA  1.0
following years  0.06666666666666667
The common  0.005208333333333333
common feature  0.04
feature of  0.23076923076923078
all these  0.023255813953488372
systems is  0.026785714285714284
they had  0.025
had a  0.2857142857142857
a core  0.001226993865030675
core  1.0
core database  0.5
or knowledge  0.0045045045045045045
knowledge system  0.037037037037037035
system that  0.03225806451612903
that was  0.010638297872340425
was hand-written  0.012987012987012988
hand-written by  0.14285714285714285
by experts  0.005714285714285714
experts  1.0
experts of  1.0
the chosen  0.0006920415224913495
chosen domain  0.2
systems included  0.008928571428571428
included question-answering  0.125
question-answering  1.0
question-answering abilities  0.5
abilities  1.0
abilities .  1.0
famous early  0.3333333333333333
early systems  0.1
are SHRDLU  0.004149377593360996
SHRDLU and  0.16666666666666666
ELIZA .  0.1111111111111111
SHRDLU simulated  0.16666666666666666
simulated  1.0
simulated the  0.5
the operation  0.0006920415224913495
operation of  0.5
a robot  0.001226993865030675
robot  1.0
robot in  0.5
toy world  0.5
world -LRB-  0.06666666666666667
blocks world  0.25
world ''  0.06666666666666667
it offered  0.008547008547008548
offered  1.0
offered the  1.0
possibility to  0.25
to ask  0.0013280212483399733
the robot  0.0006920415224913495
robot questions  0.5
the state  0.001384083044982699
Again ,  1.0
the strength  0.001384083044982699
strength  1.0
strength of  0.6
this system  0.01098901098901099
the choice  0.0006920415224913495
very specific  0.024390243902439025
domain and  0.05
very simple  0.04878048780487805
simple world  0.038461538461538464
world with  0.06666666666666667
with rules  0.00546448087431694
of physics  0.00089126559714795
physics  1.0
physics that  1.0
were easy  0.024390243902439025
to encode  0.0013280212483399733
encode  1.0
encode in  1.0
in contrast  0.0018726591760299626
, simulated  0.0005614823133071309
simulated a  0.5
a conversation  0.001226993865030675
a psychologist  0.001226993865030675
psychologist  1.0
psychologist .  1.0
ELIZA was  0.1111111111111111
to converse  0.0013280212483399733
converse  1.0
converse on  1.0
topic by  0.125
by resorting  0.005714285714285714
resorting  1.0
resorting to  1.0
to very  0.0013280212483399733
simple rules  0.038461538461538464
that detected  0.0035460992907801418
detected  1.0
detected important  0.5
important words  0.0625
the person  0.002768166089965398
person 's  0.21052631578947367
's input  0.0196078431372549
It had  0.02631578947368421
very rudimentary  0.024390243902439025
rudimentary way  0.5
answer questions  0.03333333333333333
own it  0.16666666666666666
it led  0.008547008547008548
led to  0.6666666666666666
of chatterbots  0.00089126559714795
chatterbots such  0.5
ones that  0.1
that participate  0.0035460992907801418
participate  1.0
participate in  1.0
the annual  0.0006920415224913495
annual Loebner  0.5
Loebner  1.0
Loebner prize  1.0
prize  1.0
prize .  1.0
The 1970s  0.005208333333333333
1980s saw  0.1111111111111111
saw  1.0
saw the  1.0
of comprehensive  0.00089126559714795
comprehensive theories  0.2
theories in  0.2
which led  0.007246376811594203
of ambitious  0.00089126559714795
ambitious  1.0
ambitious projects  1.0
projects  1.0
projects in  0.5
in text  0.0018726591760299626
text comprehension  0.006289308176100629
comprehension and  0.14285714285714285
and question  0.001445086705202312
One example  0.07692307692307693
the Unix  0.001384083044982699
Unix  1.0
Unix Consultant  0.5
Consultant  1.0
Consultant -LRB-  1.0
-LRB- UC  0.0027100271002710027
UC  1.0
UC -RRB-  0.5
that answered  0.0035460992907801418
questions pertaining  0.038461538461538464
pertaining  1.0
pertaining to  1.0
Unix operating  0.5
operating system  0.5
system had  0.010752688172043012
comprehensive hand-crafted  0.2
hand-crafted knowledge  0.5
base of  0.25
it aimed  0.008547008547008548
at phrasing  0.014705882352941176
phrasing  1.0
phrasing the  1.0
accommodate various  0.2
Another project  0.07692307692307693
project was  0.07692307692307693
was LILOG  0.012987012987012988
LILOG  1.0
LILOG ,  0.5
a text-understanding  0.001226993865030675
text-understanding  1.0
text-understanding system  1.0
that operated  0.0035460992907801418
operated  1.0
operated on  0.5
of tourism  0.00089126559714795
tourism  1.0
tourism information  1.0
information in  0.043478260869565216
a German  0.001226993865030675
German city  0.25
city  1.0
city .  1.0
The systems  0.005208333333333333
the UC  0.0006920415224913495
UC and  0.5
and LILOG  0.001445086705202312
LILOG projects  0.5
projects never  0.5
never went  0.2
went past  0.2
past  1.0
past the  0.3333333333333333
the stage  0.0006920415224913495
stage of  0.4
simple demonstrations  0.038461538461538464
demonstrations  1.0
demonstrations ,  1.0
they helped  0.025
helped the  0.3333333333333333
of theories  0.00089126559714795
theories on  0.2
on computational  0.0047169811320754715
linguistics and  0.05
and reasoning  0.002890173410404624
reasoning .  0.14285714285714285
An increasing  0.0625
increasing number  0.3333333333333333
systems include  0.008928571428571428
Web as  0.1111111111111111
one more  0.015384615384615385
more corpus  0.010526315789473684
. .  0.0625
these tools  0.023809523809523808
tools mostly  0.16666666666666666
mostly  1.0
mostly work  0.5
using shallow  0.01694915254237288
shallow methods  0.16666666666666666
above --  0.07692307692307693
-- thus  0.04
thus returning  0.1
returning  1.0
returning a  1.0
an excerpt  0.007575757575757576
excerpt  1.0
excerpt containing  1.0
the probable  0.0006920415224913495
probable  1.0
probable answer  1.0
answer highlighted  0.03333333333333333
highlighted  1.0
highlighted ,  1.0
, plus  0.0005614823133071309
plus  1.0
plus some  1.0
some context  0.012048192771084338
, highly-specialized  0.0005614823133071309
highly-specialized  1.0
highly-specialized natural  1.0
language question-answering  0.006756756756756757
question-answering engines  0.5
engines ,  0.3333333333333333
as EAGLi  0.003484320557491289
EAGLi  1.0
EAGLi for  1.0
for health  0.0036101083032490976
health  1.0
health and  1.0
and life  0.001445086705202312
life scientists  0.25
scientists  1.0
scientists ,  1.0
been made  0.014705882352941176
made available  0.0625
The Future  0.005208333333333333
Future  0.5
Future of  0.5
of Question  0.00089126559714795
Question  0.2857142857142857
Question Answering  0.14285714285714285
Answering  1.0
Answering QA  1.0
been extended  0.014705882352941176
extended  1.0
extended in  1.0
years to  0.047619047619047616
explore critical  0.25
critical new  0.25
new scientific  0.041666666666666664
scientific and  0.5
and practical  0.001445086705202312
practical dimensions  0.5
dimensions For  0.3333333333333333
been developed  0.014705882352941176
developed to  0.038461538461538464
automatically answer  0.047619047619047616
answer temporal  0.03333333333333333
temporal  1.0
temporal and  0.5
and geospatial  0.001445086705202312
geospatial  1.0
geospatial questions  1.0
, definitional  0.0005614823133071309
definitional  1.0
definitional questions  1.0
, biographical  0.0005614823133071309
biographical  1.0
biographical questions  1.0
, multilingual  0.0005614823133071309
multilingual questions  0.3333333333333333
and questions  0.001445086705202312
questions from  0.038461538461538464
from multimedia  0.009615384615384616
multimedia -LRB-  0.5
, audio  0.0011229646266142617
audio  1.0
audio ,  1.0
, imagery  0.0005614823133071309
imagery  1.0
imagery ,  1.0
, video  0.0011229646266142617
video  1.0
video -RRB-  0.2
Additional aspects  1.0
aspects such  0.14285714285714285
as interactivity  0.003484320557491289
interactivity  1.0
interactivity -LRB-  1.0
often required  0.022727272727272728
required for  0.14285714285714285
for clarification  0.0036101083032490976
questions or  0.038461538461538464
or answers  0.0045045045045045045
answers -RRB-  0.08333333333333333
, answer  0.0005614823133071309
answer reuse  0.03333333333333333
reuse  1.0
reuse ,  1.0
and knowledge  0.001445086705202312
knowledge representation  0.037037037037037035
reasoning to  0.14285714285714285
support question  0.25
answering have  0.08333333333333333
been explored  0.014705882352941176
explored .  0.5
Future research  0.5
research may  0.023809523809523808
may explore  0.019230769230769232
explore what  0.25
what kinds  0.03125
kinds  1.0
kinds of  1.0
questions can  0.038461538461538464
be asked  0.004219409282700422
asked and  0.3333333333333333
and answered  0.001445086705202312
answered about  0.2
about social  0.025
including sentiment  0.07142857142857142
sentiment  1.0
sentiment analysis  0.52
analysis .  0.09230769230769231
A relationship  0.02
extraction task  0.03225806451612903
task requires  0.023809523809523808
the detection  0.0006920415224913495
detection  1.0
detection and  0.5
and classification  0.001445086705202312
classification of  0.058823529411764705
semantic relationship  0.047619047619047616
relationship mentions  0.16666666666666666
mentions within  0.3333333333333333
of artifacts  0.00089126559714795
artifacts  1.0
artifacts ,  1.0
typically from  0.05555555555555555
or XML  0.0045045045045045045
XML  1.0
XML documents  1.0
information extraction  0.021739130434782608
but IE  0.014705882352941176
IE additionally  0.3333333333333333
additionally  1.0
additionally requires  1.0
the removal  0.0006920415224913495
removal  1.0
removal of  1.0
of repeated  0.00089126559714795
repeated relations  0.5
relations -LRB-  0.08333333333333333
-LRB- disambiguation  0.0027100271002710027
and generally  0.001445086705202312
generally refers  0.09090909090909091
different relationships  0.02040816326530612
relationships .  0.16666666666666666
Approaches One  0.3333333333333333
One  0.07692307692307693
One approach  0.07692307692307693
problem involves  0.045454545454545456
of domain  0.00089126559714795
domain ontologies  0.1
Another approach  0.15384615384615385
approach involves  0.02857142857142857
involves visual  0.1
visual  1.0
visual detection  0.5
detection of  0.5
of meaningful  0.00089126559714795
meaningful relationships  0.125
relationships in  0.16666666666666666
in parametric  0.0018726591760299626
parametric  1.0
parametric values  1.0
of objects  0.00089126559714795
objects listed  0.2
listed  1.0
listed on  1.0
a data  0.001226993865030675
data table  0.012987012987012988
table that  0.14285714285714285
that shift  0.0035460992907801418
shift  1.0
shift positions  1.0
positions  1.0
positions as  1.0
table is  0.14285714285714285
is permuted  0.0020325203252032522
permuted  1.0
permuted automatically  1.0
automatically as  0.047619047619047616
as controlled  0.003484320557491289
controlled  1.0
controlled by  1.0
software user  0.037037037037037035
The poor  0.005208333333333333
poor  1.0
poor coverage  1.0
coverage ,  0.3333333333333333
, rarity  0.0005614823133071309
rarity  1.0
rarity and  1.0
development cost  0.08333333333333333
cost related  0.5
to structured  0.0013280212483399733
structured resources  0.16666666666666666
resources such  0.16666666666666666
as semantic  0.003484320557491289
semantic lexicons  0.047619047619047616
lexicons -LRB-  0.5
e.g. WordNet  0.017857142857142856
WordNet ,  0.5
, UMLS  0.0005614823133071309
UMLS  1.0
UMLS -RRB-  1.0
and domain  0.001445086705202312
ontologies -LRB-  0.16666666666666666
the Gene  0.0006920415224913495
Gene  1.0
Gene Ontology  1.0
Ontology  1.0
Ontology -RRB-  1.0
has given  0.011904761904761904
given rise  0.041666666666666664
rise  1.0
rise to  0.5
new approaches  0.041666666666666664
approaches based  0.03571428571428571
on broad  0.0047169811320754715
broad ,  0.25
, dynamic  0.0005614823133071309
dynamic background  0.2
background  1.0
background knowledge  0.3333333333333333
knowledge on  0.037037037037037035
the Web  0.001384083044982699
the ARCHILES  0.0006920415224913495
ARCHILES  1.0
ARCHILES technique  1.0
technique uses  0.14285714285714285
uses only  0.07142857142857142
only Wikipedia  0.02631578947368421
Wikipedia and  0.5
and search  0.001445086705202312
search engine  0.09090909090909091
engine page  0.16666666666666666
page count  0.14285714285714285
count for  0.2
for acquiring  0.0036101083032490976
acquiring  1.0
acquiring coarse-grained  1.0
coarse-grained  1.0
coarse-grained relations  1.0
relations to  0.08333333333333333
to construct  0.0013280212483399733
construct lightweight  0.3333333333333333
lightweight  1.0
lightweight ontologies  1.0
The relationships  0.005208333333333333
relationships can  0.16666666666666666
represented using  0.16666666666666666
of formalisms\/languages  0.00089126559714795
formalisms\/languages  1.0
formalisms\/languages .  1.0
One such  0.07692307692307693
such representation  0.008130081300813009
data on  0.012987012987012988
Web is  0.1111111111111111
is RDF  0.0020325203252032522
RDF  1.0
RDF .  1.0
Jump to  1.0
to :  0.0013280212483399733
: navigation  0.00980392156862745
navigation  1.0
navigation ,  0.5
, search  0.0011229646266142617
search Sentence  0.09090909090909091
Sentence  0.4
Sentence boundary  0.2
disambiguation -LRB-  0.1
-LRB- SBD  0.0027100271002710027
SBD  1.0
SBD -RRB-  1.0
sentence breaking  0.020833333333333332
breaking ,  0.5
processing of  0.037037037037037035
of deciding  0.00089126559714795
deciding where  0.16666666666666666
where sentences  0.05714285714285714
sentences begin  0.013157894736842105
begin and  0.3333333333333333
and end  0.001445086705202312
end .  0.125
Often natural  0.3333333333333333
processing tools  0.018518518518518517
tools require  0.16666666666666666
require their  0.045454545454545456
their input  0.029411764705882353
be divided  0.004219409282700422
into sentences  0.01282051282051282
sentences for  0.013157894736842105
of reasons  0.00089126559714795
reasons .  0.5
However sentence  0.02702702702702703
boundary identification  0.16666666666666666
identification is  0.2
is challenging  0.0020325203252032522
challenging  1.0
challenging because  1.0
because punctuation  0.03333333333333333
marks are  0.25
often ambiguous  0.022727272727272728
period may  0.5
may denote  0.019230769230769232
denote  1.0
denote an  0.5
an abbreviation  0.007575757575757576
abbreviation ,  0.5
, decimal  0.0005614823133071309
decimal  1.0
decimal point  1.0
point  1.0
point ,  0.6666666666666666
an ellipsis  0.007575757575757576
ellipsis  1.0
ellipsis ,  1.0
or an  0.009009009009009009
an email  0.007575757575757576
email  1.0
email address  0.5
address -  0.25
- not  0.0625
About 47  0.5
47  1.0
47 %  1.0
the periods  0.0006920415224913495
periods in  0.3333333333333333
Journal corpus  0.3333333333333333
corpus denote  0.03225806451612903
denote abbreviations  0.5
abbreviations .  0.2
As well  0.05555555555555555
well ,  0.03571428571428571
question marks  0.023809523809523808
marks and  0.25
and exclamation  0.001445086705202312
exclamation  1.0
exclamation marks  1.0
marks may  0.25
may appear  0.019230769230769232
in embedded  0.0018726591760299626
embedded quotations  0.25
quotations  1.0
quotations ,  1.0
, emoticons  0.0005614823133071309
emoticons  1.0
emoticons ,  1.0
computer code  0.022727272727272728
code ,  0.14285714285714285
and slang  0.001445086705202312
slang  1.0
slang .  1.0
Languages like  0.3333333333333333
like Japanese  0.03571428571428571
and Chinese  0.001445086705202312
Chinese have  0.14285714285714285
have unambiguous  0.009615384615384616
unambiguous sentence-ending  0.5
sentence-ending  1.0
sentence-ending markers  1.0
markers .  0.3333333333333333
-LRB- b  0.0027100271002710027
b  1.0
b -RRB-  1.0
-RRB- If  0.005633802816901409
If  0.2
the preceding  0.0006920415224913495
preceding  1.0
preceding token  1.0
token is  0.5
is on  0.0040650406504065045
on my  0.0047169811320754715
my  1.0
my hand-compiled  1.0
hand-compiled  1.0
hand-compiled list  1.0
of abbreviations  0.0017825311942959
abbreviations ,  0.4
then it  0.05714285714285714
it does  0.008547008547008548
does n't  0.1
n't end  0.25
end a  0.125
-LRB- c  0.0027100271002710027
c  1.0
c -RRB-  1.0
next token  0.14285714285714285
is capitalized  0.0020325203252032522
ends a  0.5
This strategy  0.015873015873015872
strategy gets  0.2
gets about  0.5
about 95  0.025
sentences correct  0.013157894736842105
rules from  0.023255813953488372
documents where  0.02631578947368421
sentence breaks  0.020833333333333332
breaks  1.0
breaks are  0.5
are pre-marked  0.004149377593360996
pre-marked  1.0
pre-marked .  1.0
Solutions have  1.0
been based  0.014705882352941176
entropy model  0.2
The SATZ  0.005208333333333333
SATZ  1.0
SATZ architecture  1.0
architecture uses  0.5
a neural  0.001226993865030675
neural network  0.3333333333333333
network to  0.16666666666666666
disambiguate sentence  0.3333333333333333
boundaries and  0.09090909090909091
and achieves  0.001445086705202312
achieves 98.5  0.5
98.5  1.0
98.5 %  1.0
or opinion  0.0045045045045045045
opinion mining  0.2
mining refers  0.2
the application  0.0006920415224913495
, computational  0.0005614823133071309
text analytics  0.006289308176100629
analytics  1.0
analytics to  1.0
identify and  0.08333333333333333
and extract  0.001445086705202312
extract subjective  0.25
in source  0.0018726591760299626
source materials  0.041666666666666664
materials .  0.5
, sentiment  0.0005614823133071309
analysis aims  0.015384615384615385
the attitude  0.0006920415224913495
attitude  1.0
attitude of  0.5
a speaker  0.00245398773006135
speaker or  0.05555555555555555
a writer  0.001226993865030675
writer  1.0
writer with  1.0
some topic  0.012048192771084338
topic or  0.125
overall contextual  0.16666666666666666
contextual polarity  0.5
polarity of  0.25
The attitude  0.005208333333333333
attitude may  0.5
be his  0.004219409282700422
his or  0.08333333333333333
her judgement  0.5
judgement or  0.3333333333333333
or evaluation  0.0045045045045045045
evaluation -LRB-  0.018518518518518517
see appraisal  0.05
appraisal  1.0
appraisal theory  1.0
theory -RRB-  0.07692307692307693
, affective  0.0005614823133071309
affective  1.0
affective state  1.0
state -LRB-  0.07142857142857142
say ,  0.2857142857142857
the emotional  0.001384083044982699
emotional  1.0
emotional state  0.25
author when  0.3333333333333333
writing -RRB-  0.1111111111111111
intended emotional  0.2
emotional communication  0.25
communication -LRB-  0.4
emotional effect  0.25
effect the  0.5
author wishes  0.3333333333333333
wishes  1.0
wishes to  1.0
have on  0.009615384615384616
reader -RRB-  0.1
Advanced ,  0.2
`` beyond  0.005291005291005291
beyond polarity  0.16666666666666666
'' sentiment  0.005376344086021506
sentiment classification  0.04
classification looks  0.058823529411764705
looks ,  0.25
at emotional  0.014705882352941176
emotional states  0.25
states such  0.25
`` angry  0.005291005291005291
angry  1.0
angry ,  0.5
'' ``  0.005376344086021506
`` sad  0.005291005291005291
sad  1.0
sad ,  1.0
`` happy  0.005291005291005291
happy  1.0
happy .  1.0
Early work  0.5
work in  0.125
that area  0.0035460992907801418
area includes  0.09090909090909091
includes Turney  0.14285714285714285
and Pang  0.001445086705202312
Pang  1.0
Pang who  0.3333333333333333
who applied  0.1
applied different  0.06666666666666667
different methods  0.02040816326530612
for detecting  0.0036101083032490976
detecting  1.0
detecting the  1.0
the polarity  0.0006920415224913495
of product  0.00089126559714795
product reviews  0.14285714285714285
reviews and  0.16666666666666666
and movie  0.001445086705202312
movie reviews  0.3333333333333333
reviews respectively  0.16666666666666666
respectively  1.0
respectively .  1.0
is at  0.0020325203252032522
document level  0.027777777777777776
level .  0.05
One can  0.07692307692307693
also classify  0.014492753623188406
classify a  0.5
document 's  0.027777777777777776
's polarity  0.0196078431372549
polarity on  0.125
a multi-way  0.001226993865030675
multi-way  1.0
multi-way scale  1.0
scale  1.0
scale ,  0.3333333333333333
was attempted  0.012987012987012988
attempted  1.0
attempted by  1.0
by Pang  0.005714285714285714
Pang and  0.3333333333333333
and Snyder  0.001445086705202312
Snyder  1.0
Snyder -LRB-  0.5
among others  0.125
others -RRB-  0.08333333333333333
: expanded  0.00980392156862745
expanded  1.0
expanded the  1.0
the basic  0.001384083044982699
basic task  0.07692307692307693
of classifying  0.00089126559714795
classifying a  0.4
movie review  0.3333333333333333
review as  0.3333333333333333
as either  0.003484320557491289
either positive  0.1
negative to  0.125
to predicting  0.0013280212483399733
predicting  1.0
predicting star  0.5
star  1.0
star ratings  0.5
ratings on  0.1111111111111111
on either  0.0047169811320754715
a 3  0.001226993865030675
3 or  0.2
a 4  0.001226993865030675
4 star  0.2
star scale  0.5
while Snyder  0.05
Snyder performed  0.5
performed an  0.1
an in-depth  0.007575757575757576
in-depth analysis  0.3333333333333333
of restaurant  0.00089126559714795
restaurant  1.0
restaurant reviews  0.5
reviews ,  0.5
, predicting  0.0005614823133071309
predicting ratings  0.5
ratings for  0.1111111111111111
for various  0.0036101083032490976
various aspects  0.05555555555555555
given restaurant  0.041666666666666664
restaurant ,  0.5
the food  0.0006920415224913495
food  1.0
food and  1.0
and atmosphere  0.001445086705202312
atmosphere  1.0
atmosphere -LRB-  1.0
a five-star  0.001226993865030675
five-star  1.0
five-star scale  1.0
scale -RRB-  0.16666666666666666
different method  0.02040816326530612
determining sentiment  0.16666666666666666
sentiment is  0.04
a scaling  0.001226993865030675
scaling  1.0
scaling system  1.0
system whereby  0.010752688172043012
whereby  1.0
whereby words  1.0
words commonly  0.009174311926605505
commonly associated  0.125
associated with  0.25
with having  0.00546448087431694
having a  0.2
a negative  0.001226993865030675
negative ,  0.125
, neutral  0.0005614823133071309
neutral  1.0
neutral or  0.5
or positive  0.0045045045045045045
positive sentiment  0.14285714285714285
sentiment with  0.04
with them  0.01639344262295082
given an  0.041666666666666664
an associated  0.007575757575757576
associated number  0.25
number on  0.023255813953488372
a -5  0.001226993865030675
-5  1.0
-5 to  1.0
to +5  0.0013280212483399733
+5  1.0
+5 scale  1.0
scale -LRB-  0.16666666666666666
most negative  0.017241379310344827
negative up  0.125
to most  0.0013280212483399733
most positive  0.017241379310344827
positive -RRB-  0.14285714285714285
and when  0.001445086705202312
of unstructured  0.00089126559714795
unstructured  1.0
unstructured text  1.0
analyzed using  0.2
using natural  0.01694915254237288
the subsequent  0.0006920415224913495
subsequent concepts  0.5
are analyzed  0.004149377593360996
analyzed for  0.2
for an  0.0036101083032490976
they relate  0.025
relate  1.0
relate to  1.0
the concept  0.001384083044982699
concept -LRB-  0.25
Each concept  0.16666666666666666
concept is  0.25
then given  0.02857142857142857
a score  0.001226993865030675
score based  0.16666666666666666
way sentiment  0.041666666666666664
sentiment words  0.04
words relate  0.009174311926605505
concept ,  0.25
their associated  0.029411764705882353
associated score  0.25
allows movement  0.125
movement  1.0
movement to  1.0
sophisticated understanding  0.14285714285714285
of sentiment  0.004456327985739751
sentiment based  0.04
an 11  0.007575757575757576
11  1.0
11 point  1.0
point scale  0.3333333333333333
scale .  0.16666666666666666
, texts  0.0005614823133071309
texts can  0.058823529411764705
be given  0.004219409282700422
a positive  0.001226993865030675
negative sentiment  0.125
sentiment strength  0.04
strength score  0.2
score if  0.16666666666666666
the sentiment  0.001384083044982699
sentiment in  0.08
text rather  0.006289308176100629
overall polarity  0.16666666666666666
polarity and  0.125
and strength  0.001445086705202312
Another research  0.07692307692307693
research direction  0.023809523809523808
direction is  0.3333333333333333
is subjectivity\/objectivity  0.0020325203252032522
subjectivity\/objectivity  1.0
subjectivity\/objectivity identification  1.0
identification .  0.2
commonly defined  0.125
defined as  0.16666666666666666
as classifying  0.003484320557491289
given text  0.041666666666666664
-LRB- usually  0.005420054200542005
usually a  0.03125
sentence -RRB-  0.041666666666666664
two classes  0.034482758620689655
classes :  0.2
: objective  0.00980392156862745
objective or  0.2
problem can  0.022727272727272728
can sometimes  0.0055248618784530384
sometimes be  0.07692307692307693
than polarity  0.022222222222222223
polarity classification  0.125
classification :  0.058823529411764705
the subjectivity  0.0006920415224913495
subjectivity  1.0
subjectivity of  0.5
phrases may  0.0625
may depend  0.019230769230769232
their context  0.029411764705882353
an objective  0.007575757575757576
objective document  0.2
document may  0.05555555555555555
may contain  0.038461538461538464
contain subjective  0.08333333333333333
subjective sentences  0.16666666666666666
article quoting  0.034482758620689655
quoting  1.0
quoting people  1.0
people 's  0.0625
's opinions  0.0196078431372549
opinions  1.0
opinions -RRB-  0.5
as mentioned  0.003484320557491289
mentioned by  0.16666666666666666
by Su  0.005714285714285714
Su  1.0
Su ,  1.0
, results  0.0005614823133071309
are largely  0.004149377593360996
largely dependent  0.2
of subjectivity  0.00089126559714795
subjectivity used  0.5
when annotating  0.02857142857142857
annotating  1.0
annotating texts  1.0
, Pang  0.0005614823133071309
Pang showed  0.3333333333333333
that removing  0.0035460992907801418
removing objective  0.5
objective sentences  0.2
document before  0.027777777777777776
before classifying  0.16666666666666666
classifying its  0.2
its polarity  0.02857142857142857
polarity helped  0.125
helped improve  0.3333333333333333
improve performance  0.07692307692307693
performance .  0.16666666666666666
The more  0.005208333333333333
more fine-grained  0.010526315789473684
fine-grained  1.0
fine-grained analysis  1.0
analysis model  0.015384615384615385
the feature\/aspect-based  0.0006920415224913495
feature\/aspect-based  1.0
feature\/aspect-based sentiment  1.0
It refers  0.02631578947368421
to determining  0.0013280212483399733
the opinions  0.0006920415224913495
opinions or  0.5
or sentiments  0.0045045045045045045
sentiments  1.0
sentiments expressed  1.0
expressed on  0.3333333333333333
on different  0.0047169811320754715
different features  0.02040816326530612
features or  0.038461538461538464
or aspects  0.0045045045045045045
of entities  0.00089126559714795
entities ,  0.2857142857142857
a cell  0.00245398773006135
cell  1.0
cell phone  1.0
phone ,  0.5
a digital  0.00245398773006135
digital camera  0.14285714285714285
camera  1.0
camera ,  0.5
a bank  0.001226993865030675
bank  1.0
bank .  1.0
A feature  0.02
feature or  0.07692307692307693
or aspect  0.0045045045045045045
aspect is  0.5
an attribute  0.007575757575757576
attribute or  0.5
or component  0.0045045045045045045
an entity  0.007575757575757576
the screen  0.0006920415224913495
screen  1.0
screen of  1.0
the picture  0.001384083044982699
picture quality  0.25
a camera  0.001226993865030675
camera .  0.5
involves several  0.1
several sub-problems  0.045454545454545456
sub-problems  1.0
sub-problems ,  1.0
identifying relevant  0.16666666666666666
relevant entities  0.14285714285714285
, extracting  0.0005614823133071309
extracting their  0.2
their features\/aspects  0.029411764705882353
features\/aspects  1.0
features\/aspects ,  1.0
and determining  0.001445086705202312
determining whether  0.16666666666666666
whether an  0.07692307692307693
an opinion  0.007575757575757576
opinion expressed  0.2
on each  0.0047169811320754715
each feature\/aspect  0.022222222222222223
feature\/aspect  1.0
feature\/aspect is  1.0
is positive  0.0020325203252032522
positive ,  0.14285714285714285
, negative  0.0005614823133071309
negative or  0.125
or neutral  0.0045045045045045045
neutral .  0.5
More detailed  0.1111111111111111
detailed  1.0
detailed discussions  0.5
discussions  1.0
discussions about  0.3333333333333333
about this  0.025
this level  0.01098901098901099
in Liu  0.0018726591760299626
Liu  1.0
Liu 's  1.0
's NLP  0.0196078431372549
NLP Handbook  0.02127659574468085
Handbook  1.0
Handbook chapter  1.0
chapter  1.0
chapter ,  1.0
`` Sentiment  0.005291005291005291
Sentiment  0.16666666666666666
Sentiment Analysis  0.16666666666666666
Analysis and  0.2
and Subjectivity  0.001445086705202312
Subjectivity  1.0
Subjectivity ''  1.0
Methods Computers  0.25
Computers  1.0
Computers can  1.0
can perform  0.0055248618784530384
perform automated  0.09090909090909091
automated sentiment  0.14285714285714285
of digital  0.00089126559714795
digital texts  0.14285714285714285
texts ,  0.11764705882352941
using elements  0.01694915254237288
elements from  0.25
learning such  0.023255813953488372
as latent  0.003484320557491289
latent  1.0
latent semantic  1.0
, support  0.0005614823133071309
support vector  0.25
vector machines  0.3333333333333333
machines ,  0.25
`` bag  0.005291005291005291
bag  1.0
bag of  1.0
and Semantic  0.001445086705202312
Semantic Orientation  0.3333333333333333
Orientation  1.0
Orientation --  1.0
-- Pointwise  0.04
Pointwise  1.0
Pointwise Mutual  1.0
Mutual  1.0
Mutual Information  1.0
Information -LRB-  0.2
See Peter  0.16666666666666666
Peter  1.0
Peter Turney  1.0
's work  0.0196078431372549
area -RRB-  0.09090909090909091
sophisticated methods  0.14285714285714285
methods try  0.022727272727272728
to detect  0.0013280212483399733
detect  1.0
detect the  1.0
the holder  0.0006920415224913495
holder  1.0
holder of  1.0
a sentiment  0.00245398773006135
sentiment -LRB-  0.04
person who  0.05263157894736842
who maintains  0.1
maintains  1.0
maintains that  1.0
that affective  0.0035460992907801418
state -RRB-  0.07142857142857142
target -LRB-  0.09090909090909091
the entity  0.0006920415224913495
entity about  0.2
about which  0.025
the affect  0.0006920415224913495
affect is  0.3333333333333333
is felt  0.0020325203252032522
felt  1.0
felt -RRB-  1.0
To mine  0.1111111111111111
mine  1.0
mine the  1.0
the opinion  0.0006920415224913495
and get  0.001445086705202312
the feature  0.0006920415224913495
feature which  0.07692307692307693
been opinionated  0.014705882352941176
opinionated  1.0
opinionated ,  1.0
grammatical relationships  0.09090909090909091
relationships of  0.16666666666666666
Grammatical dependency  1.0
dependency relations  0.2
relations are  0.08333333333333333
are obtained  0.008298755186721992
by deep  0.005714285714285714
deep parsing  0.14285714285714285
parsing of  0.07142857142857142
Open source  1.0
source software  0.041666666666666664
software tools  0.037037037037037035
tools deploy  0.16666666666666666
deploy  1.0
deploy machine  1.0
statistics ,  0.125
to automate  0.0026560424966799467
automate sentiment  0.3333333333333333
analysis on  0.015384615384615385
on large  0.0047169811320754715
large collections  0.043478260869565216
collections of  0.25
including web  0.07142857142857142
web pages  0.125
, online  0.0016844469399213925
online news  0.125
news ,  0.07692307692307693
, internet  0.0005614823133071309
internet  1.0
internet discussion  1.0
discussion groups  0.5
groups ,  0.2
, web  0.0005614823133071309
web blogs  0.125
blogs  1.0
blogs ,  0.5
media .  0.16666666666666666
Evaluation The  0.1111111111111111
analysis system  0.015384615384615385
in principle  0.0018726591760299626
principle  1.0
principle ,  1.0
, how  0.0005614823133071309
it agrees  0.008547008547008548
agrees  1.0
agrees with  1.0
human judgments  0.021739130434782608
judgments  1.0
judgments .  1.0
usually measured  0.03125
by precision  0.005714285714285714
human raters  0.021739130434782608
raters  1.0
raters typically  1.0
typically agree  0.05555555555555555
agree about  0.3333333333333333
see Inter-rater  0.05
Inter-rater  1.0
Inter-rater reliability  1.0
reliability -RRB-  0.5
a 70  0.001226993865030675
accurate program  0.14285714285714285
program is  0.045454545454545456
is doing  0.0020325203252032522
doing as  0.5
as humans  0.003484320557491289
humans ,  0.16666666666666666
though such  0.1
such accuracy  0.008130081300813009
accuracy may  0.03225806451612903
not sound  0.008928571428571428
sound impressive  0.05
impressive .  0.5
If a  0.1
program were  0.045454545454545456
were ``  0.024390243902439025
`` right  0.005291005291005291
right ''  0.1
'' 100  0.005376344086021506
, humans  0.0005614823133071309
humans would  0.08333333333333333
would still  0.018867924528301886
still disagree  0.06666666666666667
disagree  1.0
disagree with  0.3333333333333333
it about  0.008547008547008548
about 30  0.025
30 %  0.3333333333333333
since they  0.1
they disagree  0.025
disagree that  0.3333333333333333
that much  0.0035460992907801418
much about  0.045454545454545456
about any  0.025
any answer  0.03225806451612903
sophisticated measures  0.14285714285714285
applied ,  0.06666666666666667
but evaluation  0.014705882352941176
analysis systems  0.015384615384615385
systems remains  0.008928571428571428
remains a  0.25
complex matter  0.041666666666666664
matter .  0.3333333333333333
For sentiment  0.01639344262295082
analysis tasks  0.015384615384615385
tasks returning  0.03125
a scale  0.001226993865030675
scale rather  0.16666666666666666
binary judgement  0.25
judgement ,  0.3333333333333333
, correlation  0.0005614823133071309
correlation  1.0
correlation is  0.5
a better  0.00245398773006135
better measure  0.1111111111111111
measure than  0.09090909090909091
than precision  0.022222222222222223
precision because  0.2
it takes  0.008547008547008548
takes into  0.3333333333333333
account how  0.3333333333333333
how close  0.034482758620689655
close  1.0
close the  1.0
the predicted  0.0006920415224913495
predicted value  0.5
value is  0.3333333333333333
target value  0.09090909090909091
value .  0.3333333333333333
test the  0.1
the relationship  0.0006920415224913495
relationship between  0.16666666666666666
between Internet  0.02564102564102564
Internet  0.5
Internet financial  0.5
financial message  0.25
message boards  0.5
boards  1.0
boards and  1.0
the behavior  0.0006920415224913495
behavior of  0.5
the stock  0.0006920415224913495
stock market  0.3333333333333333
market  1.0
market to  0.3333333333333333
find a  0.15384615384615385
a strong  0.00245398773006135
strong  1.0
strong correlation  0.25
correlation between  0.5
between posts  0.02564102564102564
posts  1.0
posts and  1.0
and volume  0.001445086705202312
of stock  0.00089126559714795
and Web  0.001445086705202312
Web 2.0  0.1111111111111111
2.0  1.0
2.0 The  0.5
The rise  0.005208333333333333
rise of  0.5
media such  0.16666666666666666
as blogs  0.003484320557491289
blogs and  0.5
networks has  0.07142857142857142
has fueled  0.011904761904761904
fueled  1.0
fueled interest  1.0
in sentiment  0.0018726591760299626
the proliferation  0.0006920415224913495
proliferation  1.0
proliferation of  1.0
of reviews  0.00089126559714795
, ratings  0.0005614823133071309
ratings ,  0.1111111111111111
, recommendations  0.0005614823133071309
recommendations  1.0
recommendations and  1.0
other forms  0.014285714285714285
of online  0.00089126559714795
online expression  0.125
online opinion  0.125
opinion has  0.2
has turned  0.011904761904761904
turned  1.0
turned into  1.0
a kind  0.001226993865030675
of virtual  0.00089126559714795
virtual  1.0
virtual currency  1.0
currency  1.0
currency for  1.0
for businesses  0.0036101083032490976
businesses  1.0
businesses looking  0.5
looking to  0.4
to market  0.0013280212483399733
market their  0.3333333333333333
their products  0.029411764705882353
products ,  0.25
identify new  0.08333333333333333
new opportunities  0.041666666666666664
opportunities  1.0
opportunities and  1.0
and manage  0.001445086705202312
manage  1.0
manage their  1.0
their reputations  0.029411764705882353
reputations  1.0
reputations .  1.0
As businesses  0.05555555555555555
businesses look  0.5
look to  0.2
automate the  0.3333333333333333
of filtering  0.00089126559714795
filtering  1.0
filtering out  1.0
the noise  0.0006920415224913495
noise  1.0
noise ,  0.125
, understanding  0.0005614823133071309
the conversations  0.0006920415224913495
conversations ,  0.3333333333333333
the relevant  0.0006920415224913495
relevant content  0.14285714285714285
and actioning  0.001445086705202312
actioning  1.0
actioning it  1.0
it appropriately  0.008547008547008548
appropriately  1.0
appropriately ,  0.5
many are  0.019230769230769232
now looking  0.07692307692307693
If web  0.1
web 2.0  0.125
2.0 was  0.5
was all  0.012987012987012988
all about  0.023255813953488372
about democratizing  0.025
democratizing  1.0
democratizing publishing  0.5
publishing  1.0
publishing ,  1.0
web may  0.125
may well  0.019230769230769232
well be  0.03571428571428571
be based  0.004219409282700422
on democratizing  0.0047169811320754715
democratizing data  0.5
mining of  0.2
content that  0.08333333333333333
getting published  0.25
published .  0.14285714285714285
One step  0.07692307692307693
step towards  0.06666666666666667
towards  1.0
towards this  1.0
this aim  0.01098901098901099
aim is  0.5
is accomplished  0.0020325203252032522
accomplished  1.0
accomplished in  1.0
in research  0.0018726591760299626
Several research  0.3333333333333333
research teams  0.023809523809523808
teams in  0.5
in universities  0.0018726591760299626
universities  1.0
universities around  1.0
world currently  0.06666666666666667
currently focus  0.14285714285714285
on understanding  0.0047169811320754715
the dynamics  0.0006920415224913495
dynamics  1.0
dynamics of  0.5
in e-communities  0.0018726591760299626
e-communities  1.0
e-communities through  0.5
through sentiment  0.125
The CyberEmotions  0.005208333333333333
CyberEmotions  1.0
CyberEmotions project  1.0
, recently  0.0005614823133071309
recently identified  0.3333333333333333
identified the  0.2
role of  0.25
of negative  0.00089126559714795
negative emotions  0.125
emotions  1.0
emotions in  1.0
in driving  0.0018726591760299626
driving  1.0
driving social  1.0
networks discussions  0.07142857142857142
discussions .  0.3333333333333333
analysis could  0.015384615384615385
could therefore  0.0625
therefore help  0.2
help understand  0.1111111111111111
understand why  0.14285714285714285
why certain  0.14285714285714285
certain e-communities  0.14285714285714285
e-communities die  0.5
die  1.0
die or  1.0
or fade  0.0045045045045045045
fade  1.0
fade away  1.0
away -LRB-  0.5
, MySpace  0.0005614823133071309
MySpace  1.0
MySpace -RRB-  1.0
-RRB- while  0.0028169014084507044
others seem  0.08333333333333333
seem to  0.5
to grow  0.0013280212483399733
grow  1.0
grow without  1.0
without limits  0.07692307692307693
limits  1.0
limits -LRB-  1.0
, Facebook  0.0005614823133071309
Facebook  1.0
Facebook -RRB-  1.0
problem is  0.11363636363636363
that most  0.0035460992907801418
most sentiment  0.017241379310344827
analysis algorithms  0.015384615384615385
algorithms use  0.02857142857142857
use simple  0.013888888888888888
simple terms  0.038461538461538464
terms to  0.07692307692307693
to express  0.0013280212483399733
express sentiment  0.2
sentiment about  0.04
about a  0.025
a product  0.001226993865030675
product or  0.14285714285714285
or service  0.0045045045045045045
service .  0.2
, cultural  0.0005614823133071309
cultural  1.0
cultural factors  1.0
factors ,  0.3333333333333333
, linguistic  0.0005614823133071309
linguistic nuances  0.0625
nuances  1.0
nuances and  1.0
and differing  0.001445086705202312
contexts make  0.14285714285714285
it extremely  0.008547008547008548
to turn  0.0013280212483399733
turn a  0.16666666666666666
a string  0.00245398773006135
written text  0.11538461538461539
a simple  0.001226993865030675
simple pro  0.038461538461538464
pro  1.0
pro or  1.0
or con  0.0045045045045045045
con  1.0
con sentiment  1.0
sentiment .  0.04
The fact  0.005208333333333333
humans often  0.08333333333333333
often disagree  0.022727272727272728
disagree on  0.3333333333333333
sentiment of  0.04
text illustrates  0.006289308176100629
illustrates how  0.5
how big  0.034482758620689655
big a  0.5
task it  0.023809523809523808
is for  0.0020325203252032522
for computers  0.0036101083032490976
computers to  0.1111111111111111
get this  0.14285714285714285
this right  0.01098901098901099
The shorter  0.005208333333333333
shorter  1.0
shorter the  0.5
the string  0.0006920415224913495
harder it  0.14285714285714285
it becomes  0.03418803418803419
becomes  1.0
becomes .  0.25
n Computer  0.5
Computer Science  0.16666666666666666
Science ,  0.5
, Speech  0.0016844469399213925
of spoken  0.0017825311942959
spoken words  0.14285714285714285
into text  0.038461538461538464
`` automatic  0.005291005291005291
recognition ''  0.01652892561983471
`` ASR  0.005291005291005291
ASR  1.0
ASR ''  0.16666666666666666
`` computer  0.005291005291005291
computer speech  0.022727272727272728
`` speech  0.005291005291005291
to text  0.0026560424966799467
text ''  0.006289308176100629
or just  0.0045045045045045045
just ``  0.1111111111111111
`` STT  0.005291005291005291
STT  1.0
STT ''  1.0
Speech Recognition  0.0967741935483871
Recognition is  0.125
is technology  0.0020325203252032522
technology that  0.045454545454545456
can translate  0.0055248618784530384
translate spoken  0.16666666666666666
Some SR  0.047619047619047616
SR  1.0
SR systems  0.3333333333333333
use ``  0.013888888888888888
`` training  0.005291005291005291
training ''  0.03571428571428571
'' where  0.005376344086021506
where an  0.02857142857142857
an individual  0.007575757575757576
individual speaker  0.08333333333333333
speaker reads  0.05555555555555555
reads  1.0
reads sections  0.5
the SR  0.0006920415224913495
SR system  0.3333333333333333
systems analyze  0.008928571428571428
analyze the  0.25
's specific  0.0196078431372549
specific voice  0.047619047619047616
voice  1.0
voice and  0.07692307692307693
and use  0.004335260115606936
to fine  0.0013280212483399733
fine tune  0.5
tune  1.0
tune the  1.0
that person  0.0035460992907801418
's speech  0.0196078431372549
, resulting  0.0005614823133071309
resulting in  0.25
in more  0.0018726591760299626
accurate transcription  0.14285714285714285
transcription  1.0
transcription .  0.5
that do  0.0035460992907801418
use training  0.027777777777777776
training are  0.07142857142857142
are called  0.008298755186721992
`` Speaker  0.010582010582010581
Speaker  0.6666666666666666
Speaker Independent  0.16666666666666666
Independent  1.0
Independent ''  1.0
Speaker Dependent  0.16666666666666666
Dependent  1.0
Dependent ''  1.0
recognition applications  0.008264462809917356
include voice  0.037037037037037035
voice user  0.07692307692307693
interfaces such  0.5
as voice  0.003484320557491289
voice dialing  0.07692307692307693
dialing  1.0
dialing -LRB-  1.0
`` Call  0.005291005291005291
Call  1.0
Call home  1.0
home  1.0
home ''  1.0
, call  0.0005614823133071309
call routing  0.3333333333333333
routing -LRB-  0.3333333333333333
`` I  0.005291005291005291
I  1.0
I would  1.0
a collect  0.001226993865030675
collect  1.0
collect call  1.0
call ''  0.3333333333333333
, domotic  0.0005614823133071309
domotic  1.0
domotic appliance  1.0
appliance  1.0
appliance control  1.0
control ,  0.2
search -LRB-  0.09090909090909091
a podcast  0.001226993865030675
podcast  1.0
podcast where  1.0
where particular  0.02857142857142857
particular words  0.07692307692307693
were spoken  0.024390243902439025
spoken -RRB-  0.07142857142857142
simple data  0.038461538461538464
entry -LRB-  0.25
, entering  0.0011229646266142617
entering  1.0
entering a  0.5
card number  0.25
, preparation  0.0005614823133071309
preparation  1.0
preparation of  1.0
of structured  0.00089126559714795
structured documents  0.16666666666666666
a radiology  0.001226993865030675
radiology  1.0
radiology report  1.0
report -RRB-  0.25
, speech-to-text  0.0005614823133071309
speech-to-text  1.0
speech-to-text processing  0.5
, word  0.0005614823133071309
word processors  0.016666666666666666
processors  1.0
processors or  1.0
or emails  0.0045045045045045045
emails -RRB-  0.5
and aircraft  0.001445086705202312
aircraft  1.0
aircraft -LRB-  0.2857142857142857
usually termed  0.03125
termed Direct  0.25
Direct  1.0
Direct Voice  1.0
Voice  0.8
Voice Input  0.2
Input -RRB-  0.5
term voice  0.05555555555555555
voice recognition  0.07692307692307693
recognition refers  0.008264462809917356
to finding  0.0013280212483399733
`` who  0.005291005291005291
who ''  0.1
is speaking  0.0020325203252032522
than what  0.022222222222222223
what they  0.03125
are saying  0.004149377593360996
saying  1.0
saying .  1.0
Recognizing the  1.0
the speaker  0.001384083044982699
speaker can  0.05555555555555555
can simplify  0.0055248618784530384
simplify  1.0
simplify the  1.0
of translating  0.00089126559714795
translating speech  0.25
been trained  0.014705882352941176
trained  1.0
trained on  0.3333333333333333
on specific  0.0047169811320754715
specific person  0.047619047619047616
's voices  0.0196078431372549
voices  1.0
voices or  1.0
or it  0.0045045045045045045
to authenticate  0.0013280212483399733
authenticate  1.0
authenticate or  1.0
or verify  0.0045045045045045045
verify  1.0
verify the  1.0
speaker as  0.05555555555555555
a security  0.001226993865030675
security  1.0
security process  1.0
Front-End speech  1.0
is where  0.0040650406504065045
the provider  0.001384083044982699
provider  1.0
provider dictates  1.0
dictates  1.0
dictates into  1.0
a speech-recognition  0.0036809815950920245
speech-recognition  1.0
speech-recognition engine  0.6666666666666666
engine ,  0.16666666666666666
the recognized  0.001384083044982699
recognized words  0.16666666666666666
are displayed  0.004149377593360996
displayed as  0.5
are spoken  0.004149377593360996
the dictator  0.0006920415224913495
dictator  1.0
dictator is  1.0
is responsible  0.0020325203252032522
responsible  1.0
responsible for  1.0
for editing  0.0036101083032490976
editing and  0.5
and signing  0.001445086705202312
signing  1.0
signing off  1.0
off on  0.5
Back-End or  1.0
or deferred  0.0045045045045045045
deferred  1.0
deferred speech  1.0
digital dictation  0.14285714285714285
dictation  1.0
dictation system  1.0
the voice  0.0006920415224913495
voice is  0.07692307692307693
is routed  0.0040650406504065045
routed  1.0
routed through  0.5
speech-recognition machine  0.3333333333333333
machine and  0.012658227848101266
recognized draft  0.16666666666666666
draft  1.0
draft document  0.5
document is  0.027777777777777776
routed along  0.5
original voice  0.07692307692307693
voice file  0.07692307692307693
file  1.0
file to  1.0
the editor  0.0006920415224913495
editor  1.0
editor ,  1.0
the draft  0.0006920415224913495
draft is  0.5
is edited  0.0020325203252032522
edited  1.0
edited and  1.0
and report  0.001445086705202312
report finalized  0.25
finalized  1.0
finalized .  1.0
Deferred speech  1.0
the industry  0.0006920415224913495
industry currently  0.3333333333333333
currently .  0.14285714285714285
Many Electronic  0.08333333333333333
Electronic  1.0
Electronic Medical  0.5
Medical Records  0.5
Records  1.0
Records -LRB-  1.0
-LRB- EMR  0.0027100271002710027
EMR  1.0
EMR -RRB-  0.3333333333333333
-RRB- applications  0.0028169014084507044
applications can  0.04
more effective  0.010526315789473684
effective and  0.16666666666666666
performed more  0.1
more easily  0.010526315789473684
easily when  0.1111111111111111
when deployed  0.02857142857142857
deployed in  0.5
in conjunction  0.003745318352059925
conjunction with  0.6666666666666666
Searches ,  1.0
, queries  0.0005614823133071309
queries ,  0.3333333333333333
and form  0.001445086705202312
form filling  0.05
filling  1.0
filling may  1.0
may all  0.019230769230769232
all be  0.023255813953488372
be faster  0.004219409282700422
faster  1.0
faster to  0.3333333333333333
perform by  0.09090909090909091
by voice  0.011428571428571429
voice than  0.07692307692307693
than by  0.022222222222222223
a keyboard  0.001226993865030675
keyboard  1.0
keyboard .  0.3333333333333333
major issues  0.08333333333333333
issues relating  0.2
relating  1.0
relating to  1.0
recognition in  0.024793388429752067
in healthcare  0.0018726591760299626
healthcare  1.0
healthcare is  1.0
American Recovery  0.2
Recovery  1.0
Recovery and  1.0
and Reinvestment  0.001445086705202312
Reinvestment  1.0
Reinvestment Act  1.0
Act  1.0
Act of  1.0
of 2009  0.00089126559714795
2009 -LRB-  0.3333333333333333
-LRB- ARRA  0.0027100271002710027
ARRA  1.0
ARRA -RRB-  1.0
-RRB- provides  0.0028169014084507044
provides  1.0
provides for  0.5
for substantial  0.0036101083032490976
substantial financial  0.2
financial benefits  0.25
benefits to  0.5
to physicians  0.0013280212483399733
physicians  1.0
physicians who  1.0
who utilize  0.1
utilize  1.0
utilize an  0.5
an EMR  0.007575757575757576
EMR according  0.3333333333333333
`` Meaningful  0.005291005291005291
Meaningful  1.0
Meaningful Use  1.0
Use  0.5
Use ''  0.5
'' standards  0.005376344086021506
These standards  0.058823529411764705
standards require  0.2
require that  0.045454545454545456
a substantial  0.001226993865030675
substantial amount  0.2
data be  0.012987012987012988
be maintained  0.004219409282700422
maintained  1.0
maintained by  0.5
the EMR  0.0006920415224913495
EMR -LRB-  0.3333333333333333
-LRB- now  0.008130081300813009
now more  0.07692307692307693
commonly referred  0.125
an Electronic  0.007575757575757576
Electronic Health  0.5
Health Record  0.5
Record  1.0
Record or  1.0
or EHR  0.0045045045045045045
EHR  1.0
EHR -RRB-  0.3333333333333333
Unfortunately ,  1.0
many instances  0.019230769230769232
instances ,  0.3333333333333333
recognition within  0.008264462809917356
within an  0.05555555555555555
an EHR  0.007575757575757576
EHR will  0.3333333333333333
not lead  0.008928571428571428
data maintained  0.012987012987012988
maintained within  0.5
database ,  0.1
rather to  0.0625
to narrative  0.0013280212483399733
narrative  1.0
narrative text  1.0
this reason  0.02197802197802198
reason ,  0.5
, substantial  0.0005614823133071309
substantial resources  0.2
resources are  0.16666666666666666
being expended  0.05555555555555555
expended  1.0
expended to  1.0
allow for  0.2
of front-end  0.00089126559714795
front-end  1.0
front-end SR  1.0
SR while  0.3333333333333333
while capturing  0.05
capturing  1.0
capturing data  1.0
data within  0.012987012987012988
the EHR  0.0006920415224913495
EHR .  0.3333333333333333
Military High-performance  1.0
High-performance  1.0
High-performance fighter  1.0
fighter  1.0
fighter aircraft  0.5
aircraft Substantial  0.14285714285714285
Substantial  0.5
Substantial efforts  0.5
been devoted  0.014705882352941176
devoted in  0.2
last decade  0.4
decade  1.0
decade to  0.3333333333333333
test and  0.2
in fighter  0.0056179775280898875
aircraft .  0.14285714285714285
Of particular  1.0
particular note  0.07692307692307693
note  1.0
note is  1.0
U.S. program  0.14285714285714285
program in  0.09090909090909091
in speech  0.013108614232209739
recognition for  0.008264462809917356
the Advanced  0.0006920415224913495
Advanced  0.4
Advanced Fighter  0.2
Fighter  1.0
Fighter Technology  1.0
Technology  1.0
Technology Integration  0.3333333333333333
Integration  1.0
Integration -LRB-  1.0
-LRB- AFTI  0.0027100271002710027
AFTI  1.0
AFTI -RRB-  1.0
-RRB- \/  0.0028169014084507044
\/ F-16  0.3333333333333333
F-16  1.0
F-16 aircraft  0.5
-LRB- F-16  0.0027100271002710027
F-16 VISTA  0.5
VISTA  1.0
VISTA -RRB-  1.0
in France  0.003745318352059925
France installing  0.25
installing  1.0
installing speech  1.0
systems on  0.008928571428571428
on Mirage  0.0047169811320754715
Mirage  1.0
Mirage aircraft  1.0
aircraft ,  0.2857142857142857
and also  0.001445086705202312
also programs  0.014492753623188406
programs in  0.09090909090909091
UK dealing  0.25
of aircraft  0.00089126559714795
aircraft platforms  0.14285714285714285
platforms  1.0
platforms .  1.0
In these  0.009523809523809525
these programs  0.023809523809523808
speech recognizers  0.006578947368421052
recognizers  1.0
recognizers have  0.5
been operated  0.014705882352941176
operated successfully  0.5
successfully in  0.3333333333333333
with applications  0.00546448087431694
applications including  0.04
: setting  0.00980392156862745
setting radio  0.2
radio  1.0
radio frequencies  1.0
frequencies ,  0.5
, commanding  0.0005614823133071309
commanding  1.0
commanding an  1.0
an autopilot  0.007575757575757576
autopilot  1.0
autopilot system  1.0
, setting  0.0011229646266142617
setting steer-point  0.2
steer-point  1.0
steer-point coordinates  1.0
coordinates  1.0
coordinates and  1.0
and weapons  0.001445086705202312
weapons  1.0
weapons release  1.0
release parameters  0.3333333333333333
parameters ,  0.25
and controlling  0.001445086705202312
controlling  1.0
controlling flight  1.0
flight displays  0.5
displays  1.0
displays .  1.0
Working with  1.0
with Swedish  0.00546448087431694
Swedish  1.0
Swedish pilots  1.0
pilots  1.0
pilots flying  0.5
flying  1.0
flying in  1.0
the JAS-39  0.0006920415224913495
JAS-39  1.0
JAS-39 Gripen  1.0
Gripen  1.0
Gripen cockpit  1.0
cockpit  1.0
cockpit ,  0.5
, Englund  0.0005614823133071309
Englund  1.0
Englund -LRB-  1.0
-LRB- 2004  0.0027100271002710027
2004 -RRB-  0.3333333333333333
-RRB- found  0.0028169014084507044
found recognition  0.07142857142857142
recognition deteriorated  0.008264462809917356
deteriorated  1.0
deteriorated with  1.0
with increasing  0.00546448087431694
increasing G-loads  0.3333333333333333
G-loads  1.0
G-loads .  1.0
It was  0.05263157894736842
also concluded  0.014492753623188406
that adaptation  0.0035460992907801418
adaptation  1.0
adaptation greatly  0.3333333333333333
greatly improved  0.14285714285714285
improved the  0.25
results in  0.047619047619047616
in all  0.0056179775280898875
all cases  0.023255813953488372
cases and  0.05555555555555555
and introducing  0.001445086705202312
introducing  1.0
introducing models  1.0
for breathing  0.0036101083032490976
breathing  1.0
breathing was  1.0
improve recognition  0.15384615384615385
recognition scores  0.008264462809917356
scores significantly  0.2
significantly  1.0
significantly .  1.0
what might  0.03125
be expected  0.012658227848101266
expected ,  0.14285714285714285
no effects  0.07692307692307693
effects  1.0
effects of  1.0
the broken  0.0006920415224913495
broken  1.0
broken English  0.2
English of  0.02702702702702703
the speakers  0.0006920415224913495
speakers were  0.25
found .  0.07142857142857142
was evident  0.012987012987012988
evident that  0.5
that spontaneous  0.0035460992907801418
spontaneous  1.0
spontaneous speech  1.0
speech caused  0.006578947368421052
caused  1.0
caused problems  1.0
problems for  0.058823529411764705
the recognizer  0.0006920415224913495
recognizer  1.0
recognizer ,  1.0
as could  0.003484320557491289
expected .  0.14285714285714285
A restricted  0.02
restricted vocabulary  0.25
vocabulary ,  0.125
and above  0.001445086705202312
above all  0.07692307692307693
a proper  0.001226993865030675
proper syntax  0.14285714285714285
, could  0.0005614823133071309
could thus  0.0625
thus be  0.1
accuracy substantially  0.03225806451612903
substantially  1.0
substantially .  1.0
The Eurofighter  0.005208333333333333
Eurofighter  1.0
Eurofighter Typhoon  1.0
Typhoon  1.0
Typhoon currently  1.0
currently in  0.14285714285714285
in service  0.0018726591760299626
service with  0.2
UK RAF  0.25
RAF  1.0
RAF employs  1.0
employs a  0.5
a speaker-dependent  0.001226993865030675
speaker-dependent  1.0
speaker-dependent system  1.0
i.e. it  0.05263157894736842
requires each  0.0625
each pilot  0.022222222222222223
pilot  1.0
pilot to  0.4
template .  0.25
for any  0.0036101083032490976
any safety  0.03225806451612903
safety  1.0
safety critical  1.0
critical or  0.25
or weapon  0.0045045045045045045
weapon  1.0
weapon critical  0.5
critical tasks  0.25
as weapon  0.003484320557491289
weapon release  0.5
release or  0.3333333333333333
or lowering  0.0045045045045045045
lowering  1.0
lowering of  1.0
the undercarriage  0.0006920415224913495
undercarriage  1.0
undercarriage ,  1.0
other cockpit  0.014285714285714285
cockpit functions  0.5
functions .  0.5
Voice commands  0.2
commands are  0.2
are confirmed  0.004149377593360996
confirmed  1.0
confirmed by  1.0
by visual  0.005714285714285714
visual and\/or  0.5
and\/or aural  0.3333333333333333
aural  1.0
aural feedback  1.0
feedback .  0.5
is seen  0.0020325203252032522
major design  0.08333333333333333
design feature  0.25
feature in  0.07692307692307693
the reduction  0.0006920415224913495
reduction  1.0
reduction of  0.5
of pilot  0.00089126559714795
pilot workload  0.2
workload  1.0
workload ,  1.0
even allows  0.037037037037037035
the pilot  0.0006920415224913495
assign targets  0.2
targets  1.0
targets to  1.0
to himself  0.0013280212483399733
himself with  0.5
two simple  0.034482758620689655
simple voice  0.038461538461538464
voice commands  0.07692307692307693
commands or  0.2
his wingmen  0.08333333333333333
wingmen  1.0
wingmen with  1.0
with only  0.00546448087431694
only five  0.02631578947368421
five commands  0.2
commands .  0.4
Speaker independent  0.16666666666666666
independent  1.0
independent systems  0.5
also being  0.014492753623188406
being developed  0.05555555555555555
developed and  0.038461538461538464
in testing  0.0018726591760299626
testing for  0.2
the F35  0.0006920415224913495
F35  1.0
F35 Lightning  1.0
Lightning  1.0
Lightning II  1.0
II -LRB-  0.5
-LRB- JSF  0.0027100271002710027
JSF  1.0
JSF -RRB-  1.0
the Alenia  0.0006920415224913495
Alenia  1.0
Alenia Aermacchi  1.0
Aermacchi  1.0
Aermacchi M-346  1.0
M-346  1.0
M-346 Master  1.0
Master  1.0
Master lead-in  1.0
lead-in  1.0
lead-in fighter  1.0
fighter trainer  0.16666666666666666
trainer  1.0
trainer .  1.0
have produced  0.009615384615384616
produced word  0.1111111111111111
word accuracies  0.016666666666666666
accuracies  1.0
accuracies in  1.0
in excess  0.003745318352059925
excess  1.0
excess of  1.0
of 98  0.00089126559714795
Helicopters The  1.0
The problems  0.005208333333333333
problems of  0.058823529411764705
achieving high  0.5
high recognition  0.05555555555555555
accuracy under  0.03225806451612903
under stress  0.2
stress  1.0
stress and  0.5
and noise  0.001445086705202312
noise pertain  0.125
pertain  1.0
pertain strongly  1.0
strongly to  0.5
the helicopter  0.0020761245674740486
helicopter  1.0
helicopter environment  0.5
environment as  0.16666666666666666
the jet  0.0006920415224913495
jet  1.0
jet fighter  1.0
fighter environment  0.16666666666666666
environment .  0.3333333333333333
The acoustic  0.005208333333333333
acoustic  1.0
acoustic noise  0.3333333333333333
noise problem  0.125
actually more  0.3333333333333333
more severe  0.010526315789473684
severe  1.0
severe in  1.0
environment ,  0.16666666666666666
only because  0.02631578947368421
the high  0.001384083044982699
high noise  0.05555555555555555
noise levels  0.125
levels but  0.045454545454545456
also because  0.014492753623188406
helicopter pilot  0.25
pilot ,  0.2
not wear  0.008928571428571428
wear  1.0
wear a  1.0
a facemask  0.001226993865030675
facemask  1.0
facemask ,  1.0
would reduce  0.018867924528301886
reduce  1.0
reduce acoustic  1.0
noise in  0.125
the microphone  0.0006920415224913495
microphone  1.0
microphone .  1.0
Substantial test  0.5
evaluation programs  0.018518518518518517
programs have  0.09090909090909091
been carried  0.014705882352941176
carried out  0.5
the past  0.001384083044982699
past decade  0.3333333333333333
decade in  0.3333333333333333
systems applications  0.008928571428571428
in helicopters  0.003745318352059925
helicopters  1.0
helicopters ,  0.5
, notably  0.0005614823133071309
notably by  0.3333333333333333
U.S. Army  0.14285714285714285
Army Avionics  0.25
Avionics  1.0
Avionics Research  1.0
Research and  0.125
and Development  0.001445086705202312
Development  1.0
Development Activity  1.0
Activity  1.0
Activity -LRB-  1.0
-LRB- AVRADA  0.0027100271002710027
AVRADA  1.0
AVRADA -RRB-  0.5
and by  0.001445086705202312
the Royal  0.001384083044982699
Royal  1.0
Royal Aerospace  0.5
Aerospace  1.0
Aerospace Establishment  0.5
Establishment  1.0
Establishment -LRB-  1.0
-LRB- RAE  0.0027100271002710027
RAE  1.0
RAE -RRB-  1.0
Work in  0.5
France has  0.25
has included  0.011904761904761904
included speech  0.125
the Puma  0.0006920415224913495
Puma  1.0
Puma helicopter  1.0
helicopter .  0.25
There has  0.18181818181818182
been much  0.014705882352941176
much useful  0.045454545454545456
useful work  0.07142857142857142
in Canada  0.0018726591760299626
Canada .  0.16666666666666666
Results have  1.0
been encouraging  0.014705882352941176
encouraging  1.0
encouraging ,  1.0
and voice  0.001445086705202312
voice applications  0.07692307692307693
have included  0.009615384615384616
included :  0.125
: control  0.00980392156862745
control of  0.6
communication radios  0.2
radios  1.0
radios ,  1.0
setting of  0.2
of navigation  0.00089126559714795
navigation systems  0.5
an automated  0.007575757575757576
automated target  0.14285714285714285
target handover  0.09090909090909091
handover  1.0
handover system  1.0
fighter applications  0.16666666666666666
the overriding  0.0006920415224913495
overriding  1.0
overriding issue  1.0
issue for  0.125
for voice  0.0036101083032490976
voice in  0.07692307692307693
helicopters is  0.5
impact on  0.5
on pilot  0.0047169811320754715
pilot effectiveness  0.2
effectiveness .  0.3333333333333333
Encouraging results  1.0
are reported  0.004149377593360996
reported for  0.2
the AVRADA  0.0006920415224913495
AVRADA tests  0.5
tests ,  0.25
although these  0.16666666666666666
these represent  0.023809523809523808
represent only  0.1111111111111111
a feasibility  0.001226993865030675
feasibility  1.0
feasibility demonstration  0.5
demonstration in  0.2
test environment  0.1
Much remains  0.3333333333333333
remains to  0.25
done both  0.09090909090909091
both in  0.03225806451612903
in overall  0.0018726591760299626
overall speech  0.16666666666666666
recognition technology  0.008264462809917356
to consistently  0.0013280212483399733
consistently achieve  0.3333333333333333
achieve performance  0.5
performance improvements  0.05555555555555555
improvements  1.0
improvements in  0.5
in operational  0.0018726591760299626
operational  1.0
operational settings  1.0
settings  1.0
settings .  1.0
Battle management  0.5
management Question  0.14285714285714285
Question book-new  0.14285714285714285
book-new  1.0
book-new .  1.0
svg This  1.0
This unreferenced  0.015873015873015872
unreferenced  1.0
unreferenced section  1.0
section requires  0.3333333333333333
requires citations  0.0625
to ensure  0.0013280212483399733
ensure  1.0
ensure verifiability  1.0
verifiability  1.0
verifiability .  1.0
, Battle  0.0005614823133071309
Battle  0.5
Battle Management  0.5
Management  1.0
Management command  1.0
command centres  0.5
centres  1.0
centres require  1.0
require rapid  0.045454545454545456
rapid  1.0
rapid access  1.0
to and  0.0013280212483399733
large ,  0.043478260869565216
, rapidly  0.0005614823133071309
rapidly changing  0.5
changing  1.0
changing information  1.0
information databases  0.021739130434782608
Commanders and  1.0
and system  0.001445086705202312
system operators  0.010752688172043012
operators  1.0
operators need  1.0
to query  0.0013280212483399733
query these  0.3333333333333333
these databases  0.023809523809523808
databases as  0.125
as conveniently  0.003484320557491289
conveniently  1.0
conveniently as  1.0
an eyes-busy  0.007575757575757576
eyes-busy  1.0
eyes-busy environment  1.0
environment where  0.16666666666666666
where much  0.02857142857142857
is presented  0.0040650406504065045
a display  0.001226993865030675
display  1.0
display format  0.5
format .  0.5
Human-machine interaction  1.0
interaction by  0.125
voice has  0.07692307692307693
the potential  0.0020761245674740486
potential to  0.2857142857142857
very useful  0.04878048780487805
these environments  0.023809523809523808
environments  1.0
environments .  1.0
of efforts  0.00089126559714795
been undertaken  0.014705882352941176
undertaken to  0.5
to interface  0.0013280212483399733
interface commercially  0.25
commercially  1.0
commercially available  1.0
available isolated-word  0.058823529411764705
isolated-word  1.0
isolated-word recognizers  1.0
recognizers into  0.5
into battle  0.01282051282051282
battle  1.0
battle management  1.0
management environments  0.14285714285714285
In one  0.009523809523809525
one feasibility  0.015384615384615385
feasibility study  0.5
study ,  0.25
recognition equipment  0.008264462809917356
equipment was  0.3333333333333333
was tested  0.012987012987012988
tested in  0.5
an integrated  0.007575757575757576
integrated information  0.3333333333333333
information display  0.021739130434782608
display for  0.5
for naval  0.0036101083032490976
naval  1.0
naval battle  0.3333333333333333
management applications  0.14285714285714285
Users were  1.0
very optimistic  0.024390243902439025
optimistic  1.0
optimistic about  1.0
although capabilities  0.16666666666666666
capabilities were  0.2
were limited  0.024390243902439025
Speech understanding  0.03225806451612903
understanding programs  0.030303030303030304
programs sponsored  0.09090909090909091
sponsored  1.0
sponsored by  0.5
the Defense  0.0006920415224913495
Defense  1.0
Defense Advanced  1.0
Advanced Research  0.2
Research Projects  0.125
Projects  1.0
Projects Agency  1.0
Agency -LRB-  0.5
-LRB- DARPA  0.0027100271002710027
DARPA  0.75
DARPA -RRB-  0.25
U.S. has  0.14285714285714285
speech interface  0.006578947368421052
interface .  0.25
recognition efforts  0.008264462809917356
continuous speech  0.5
-LRB- CSR  0.005420054200542005
CSR  1.0
CSR -RRB-  0.6666666666666666
, large-vocabulary  0.0005614823133071309
large-vocabulary  1.0
large-vocabulary speech  0.6666666666666666
speech designed  0.006578947368421052
be representative  0.004219409282700422
representative  1.0
representative of  1.0
the naval  0.0006920415224913495
naval resource  0.6666666666666666
resource management  0.4
management task  0.14285714285714285
Significant advances  1.0
advances  1.0
advances in  1.0
the state-of-the-art  0.0006920415224913495
state-of-the-art in  0.5
in CSR  0.0018726591760299626
CSR have  0.3333333333333333
and current  0.001445086705202312
current efforts  0.14285714285714285
efforts are  0.14285714285714285
are focused  0.004149377593360996
on integrating  0.0047169811320754715
integrating  1.0
integrating speech  1.0
processing to  0.018518518518518517
allow spoken  0.2
language interaction  0.006756756756756757
interaction with  0.125
a naval  0.001226993865030675
management system  0.14285714285714285
Training air  0.5
air  1.0
air traffic  0.6
traffic  1.0
traffic controllers  1.0
controllers  1.0
controllers Training  0.3333333333333333
Training  0.5
Training for  0.5
for air  0.0036101083032490976
controllers -LRB-  0.3333333333333333
-LRB- ATC  0.0027100271002710027
ATC  1.0
ATC -RRB-  0.2
-RRB- represents  0.0028169014084507044
represents an  0.25
an excellent  0.007575757575757576
excellent  1.0
excellent application  1.0
application for  0.07142857142857142
for speech  0.01444043321299639
Many ATC  0.08333333333333333
ATC training  0.4
training systems  0.03571428571428571
systems currently  0.008928571428571428
currently require  0.14285714285714285
person to  0.10526315789473684
to act  0.0026560424966799467
`` pseudo-pilot  0.005291005291005291
pseudo-pilot  1.0
pseudo-pilot ''  0.5
, engaging  0.0005614823133071309
engaging  1.0
engaging in  1.0
a voice  0.001226993865030675
voice dialog  0.07692307692307693
dialog  1.0
dialog with  0.5
the trainee  0.0006920415224913495
trainee  1.0
trainee controller  1.0
controller  1.0
controller ,  0.5
which simulates  0.007246376811594203
simulates  1.0
simulates the  1.0
the dialog  0.0006920415224913495
dialog that  0.5
the controller  0.001384083044982699
controller would  0.25
to conduct  0.0013280212483399733
conduct  1.0
conduct with  1.0
with pilots  0.00546448087431694
pilots in  0.5
real ATC  0.1111111111111111
ATC situation  0.2
situation .  0.5
and synthesis  0.001445086705202312
synthesis  1.0
synthesis techniques  1.0
techniques offer  0.043478260869565216
offer  1.0
offer the  1.0
eliminate the  0.5
as pseudo-pilot  0.003484320557491289
pseudo-pilot ,  0.5
thus reducing  0.1
reducing  1.0
reducing training  0.5
training and  0.03571428571428571
and support  0.001445086705202312
support personnel  0.25
personnel  1.0
personnel .  1.0
, Air  0.0005614823133071309
Air controller  0.3333333333333333
controller tasks  0.25
also characterized  0.014492753623188406
characterized by  0.5
by highly  0.005714285714285714
highly structured  0.1111111111111111
structured speech  0.16666666666666666
speech as  0.013157894736842105
primary output  0.5
, hence  0.0005614823133071309
hence reducing  0.5
reducing the  0.5
the difficulty  0.0006920415224913495
recognition task  0.01652892561983471
task should  0.023809523809523808
In practice  0.009523809523809525
practice ,  0.5
is rarely  0.0020325203252032522
rarely the  0.3333333333333333
The FAA  0.005208333333333333
FAA  1.0
FAA document  0.5
document 7110.65  0.027777777777777776
7110.65  1.0
7110.65 details  1.0
details the  0.5
the phrases  0.0006920415224913495
phrases that  0.0625
by air  0.011428571428571429
controllers .  0.3333333333333333
While this  0.2
this document  0.01098901098901099
document gives  0.027777777777777776
gives less  0.5
than 150  0.022222222222222223
150 examples  0.5
such phrases  0.008130081300813009
of phrases  0.00089126559714795
phrases supported  0.0625
supported  1.0
supported by  1.0
by one  0.005714285714285714
the simulation  0.0006920415224913495
simulation vendors  0.3333333333333333
vendors speech  0.25
of 500,000  0.00089126559714795
500,000  1.0
500,000 .  1.0
The USAF  0.005208333333333333
USAF  1.0
USAF ,  1.0
, USMC  0.0005614823133071309
USMC  1.0
USMC ,  1.0
, US  0.0011229646266142617
US Army  0.14285714285714285
Army ,  0.25
US Navy  0.14285714285714285
Navy  1.0
Navy ,  1.0
and FAA  0.001445086705202312
FAA as  0.5
of international  0.00089126559714795
international ATC  0.5
training organizations  0.03571428571428571
organizations  1.0
organizations such  1.0
Royal Australian  0.5
Australian Air  0.5
Force and  0.5
and Civil  0.001445086705202312
Civil  1.0
Civil Aviation  1.0
Aviation  1.0
Aviation Authorities  1.0
Authorities  1.0
Authorities in  1.0
in Italy  0.0018726591760299626
, Brazil  0.0005614823133071309
Brazil  1.0
Brazil ,  1.0
and Canada  0.001445086705202312
Canada are  0.16666666666666666
currently using  0.14285714285714285
using ATC  0.01694915254237288
ATC simulators  0.2
simulators  1.0
simulators with  1.0
with speech  0.01092896174863388
recognition from  0.01652892561983471
different vendors  0.02040816326530612
vendors .  0.25
Telephony and  1.0
other domains  0.014285714285714285
domains ASR  0.125
ASR in  0.5
of telephony  0.00089126559714795
telephony  1.0
telephony is  0.3333333333333333
now commonplace  0.07692307692307693
commonplace  1.0
commonplace and  1.0
computer gaming  0.022727272727272728
gaming  1.0
gaming and  1.0
and simulation  0.001445086705202312
simulation is  0.3333333333333333
is becoming  0.0020325203252032522
becoming  1.0
becoming more  1.0
more widespread  0.010526315789473684
widespread  1.0
widespread .  1.0
Despite the  1.0
of integration  0.00089126559714795
integration  1.0
integration with  1.0
with word  0.01639344262295082
word processing  0.016666666666666666
general personal  0.045454545454545456
personal computing  0.25
computing  1.0
computing .  0.5
, ASR  0.0005614823133071309
of document  0.00089126559714795
document production  0.027777777777777776
production has  0.3333333333333333
not seen  0.008928571428571428
seen the  0.1
expected -LRB-  0.14285714285714285
-LRB- by  0.005420054200542005
by whom  0.005714285714285714
whom ?  0.5
The improvement  0.005208333333333333
of mobile  0.00089126559714795
mobile  1.0
mobile processor  0.5
processor  1.0
processor speeds  1.0
speeds  1.0
speeds made  0.5
made feasible  0.0625
feasible  1.0
feasible the  0.5
the speech-enabled  0.0006920415224913495
speech-enabled  1.0
speech-enabled Symbian  1.0
Symbian  1.0
Symbian and  1.0
and Windows  0.001445086705202312
Windows  1.0
Windows Mobile  1.0
Mobile Smartphones  0.3333333333333333
Smartphones  1.0
Smartphones .  1.0
Speech is  0.06451612903225806
used mostly  0.008849557522123894
mostly as  0.5
of User  0.00089126559714795
User  0.5
User Interface  0.5
Interface  1.0
Interface ,  1.0
for creating  0.0036101083032490976
creating pre-defined  0.14285714285714285
pre-defined or  0.5
or custom  0.0045045045045045045
custom  1.0
custom speech  0.5
speech commands  0.006578947368421052
Leading software  1.0
software vendors  0.037037037037037035
vendors in  0.25
field are  0.037037037037037035
: Microsoft  0.00980392156862745
Microsoft  1.0
Microsoft Corporation  0.5
-LRB- Microsoft  0.0027100271002710027
Microsoft Voice  0.5
Voice Command  0.2
Command  1.0
Command -RRB-  0.5
, Digital  0.0005614823133071309
Digital  1.0
Digital Syphon  1.0
Syphon  1.0
Syphon -LRB-  1.0
-LRB- Sonic  0.0027100271002710027
Sonic  1.0
Sonic Extractor  1.0
Extractor  1.0
Extractor -RRB-  1.0
, Nuance  0.0005614823133071309
-LRB- Nuance  0.0027100271002710027
Nuance Voice  0.3333333333333333
Voice Control  0.2
Control  1.0
Control -RRB-  1.0
Speech Technology  0.03225806451612903
Technology Center  0.3333333333333333
Center  1.0
Center ,  1.0
, Vito  0.0005614823133071309
Vito  1.0
Vito Technology  1.0
Technology -LRB-  0.3333333333333333
-LRB- VITO  0.0027100271002710027
VITO  1.0
VITO Voice2Go  1.0
Voice2Go  1.0
Voice2Go -RRB-  1.0
, Speereo  0.0005614823133071309
Speereo  1.0
Speereo Software  0.5
Software -LRB-  0.5
-LRB- Speereo  0.0027100271002710027
Speereo Voice  0.5
Voice Translator  0.2
Translator  1.0
Translator -RRB-  1.0
, Verbyx  0.0005614823133071309
Verbyx  1.0
Verbyx VRX  1.0
VRX  1.0
VRX and  1.0
and SVOX  0.001445086705202312
SVOX  1.0
SVOX .  1.0
Further applications  0.3333333333333333
applications Aerospace  0.04
Aerospace -LRB-  0.5
e.g. space  0.017857142857142856
space exploration  0.2
exploration  1.0
exploration ,  1.0
, spacecraft  0.0005614823133071309
spacecraft  1.0
spacecraft ,  1.0
-RRB- NASA  0.0028169014084507044
NASA  1.0
NASA 's  1.0
's Mars  0.0196078431372549
Mars  1.0
Mars Polar  0.5
Polar  1.0
Polar Lander  1.0
Lander  1.0
Lander used  0.5
used speech  0.008849557522123894
from technology  0.009615384615384616
technology Sensory  0.045454545454545456
Sensory  1.0
Sensory ,  1.0
Inc. in  0.5
the Mars  0.0006920415224913495
Mars Microphone  0.5
Microphone  1.0
Microphone on  1.0
the Lander  0.0006920415224913495
Lander Automatic  0.5
Automatic  0.2222222222222222
Automatic translation  0.1111111111111111
translation Automotive  0.013513513513513514
Automotive  1.0
Automotive speech  1.0
, OnStar  0.0005614823133071309
OnStar  1.0
OnStar ,  1.0
, Ford  0.0005614823133071309
Ford  1.0
Ford Sync  1.0
Sync  1.0
Sync -RRB-  1.0
-RRB- Court  0.0028169014084507044
Court  1.0
Court reporting  1.0
reporting -LRB-  0.3333333333333333
-LRB- Realtime  0.0027100271002710027
Realtime  1.0
Realtime Speech  1.0
Speech Writing  0.03225806451612903
Writing  1.0
Writing -RRB-  1.0
-RRB- Hands-free  0.0028169014084507044
Hands-free  1.0
Hands-free computing  1.0
computing :  0.5
: Speech  0.00980392156862745
computer user  0.022727272727272728
user interface  0.07142857142857142
interface Home  0.25
Home  1.0
Home automation  1.0
automation  1.0
automation Interactive  1.0
Interactive  0.5
Interactive voice  0.5
voice response  0.07692307692307693
response Mobile  0.5
Mobile telephony  0.3333333333333333
telephony ,  0.6666666666666666
including mobile  0.07142857142857142
mobile email  0.5
email Multimodal  0.5
Multimodal  1.0
Multimodal interaction  1.0
interaction Pronunciation  0.125
Pronunciation  1.0
Pronunciation evaluation  1.0
in computer-aided  0.0018726591760299626
computer-aided language  0.3333333333333333
language learning  0.006756756756756757
learning applications  0.023255813953488372
applications Robotics  0.04
Robotics  1.0
Robotics Speech-to-text  1.0
Speech-to-text  1.0
Speech-to-text reporter  1.0
reporter  1.0
reporter -LRB-  1.0
-LRB- transcription  0.0027100271002710027
transcription of  0.5
speech into  0.013157894736842105
video captioning  0.2
captioning  1.0
captioning ,  1.0
, Court  0.0005614823133071309
reporting -RRB-  0.3333333333333333
-RRB- Telematics  0.0028169014084507044
Telematics  1.0
Telematics -LRB-  1.0
, vehicle  0.0005614823133071309
vehicle  1.0
vehicle Navigation  1.0
Navigation  1.0
Navigation Systems  1.0
Systems -RRB-  0.08333333333333333
-RRB- Transcription  0.0028169014084507044
Transcription  1.0
Transcription -LRB-  1.0
-LRB- digital  0.0027100271002710027
digital speech-to-text  0.14285714285714285
speech-to-text -RRB-  0.5
-RRB- Video  0.0028169014084507044
Video  1.0
Video games  1.0
games  1.0
games ,  1.0
with Tom  0.00546448087431694
Tom  1.0
Tom Clancy  1.0
Clancy  1.0
Clancy 's  1.0
's EndWar  0.0196078431372549
EndWar  1.0
EndWar and  1.0
and Lifeline  0.001445086705202312
Lifeline  1.0
Lifeline as  1.0
as working  0.003484320557491289
working examples  0.14285714285714285
examples Performance  0.041666666666666664
Performance  1.0
Performance The  1.0
The performance  0.005208333333333333
usually evaluated  0.03125
evaluated in  0.14285714285714285
of accuracy  0.0017825311942959
accuracy and  0.03225806451612903
and speed  0.002890173410404624
speed .  0.42857142857142855
Accuracy is  0.14285714285714285
usually rated  0.03125
rated  1.0
rated with  1.0
word error  0.016666666666666666
rate -LRB-  0.09090909090909091
-LRB- WER  0.0027100271002710027
WER  1.0
WER -RRB-  1.0
whereas speed  0.3333333333333333
speed is  0.14285714285714285
measured with  0.16666666666666666
real time  0.1111111111111111
time factor  0.030303030303030304
factor .  0.5
Other measures  0.14285714285714285
measures of  0.16666666666666666
accuracy include  0.03225806451612903
include Single  0.037037037037037035
Single  1.0
Single Word  1.0
Word Error  0.14285714285714285
Error  1.0
Error Rate  0.5
Rate  1.0
Rate -LRB-  1.0
-LRB- SWER  0.0027100271002710027
SWER  1.0
SWER -RRB-  1.0
and Command  0.001445086705202312
Command Success  0.5
Success  1.0
Success Rate  1.0
machine -RRB-  0.012658227848101266
very complex  0.024390243902439025
complex problem  0.041666666666666664
Vocalizations vary  1.0
vary in  0.5
of accent  0.00089126559714795
accent  1.0
accent ,  1.0
, pronunciation  0.0005614823133071309
pronunciation  1.0
pronunciation ,  1.0
, articulation  0.0005614823133071309
articulation  1.0
articulation ,  1.0
, roughness  0.0005614823133071309
roughness  1.0
roughness ,  1.0
, nasality  0.0005614823133071309
nasality  1.0
nasality ,  1.0
, pitch  0.0005614823133071309
pitch  1.0
pitch ,  1.0
, volume  0.0005614823133071309
volume ,  0.25
is distorted  0.0040650406504065045
distorted  1.0
distorted by  0.5
a background  0.001226993865030675
background noise  0.3333333333333333
noise and  0.125
and echoes  0.001445086705202312
echoes  1.0
echoes ,  1.0
, electrical  0.0005614823133071309
electrical  1.0
electrical characteristics  1.0
characteristics .  0.5
recognition vary  0.008264462809917356
vary with  0.16666666666666666
following :  0.13333333333333333
: Vocabulary  0.00980392156862745
Vocabulary  0.6666666666666666
Vocabulary size  0.3333333333333333
and confusability  0.001445086705202312
confusability  1.0
confusability Speaker  1.0
Speaker dependence  0.16666666666666666
dependence  1.0
dependence vs.  1.0
vs. independence  0.08333333333333333
independence  1.0
independence Isolated  1.0
Isolated  0.5
Isolated ,  1.0
, discontinuous  0.0005614823133071309
discontinuous  1.0
discontinuous ,  0.3333333333333333
or continuous  0.0045045045045045045
speech Task  0.006578947368421052
Task and  0.6666666666666666
and language  0.004335260115606936
language constraints  0.006756756756756757
constraints  1.0
constraints Read  0.25
Read  0.5
Read vs.  1.0
vs. spontaneous  0.08333333333333333
speech Adverse  0.006578947368421052
Adverse  0.5
Adverse conditions  1.0
conditions Accuracy  0.2
recognition As  0.008264462809917356
earlier in  0.25
article accuracy  0.034482758620689655
speech recogniton  0.006578947368421052
recogniton  1.0
recogniton vary  0.5
in following  0.0018726591760299626
: Error  0.00980392156862745
Error Rates  0.5
Rates  1.0
Rates Increase  1.0
Increase  1.0
Increase as  1.0
the Vocabulary  0.0006920415224913495
Vocabulary Size  0.3333333333333333
Size  1.0
Size Grows  1.0
Grows  1.0
Grows :  1.0
: e.g.  0.0196078431372549
e.g. The  0.03571428571428571
The 10  0.005208333333333333
10 digits  0.125
digits  1.0
digits ``  1.0
`` zero  0.005291005291005291
zero  1.0
zero ''  1.0
`` nine  0.005291005291005291
nine  1.0
nine ''  1.0
be recognized  0.008438818565400843
recognized essentially  0.16666666666666666
essentially perfectly  0.125
perfectly  1.0
perfectly ,  1.0
but vocabulary  0.014705882352941176
vocabulary sizes  0.125
of 200  0.00089126559714795
200 ,  0.5
, 5000  0.0005614823133071309
5000  1.0
5000 or  1.0
or 100000  0.0045045045045045045
100000  1.0
100000 may  1.0
have error  0.009615384615384616
error rates  0.25
of 3  0.00089126559714795
3 %  0.2
, 7  0.0005614823133071309
7 %  0.14285714285714285
% or  0.02564102564102564
or 45  0.0045045045045045045
45  1.0
45 %  1.0
Vocabulary is  0.3333333333333333
is Hard  0.0020325203252032522
Hard  1.0
Hard to  0.5
to Recognize  0.0013280212483399733
Recognize  1.0
Recognize if  1.0
it Contains  0.008547008547008548
Contains  1.0
Contains Confusable  1.0
Confusable  1.0
Confusable Words  1.0
Words  0.5
Words :  0.25
The 26  0.005208333333333333
26  1.0
26 letters  1.0
alphabet are  0.3333333333333333
are difficult  0.004149377593360996
discriminate because  0.3333333333333333
are confusable  0.004149377593360996
confusable  1.0
confusable words  1.0
most notoriously  0.017241379310344827
notoriously  1.0
notoriously ,  1.0
the E-set  0.0006920415224913495
E-set  1.0
E-set :  1.0
`` B  0.005291005291005291
B  1.0
B ,  1.0
, C  0.0005614823133071309
C  1.0
C ,  1.0
, D  0.0005614823133071309
D  1.0
D ,  1.0
, E  0.0005614823133071309
E  1.0
E ,  1.0
, G  0.0005614823133071309
G  1.0
G ,  1.0
, P  0.0005614823133071309
P ,  0.5
, T  0.0005614823133071309
T ,  0.16666666666666666
, V  0.0005614823133071309
V  1.0
V ,  1.0
, Z  0.0005614823133071309
Z  1.0
Z ''  1.0
; An  0.02127659574468085
An 8  0.0625
8  1.0
8 %  1.0
% error  0.02564102564102564
rate is  0.18181818181818182
considered good  0.1111111111111111
good for  0.07692307692307693
this vocabulary  0.01098901098901099
vocabulary .  0.25
Speaker Dependence  0.16666666666666666
Dependence  1.0
Dependence vs.  1.0
vs. Independence  0.08333333333333333
Independence  1.0
Independence :  1.0
: A  0.00980392156862745
A speaker  0.04
speaker dependent  0.05555555555555555
dependent system  0.3333333333333333
is intended  0.0040650406504065045
intended for  0.4
for use  0.007220216606498195
use by  0.027777777777777776
single speaker  0.07142857142857142
speaker independent  0.05555555555555555
independent system  0.5
by any  0.005714285714285714
any speaker  0.03225806451612903
speaker ,  0.05555555555555555
, Discontinuous  0.0005614823133071309
Discontinuous  1.0
Discontinuous or  1.0
or Continuous  0.0045045045045045045
Continuous  1.0
Continuous speech  1.0
speech With  0.006578947368421052
With isolated  0.14285714285714285
isolated speech  0.4
speech single  0.006578947368421052
single words  0.07142857142857142
, therefore  0.0016844469399213925
therefore it  0.6
becomes easier  0.5
easier to  0.25
to recognize  0.00796812749003984
recognize the  0.4444444444444444
With discontinuous  0.14285714285714285
discontinuous speech  0.6666666666666666
speech full  0.006578947368421052
full sentenced  0.2
sentenced  1.0
sentenced separated  1.0
by silence  0.005714285714285714
silence  1.0
silence are  1.0
with isolated  0.00546448087431694
With continuous  0.14285714285714285
speech naturally  0.006578947368421052
naturally spoken  0.5
spoken sentences  0.07142857142857142
becomes harder  0.25
from both  0.009615384615384616
both isloated  0.03225806451612903
isloated  1.0
isloated and  1.0
and discontinuous  0.001445086705202312
and Language  0.005780346820809248
Language Constraints  0.08333333333333333
Constraints  0.6666666666666666
Constraints e.g.  0.3333333333333333
e.g. Querying  0.017857142857142856
Querying  1.0
Querying application  1.0
application may  0.07142857142857142
may dismiss  0.019230769230769232
dismiss  1.0
dismiss the  1.0
the hypothesis  0.0006920415224913495
hypothesis  1.0
hypothesis ``  1.0
The apple  0.010416666666666666
apple  1.0
apple is  0.6666666666666666
is red  0.0020325203252032522
red  1.0
red .  1.0
e.g. Constraints  0.017857142857142856
Constraints may  0.3333333333333333
be semantic  0.004219409282700422
semantic ;  0.047619047619047616
; rejecting  0.0425531914893617
rejecting ``  0.6666666666666666
is angry  0.0020325203252032522
angry .  0.5
e.g. Syntactic  0.017857142857142856
Syntactic  1.0
Syntactic ;  1.0
`` Red  0.005291005291005291
Red  1.0
Red is  1.0
is apple  0.0020325203252032522
apple the  0.3333333333333333
the .  0.0006920415224913495
Constraints are  0.3333333333333333
often represented  0.022727272727272728
represented by  0.3333333333333333
vs. Spontaneous  0.08333333333333333
Spontaneous  1.0
Spontaneous Speech  1.0
Speech When  0.03225806451612903
When  0.14285714285714285
When a  0.14285714285714285
person reads  0.05263157894736842
reads it  0.5
it 's  0.008547008547008548
's usually  0.0196078431372549
context that  0.030303030303030304
been previously  0.014705882352941176
previously prepared  0.5
prepared  1.0
prepared ,  1.0
but when  0.014705882352941176
person uses  0.05263157894736842
uses spontaneous  0.07142857142857142
the disfluences  0.0006920415224913495
disfluences  1.0
disfluences -LRB-  1.0
-LRB- like  0.0027100271002710027
`` uh  0.005291005291005291
uh  1.0
uh ''  1.0
`` um  0.005291005291005291
um  1.0
um ''  1.0
, false  0.0005614823133071309
false starts  0.5
starts  1.0
starts ,  0.5
, incomplete  0.0005614823133071309
incomplete  1.0
incomplete sentences  1.0
, stutering  0.0005614823133071309
stutering  1.0
stutering ,  1.0
, coughing  0.0005614823133071309
coughing  1.0
coughing ,  1.0
and laughter  0.001445086705202312
laughter  1.0
laughter -RRB-  1.0
and limited  0.001445086705202312
limited vocabulary  0.1
conditions Environmental  0.2
Environmental  1.0
Environmental noise  1.0
noise -LRB-  0.125
e.g. Noise  0.017857142857142856
Noise  1.0
Noise in  1.0
a car  0.001226993865030675
car  1.0
car or  1.0
a factory  0.001226993865030675
factory  1.0
factory -RRB-  1.0
-RRB- Acoustical  0.0028169014084507044
Acoustical  0.5
Acoustical distortions  0.5
distortions  1.0
distortions -LRB-  1.0
e.g. echoes  0.017857142857142856
, room  0.0005614823133071309
room  1.0
room acoustics  1.0
acoustics  1.0
acoustics -RRB-  1.0
-RRB- Speech  0.0028169014084507044
a multileveled  0.001226993865030675
multileveled  1.0
multileveled pattern  1.0
Acoustical signals  0.5
signals  1.0
signals are  1.0
are structured  0.004149377593360996
structured into  0.16666666666666666
a hierarchy  0.001226993865030675
hierarchy  1.0
hierarchy of  1.0
of units  0.00089126559714795
units ;  0.14285714285714285
; e.g.  0.0425531914893617
e.g. Phonemes  0.017857142857142856
Phonemes  1.0
Phonemes ,  1.0
, Words  0.0005614823133071309
Words ,  0.25
, Phrases  0.0005614823133071309
Phrases  1.0
Phrases ,  1.0
and Sentences  0.001445086705202312
Sentences  1.0
Sentences ;  1.0
; Each  0.02127659574468085
Each  0.16666666666666666
Each level  0.16666666666666666
level provides  0.05
provides additional  0.5
additional constraints  0.16666666666666666
constraints ;  0.25
e.g. Known  0.017857142857142856
Known  1.0
Known word  1.0
word pronunciations  0.016666666666666666
pronunciations  1.0
pronunciations or  1.0
or legal  0.0045045045045045045
legal word  0.3333333333333333
word sequences  0.016666666666666666
sequences ,  0.1111111111111111
can compensate  0.0055248618784530384
compensate  1.0
compensate for  1.0
for errors  0.0036101083032490976
errors or  0.2
or uncertainties  0.0045045045045045045
uncertainties  1.0
uncertainties at  1.0
at lower  0.014705882352941176
lower level  0.4
level ;  0.1
; This  0.02127659574468085
This hierarchy  0.015873015873015872
of constraints  0.00089126559714795
constraints are  0.25
are exploited  0.004149377593360996
exploited  1.0
exploited ;  1.0
; By  0.02127659574468085
By combining  0.3333333333333333
combining decisions  0.25
decisions probabilistically  0.1
probabilistically  1.0
probabilistically at  1.0
all lower  0.023255813953488372
lower levels  0.4
levels ,  0.045454545454545456
making more  0.14285714285714285
more deterministic  0.021052631578947368
deterministic decisions  0.25
decisions only  0.1
only at  0.02631578947368421
the highest  0.0006920415224913495
highest level  0.3333333333333333
; Speech  0.02127659574468085
Speech recogniton  0.03225806451612903
recogniton by  0.5
machine is  0.012658227848101266
process broken  0.027777777777777776
broken into  0.6
into several  0.01282051282051282
several phases  0.045454545454545456
phases  1.0
phases .  1.0
Computationally ,  1.0
which a  0.014492753623188406
sound pattern  0.05
pattern has  0.16666666666666666
recognized or  0.16666666666666666
or classified  0.0045045045045045045
classified  1.0
classified into  1.0
a category  0.001226993865030675
category that  0.5
that represents  0.0035460992907801418
a meaning  0.00245398773006135
meaning to  0.08695652173913043
Every acoustic  1.0
acoustic signal  0.16666666666666666
signal can  0.3333333333333333
be broken  0.004219409282700422
broken in  0.2
in smaller  0.0018726591760299626
smaller more  0.14285714285714285
more basic  0.021052631578947368
basic sub-signals  0.07692307692307693
sub-signals  1.0
sub-signals .  1.0
As the  0.05555555555555555
complex sound  0.125
sound signal  0.05
signal is  0.16666666666666666
is broken  0.0020325203252032522
the smaller  0.0006920415224913495
smaller sub-sounds  0.14285714285714285
sub-sounds  1.0
sub-sounds ,  1.0
different levels  0.02040816326530612
levels are  0.045454545454545456
where at  0.02857142857142857
top level  0.2
level we  0.05
have complex  0.009615384615384616
complex sounds  0.041666666666666664
sounds ,  0.13333333333333333
simpler sounds  0.6666666666666666
sounds on  0.06666666666666667
on lower  0.0047169811320754715
and going  0.001445086705202312
going to  0.25
to lower  0.0013280212483399733
levels even  0.045454545454545456
even more  0.037037037037037035
more ,  0.010526315789473684
create more  0.058823529411764705
basic and  0.07692307692307693
and shorter  0.001445086705202312
shorter and  0.5
and simpler  0.001445086705202312
sounds .  0.06666666666666667
The lowest  0.005208333333333333
lowest  1.0
lowest level  1.0
sounds are  0.13333333333333333
most fundamental  0.017241379310344827
fundamental ,  0.5
would check  0.018867924528301886
check for  0.5
for simple  0.0036101083032490976
simple and  0.07692307692307693
more probabilistic  0.010526315789473684
probabilistic rules  0.14285714285714285
what sound  0.03125
sound should  0.1
should represent  0.10526315789473684
represent .  0.2222222222222222
Once these  0.2
these sounds  0.023809523809523808
are put  0.004149377593360996
put together  0.25
together into  0.125
sound on  0.05
on upper  0.0047169811320754715
upper  1.0
upper level  1.0
new set  0.041666666666666664
deterministic rules  0.25
rules should  0.023255813953488372
should predict  0.05263157894736842
predict what  0.16666666666666666
what new  0.03125
new complex  0.041666666666666664
most upper  0.017241379310344827
deterministic rule  0.25
rule should  0.3333333333333333
should figure  0.05263157894736842
figure  1.0
figure out  0.5
of complex  0.00089126559714795
complex expressions  0.041666666666666664
to expand  0.0013280212483399733
expand  1.0
expand our  1.0
our knowledge  0.2
about speech  0.05
recognition we  0.008264462809917356
a consideration  0.001226993865030675
consideration neural  0.3333333333333333
are four  0.004149377593360996
four steps  0.14285714285714285
steps of  0.5
of neural  0.00089126559714795
network approaches  0.3333333333333333
: Digitize  0.00980392156862745
Digitize  1.0
Digitize the  1.0
speech that  0.006578947368421052
recognize For  0.1111111111111111
For telephone  0.01639344262295082
telephone  1.0
telephone speech  0.5
speech the  0.006578947368421052
the sampling  0.0006920415224913495
sampling  1.0
sampling rate  1.0
is 8000  0.0020325203252032522
8000  1.0
8000 samples  1.0
samples per  0.5
per second  0.5
second ;  0.1
; Compute  0.02127659574468085
Compute  1.0
Compute features  1.0
of spectral-domain  0.00089126559714795
spectral-domain  1.0
spectral-domain of  1.0
with Fourier  0.00546448087431694
Fourier  1.0
Fourier transform  0.6666666666666666
transform  1.0
transform -RRB-  0.2
; Computed  0.02127659574468085
Computed  1.0
Computed every  1.0
every 10msec  0.3333333333333333
10msec  1.0
10msec ,  0.5
with one  0.00546448087431694
one 10msec  0.015384615384615385
10msec section  0.5
section called  0.16666666666666666
called a  0.05555555555555555
a frame  0.001226993865030675
frame  1.0
frame ;  0.5
; Analysis  0.02127659574468085
Analysis of  0.2
of four  0.00089126559714795
four step  0.14285714285714285
step neural  0.06666666666666667
approaches can  0.03571428571428571
be explained  0.004219409282700422
explained  1.0
explained by  1.0
by further  0.005714285714285714
further information  0.125
Sound is  0.3333333333333333
is produced  0.0040650406504065045
air -LRB-  0.2
other medium  0.014285714285714285
medium -RRB-  0.3333333333333333
-RRB- vibration  0.0028169014084507044
vibration  1.0
vibration ,  1.0
which we  0.007246376811594203
we register  0.022222222222222223
register  1.0
register by  1.0
by ears  0.005714285714285714
ears  1.0
ears ,  1.0
but machines  0.014705882352941176
machines by  0.25
by receivers  0.005714285714285714
receivers  1.0
receivers .  1.0
Basic sound  1.0
sound creates  0.05
creates  1.0
creates a  0.5
a wave  0.0049079754601227
wave  1.0
wave which  0.1111111111111111
has 2  0.011904761904761904
2 descriptions  0.2
descriptions  1.0
descriptions ;  1.0
; Amplitude  0.02127659574468085
Amplitude  1.0
Amplitude -LRB-  1.0
how strong  0.034482758620689655
strong is  0.25
is it  0.0020325203252032522
it -RRB-  0.008547008547008548
and frequency  0.001445086705202312
frequency -LRB-  0.5
how often  0.034482758620689655
often it  0.022727272727272728
it vibrates  0.008547008547008548
vibrates  1.0
vibrates per  1.0
second -RRB-  0.1
Digitized Sound  1.0
Sound  0.3333333333333333
Sound Graph  0.3333333333333333
Graph  1.0
Graph This  1.0
the wave  0.0006920415224913495
wave in  0.1111111111111111
the water  0.0006920415224913495
water  1.0
water .  1.0
Big wave  1.0
wave is  0.2222222222222222
is strong  0.0020325203252032522
strong and  0.25
and smaller  0.001445086705202312
smaller ones  0.14285714285714285
ones are  0.1
usually faster  0.03125
faster but  0.3333333333333333
but weaker  0.014705882352941176
weaker  1.0
weaker .  1.0
how air  0.034482758620689655
air is  0.2
distorted ,  0.5
but we  0.014705882352941176
we do  0.022222222222222223
n't see  0.25
see it  0.05
it easily  0.008547008547008548
easily ,  0.1111111111111111
order for  0.07142857142857142
for sound  0.0036101083032490976
sound to  0.05
to travel  0.0013280212483399733
travel  1.0
travel .  1.0
These waves  0.058823529411764705
waves  1.0
waves can  0.14285714285714285
be digitalized  0.004219409282700422
digitalized  1.0
digitalized :  1.0
: Sample  0.00980392156862745
Sample  1.0
Sample a  1.0
a strength  0.001226993865030675
strength at  0.2
at short  0.014705882352941176
short intervals  0.125
intervals  1.0
intervals like  1.0
like in  0.03571428571428571
in picture  0.0018726591760299626
picture above  0.25
above to  0.07692307692307693
get bunch  0.14285714285714285
bunch  1.0
bunch of  1.0
of numbers  0.0017825311942959
numbers that  0.14285714285714285
that approximate  0.0035460992907801418
approximate at  0.5
at each  0.014705882352941176
each time  0.022222222222222223
time step  0.030303030303030304
step the  0.06666666666666667
wave .  0.2222222222222222
Collection of  1.0
numbers represent  0.14285714285714285
represent analog  0.1111111111111111
analog wave  0.5
This new  0.015873015873015872
new wave  0.041666666666666664
is digital  0.0020325203252032522
digital .  0.14285714285714285
Sound waves  0.3333333333333333
waves are  0.14285714285714285
are complicated  0.004149377593360996
complicated because  0.3333333333333333
they superimpose  0.025
superimpose  1.0
superimpose one  1.0
one on  0.015384615384615385
on top  0.0047169811320754715
top of  0.2
other .  0.02857142857142857
Like the  0.5
the waves  0.0006920415224913495
waves would  0.14285714285714285
would .  0.018867924528301886
This way  0.015873015873015872
way they  0.041666666666666664
they create  0.025
create odd  0.058823529411764705
odd  1.0
odd looking  1.0
looking waves  0.2
waves .  0.14285714285714285
if there  0.07142857142857142
are two  0.008298755186721992
two waves  0.034482758620689655
waves that  0.14285714285714285
that interact  0.0035460992907801418
interact  1.0
interact with  1.0
with each  0.00546448087431694
other we  0.014285714285714285
can add  0.0055248618784530384
add  1.0
add them  1.0
them which  0.05263157894736842
which creates  0.007246376811594203
creates new  0.5
new odd  0.041666666666666664
looking wave  0.2
wave as  0.1111111111111111
is shown  0.0020325203252032522
picture on  0.25
Neural Network  0.25
Network  1.0
Network classifies  1.0
classifies  1.0
classifies features  1.0
features into  0.038461538461538464
into phonetic-based  0.01282051282051282
phonetic-based  1.0
phonetic-based categories  1.0
categories ;  0.1111111111111111
; Given  0.02127659574468085
Given basic  0.07142857142857142
basic sound  0.15384615384615385
sound blocks  0.05
blocks ,  0.25
machine digitized  0.012658227848101266
digitized  1.0
digitized ,  1.0
a bunch  0.001226993865030675
numbers which  0.14285714285714285
which describe  0.007246376811594203
describe a  0.16666666666666666
wave and  0.1111111111111111
and waves  0.001445086705202312
waves describe  0.14285714285714285
describe words  0.16666666666666666
Each frame  0.16666666666666666
frame has  0.5
a unit  0.001226993865030675
unit block  0.3333333333333333
block  1.0
block of  1.0
of sound  0.0017825311942959
sound ,  0.1
are broken  0.004149377593360996
into basic  0.01282051282051282
sound waves  0.05
waves and  0.14285714285714285
and represented  0.001445086705202312
by numbers  0.005714285714285714
numbers after  0.14285714285714285
after Fourier  0.08333333333333333
Fourier Transform  0.3333333333333333
Transform  1.0
Transform ,  1.0
be statistically  0.004219409282700422
statistically  1.0
statistically evaluated  1.0
evaluated to  0.2857142857142857
set to  0.02564102564102564
which class  0.007246376811594203
of sounds  0.00089126559714795
sounds it  0.06666666666666667
it belongs  0.008547008547008548
belongs  1.0
belongs to  1.0
The nodes  0.005208333333333333
nodes  1.0
nodes in  0.14285714285714285
the figure  0.0006920415224913495
figure on  0.5
a slide  0.001226993865030675
slide  1.0
slide represent  1.0
a feature  0.00245398773006135
sound in  0.05
wave from  0.1111111111111111
from first  0.009615384615384616
first layer  0.030303030303030304
layer  1.0
layer of  1.0
of nodes  0.0035650623885918
nodes to  0.14285714285714285
second layer  0.2
nodes based  0.14285714285714285
some statistical  0.012048192771084338
statistical analysis  0.030303030303030304
This analysis  0.015873015873015872
analysis depends  0.015384615384615385
on programer  0.0047169811320754715
programer  1.0
programer 's  1.0
's instructions  0.0196078431372549
instructions  1.0
instructions .  1.0
At this  0.3333333333333333
this point  0.01098901098901099
nodes represents  0.14285714285714285
higher level  0.14285714285714285
level features  0.05
sound input  0.05
input which  0.024390243902439025
is again  0.0020325203252032522
again  1.0
again statistically  1.0
see what  0.05
what class  0.03125
class they  0.25
they belong  0.025
belong  1.0
belong to  1.0
Last level  1.0
nodes should  0.14285714285714285
be output  0.004219409282700422
output nodes  0.07692307692307693
nodes that  0.14285714285714285
that tell  0.0035460992907801418
tell us  0.3333333333333333
us with  0.5
high probability  0.05555555555555555
probability what  0.14285714285714285
what original  0.03125
original sound  0.07692307692307693
sound really  0.05
really  1.0
really was  1.0
was .  0.012987012987012988
Search to  0.5
to match  0.0026560424966799467
the neural-network  0.0006920415224913495
neural-network  1.0
neural-network output  1.0
output scores  0.038461538461538464
scores for  0.2
best word  0.05555555555555555
word ,  0.03333333333333333
was most  0.012987012987012988
likely uttered  0.0625
uttered  1.0
uttered ;  0.3333333333333333
; A  0.02127659574468085
A machine  0.02
machine speech  0.012658227848101266
recognition using  0.008264462809917356
using neural  0.01694915254237288
network is  0.16666666666666666
still just  0.06666666666666667
a fancy  0.001226993865030675
fancy  1.0
fancy statistics  1.0
Artificial neural  0.5
network has  0.16666666666666666
has specialized  0.011904761904761904
specialized  1.0
specialized output  0.5
nodes for  0.14285714285714285
for results  0.0036101083032490976
, unlike  0.0005614823133071309
unlike  1.0
unlike brain  1.0
brain  1.0
brain .  0.3333333333333333
Our brain  0.6666666666666666
brain recognizes  0.3333333333333333
in fundamentally  0.0018726591760299626
fundamentally  1.0
fundamentally different  1.0
different way  0.02040816326530612
way .  0.041666666666666664
brain is  0.3333333333333333
is entirely  0.0020325203252032522
entirely committed  0.5
committed  1.0
committed into  1.0
the perception  0.0006920415224913495
perception  1.0
perception of  0.5
sound .  0.05
When we  0.14285714285714285
we hear  0.022222222222222223
hear  1.0
hear sound  0.5
, our  0.0005614823133071309
our life  0.2
life experience  0.25
experience  1.0
experience is  0.5
is brought  0.0020325203252032522
brought  1.0
brought together  1.0
together to  0.125
to action  0.0013280212483399733
action of  0.2
of listening  0.00089126559714795
listening  1.0
listening to  1.0
sound into  0.05
a appropriate  0.001226993865030675
appropriate perspective  0.25
perspective so  0.25
is meaningful  0.0020325203252032522
meaningful .  0.125
Brain has  1.0
a purpose  0.001226993865030675
purpose when  0.2
when it  0.02857142857142857
it listens  0.008547008547008548
listens  1.0
listens for  1.0
sound that  0.05
is steered  0.0020325203252032522
steered  1.0
steered toward  1.0
toward  1.0
toward actions  1.0
In 1982  0.009523809523809525
1982 ,  0.3333333333333333
Kurzweil Applied  0.14285714285714285
Applied Intelligence  0.5
Intelligence and  0.3333333333333333
and Dragon  0.001445086705202312
Dragon  0.5
Dragon Systems  1.0
Systems released  0.16666666666666666
released  1.0
released speech  0.5
recognition products  0.008264462809917356
products .  0.25
By 1985  0.3333333333333333
1985  1.0
1985 ,  1.0
Kurzweil 's  0.14285714285714285
's software  0.0196078431372549
software had  0.037037037037037035
a vocabulary  0.001226993865030675
vocabulary of  0.125
of 1,000  0.00089126559714795
1,000 words  0.5
words --  0.009174311926605505
if uttered  0.03571428571428571
uttered one  0.3333333333333333
one word  0.03076923076923077
word at  0.016666666666666666
, its  0.0005614823133071309
its lexicon  0.02857142857142857
lexicon reached  0.1111111111111111
reached 20,000  0.5
20,000  1.0
20,000 words  1.0
entering the  0.5
the realm  0.0006920415224913495
realm  1.0
realm of  1.0
human vocabularies  0.021739130434782608
which range  0.007246376811594203
from 10,000  0.009615384615384616
10,000  1.0
10,000 to  1.0
to 150,000  0.0013280212483399733
150,000  1.0
150,000 words  1.0
But recognition  0.16666666666666666
accuracy was  0.03225806451612903
was only  0.012987012987012988
only 10  0.02631578947368421
% in  0.02564102564102564
in 1993  0.0018726591760299626
1993  1.0
1993 .  0.3333333333333333
the error  0.0006920415224913495
rate crossed  0.09090909090909091
crossed  1.0
crossed below  1.0
below 50  0.2
50 %  0.6666666666666666
released ``  0.5
`` Naturally  0.005291005291005291
Naturally  1.0
Naturally Speaking  1.0
Speaking  1.0
Speaking ''  1.0
1997 ,  0.5
which recognized  0.007246376811594203
recognized normal  0.16666666666666666
normal  1.0
normal human  0.5
human speech  0.021739130434782608
Progress mainly  1.0
mainly came  0.16666666666666666
came from  0.5
from improved  0.009615384615384616
improved computer  0.25
computer performance  0.022727272727272728
performance and  0.1111111111111111
larger source  0.0625
text databases  0.006289308176100629
major database  0.08333333333333333
database available  0.1
containing several  0.125
several million  0.045454545454545456
million words  0.3333333333333333
In 2006  0.009523809523809525
2006 ,  0.3333333333333333
, Google  0.0005614823133071309
Google published  0.25
published a  0.2857142857142857
a trillion-word  0.001226993865030675
trillion-word  1.0
trillion-word corpus  1.0
while Carnegie  0.05
Carnegie  1.0
Carnegie Mellon  1.0
Mellon  1.0
Mellon University  1.0
University researchers  0.1111111111111111
researchers found  0.1
found no  0.07142857142857142
no significant  0.07692307692307693
significant increase  0.1111111111111111
Algorithms Both  0.5
Both  0.3333333333333333
Both acoustic  0.3333333333333333
acoustic modeling  0.3333333333333333
modeling and  0.14285714285714285
language modeling  0.006756756756756757
modeling are  0.14285714285714285
are important  0.004149377593360996
important parts  0.0625
of modern  0.00089126559714795
modern statistically-based  0.2
statistically-based  1.0
statistically-based speech  1.0
recognition algorithms  0.008264462809917356
are widely  0.004149377593360996
many systems  0.019230769230769232
Language modeling  0.08333333333333333
modeling has  0.14285714285714285
other applications  0.014285714285714285
applications such  0.04
as smart  0.003484320557491289
smart  1.0
smart keyboard  1.0
keyboard and  0.3333333333333333
document classification  0.05555555555555555
classification .  0.11764705882352941
models Main  0.038461538461538464
: Hidden  0.00980392156862745
model Modern  0.03333333333333333
Modern general-purpose  0.3333333333333333
general-purpose  1.0
general-purpose speech  1.0
on Hidden  0.0047169811320754715
Models .  0.3333333333333333
are statistical  0.004149377593360996
that output  0.0035460992907801418
output a  0.07692307692307693
of symbols  0.00089126559714795
symbols or  0.3333333333333333
or quantities  0.0045045045045045045
quantities  1.0
quantities .  0.3333333333333333
HMMs are  0.25
recognition because  0.008264462809917356
because a  0.03333333333333333
a speech  0.00245398773006135
speech signal  0.006578947368421052
a piecewise  0.001226993865030675
piecewise  1.0
piecewise stationary  1.0
stationary signal  0.2857142857142857
signal or  0.16666666666666666
a short-time  0.001226993865030675
short-time  1.0
short-time stationary  0.5
signal .  0.16666666666666666
short time-scales  0.125
time-scales  1.0
time-scales -LRB-  1.0
10 milliseconds  0.25
milliseconds  1.0
milliseconds -RRB-  0.5
be approximated  0.004219409282700422
approximated  1.0
approximated as  1.0
stationary process  0.14285714285714285
Speech can  0.03225806451612903
be thought  0.004219409282700422
a Markov  0.001226993865030675
model for  0.06666666666666667
many stochastic  0.019230769230769232
stochastic purposes  0.125
Another reason  0.07692307692307693
reason why  0.25
why HMMs  0.14285714285714285
are popular  0.004149377593360996
popular is  0.1111111111111111
is because  0.0020325203252032522
be trained  0.004219409282700422
trained automatically  0.3333333333333333
automatically and  0.09523809523809523
are simple  0.004149377593360996
and computationally  0.001445086705202312
computationally feasible  0.5
feasible to  0.5
In speech  0.009523809523809525
the hidden  0.0006920415224913495
would output  0.018867924528301886
of n-dimensional  0.00089126559714795
n-dimensional  1.0
n-dimensional real-valued  1.0
real-valued vectors  0.3333333333333333
vectors -LRB-  0.3333333333333333
with n  0.00546448087431694
n  0.5
n being  0.5
small integer  0.1111111111111111
integer  1.0
integer ,  1.0
as 10  0.003484320557491289
10 -RRB-  0.125
, outputting  0.0005614823133071309
outputting one  0.5
these every  0.023809523809523808
every 10  0.3333333333333333
milliseconds .  0.5
The vectors  0.005208333333333333
vectors would  0.3333333333333333
would consist  0.018867924528301886
consist  1.0
consist of  1.0
of cepstral  0.00089126559714795
cepstral  1.0
cepstral coefficients  0.5
coefficients  1.0
coefficients ,  0.25
by taking  0.005714285714285714
taking a  0.2
a Fourier  0.001226993865030675
transform of  0.2
short time  0.125
time window  0.030303030303030304
and decorrelating  0.001445086705202312
decorrelating  1.0
decorrelating the  1.0
the spectrum  0.0006920415224913495
spectrum  1.0
spectrum using  1.0
a cosine  0.001226993865030675
cosine transform  0.3333333333333333
transform ,  0.4
then taking  0.02857142857142857
first -LRB-  0.030303030303030304
most significant  0.017241379310344827
significant -RRB-  0.1111111111111111
-RRB- coefficients  0.0028169014084507044
coefficients .  0.25
The hidden  0.005208333333333333
model will  0.03333333333333333
will tend  0.02857142857142857
in each  0.0018726591760299626
each state  0.022222222222222223
state a  0.07142857142857142
statistical distribution  0.030303030303030304
a mixture  0.001226993865030675
mixture  1.0
mixture of  1.0
of diagonal  0.00089126559714795
diagonal  1.0
diagonal covariance  1.0
covariance  1.0
covariance Gaussians  0.5
Gaussians  1.0
Gaussians ,  1.0
will give  0.02857142857142857
give a  0.25
a likelihood  0.001226993865030675
likelihood  1.0
likelihood for  0.3333333333333333
each observed  0.022222222222222223
observed  1.0
observed vector  1.0
vector .  0.3333333333333333
Each word  0.16666666666666666
or -LRB-  0.0045045045045045045
for more  0.007220216606498195
general speech  0.045454545454545456
systems -RRB-  0.008928571428571428
each phoneme  0.022222222222222223
phoneme  1.0
phoneme ,  0.5
different output  0.02040816326530612
output distribution  0.038461538461538464
distribution ;  0.25
; a  0.02127659574468085
a hidden  0.001226993865030675
or phonemes  0.009009009009009009
phonemes  1.0
phonemes is  0.16666666666666666
made by  0.0625
by concatenating  0.005714285714285714
concatenating  1.0
concatenating the  1.0
individual trained  0.08333333333333333
trained hidden  0.3333333333333333
the separate  0.0006920415224913495
and phonemes  0.001445086705202312
phonemes .  0.16666666666666666
Described above  1.0
above are  0.07692307692307693
the core  0.0006920415224913495
core elements  0.5
elements of  0.25
, HMM-based  0.0005614823133071309
HMM-based approach  0.6666666666666666
Modern speech  0.3333333333333333
use various  0.013888888888888888
various combinations  0.05555555555555555
combinations  1.0
combinations of  1.0
of standard  0.00089126559714795
standard techniques  0.07142857142857142
techniques in  0.043478260869565216
improve results  0.07692307692307693
results over  0.047619047619047616
basic approach  0.07692307692307693
approach described  0.02857142857142857
above .  0.07692307692307693
typical large-vocabulary  0.1111111111111111
large-vocabulary system  0.3333333333333333
would need  0.018867924528301886
need context  0.047619047619047616
context dependency  0.030303030303030304
dependency for  0.2
the phonemes  0.001384083044982699
phonemes -LRB-  0.16666666666666666
so phonemes  0.03333333333333333
phonemes with  0.16666666666666666
with different  0.01092896174863388
different left  0.02040816326530612
right context  0.1
context have  0.030303030303030304
have different  0.019230769230769232
different realizations  0.02040816326530612
realizations  1.0
realizations as  1.0
as HMM  0.003484320557491289
HMM  1.0
HMM states  0.3333333333333333
states -RRB-  0.25
; it  0.02127659574468085
would use  0.018867924528301886
use cepstral  0.013888888888888888
cepstral normalization  0.5
normalization to  0.16666666666666666
to normalize  0.0013280212483399733
normalize  1.0
normalize for  1.0
for different  0.0036101083032490976
different speaker  0.02040816326530612
speaker and  0.05555555555555555
and recording  0.001445086705202312
recording  1.0
recording conditions  1.0
conditions ;  0.2
further speaker  0.125
speaker normalization  0.05555555555555555
normalization it  0.16666666666666666
it might  0.008547008547008548
might use  0.07692307692307693
use vocal  0.013888888888888888
vocal  1.0
vocal tract  1.0
tract  1.0
tract length  1.0
length normalization  0.125
normalization -LRB-  0.16666666666666666
-LRB- VTLN  0.0027100271002710027
VTLN  1.0
VTLN -RRB-  1.0
-RRB- for  0.005633802816901409
for male-female  0.0036101083032490976
male-female  1.0
male-female normalization  1.0
normalization and  0.16666666666666666
and maximum  0.001445086705202312
maximum likelihood  0.3333333333333333
likelihood linear  0.6666666666666666
linear regression  0.14285714285714285
regression  1.0
regression -LRB-  1.0
-LRB- MLLR  0.0027100271002710027
MLLR  1.0
MLLR -RRB-  1.0
general speaker  0.045454545454545456
speaker adaptation  0.1111111111111111
adaptation .  0.6666666666666666
The features  0.005208333333333333
features would  0.038461538461538464
have so-called  0.009615384615384616
so-called delta  0.3333333333333333
delta  1.0
delta and  1.0
and delta-delta  0.002890173410404624
delta-delta  1.0
delta-delta coefficients  1.0
coefficients to  0.25
to capture  0.0013280212483399733
capture speech  0.5
speech dynamics  0.006578947368421052
dynamics and  0.5
addition might  0.16666666666666666
use heteroscedastic  0.013888888888888888
heteroscedastic  1.0
heteroscedastic linear  1.0
linear discriminant  0.2857142857142857
discriminant  1.0
discriminant analysis  1.0
-LRB- HLDA  0.0027100271002710027
HLDA  1.0
HLDA -RRB-  1.0
; or  0.02127659574468085
or might  0.0045045045045045045
might skip  0.038461538461538464
skip  1.0
skip the  1.0
the delta  0.0006920415224913495
coefficients and  0.25
use splicing  0.013888888888888888
splicing  1.0
splicing and  1.0
an LDA-based  0.007575757575757576
LDA-based  1.0
LDA-based projection  1.0
projection  1.0
projection followed  1.0
followed perhaps  0.25
perhaps by  0.16666666666666666
by heteroscedastic  0.005714285714285714
a global  0.001226993865030675
global semitied  0.3333333333333333
semitied  1.0
semitied covariance  1.0
covariance transform  0.5
transform -LRB-  0.2
as maximum  0.003484320557491289
linear transform  0.14285714285714285
or MLLT  0.0045045045045045045
MLLT  1.0
MLLT -RRB-  1.0
Many systems  0.08333333333333333
use so-called  0.013888888888888888
so-called discriminative  0.3333333333333333
discriminative  1.0
discriminative training  1.0
training techniques  0.03571428571428571
that dispense  0.0035460992907801418
dispense  1.0
dispense with  1.0
a purely  0.001226993865030675
purely  1.0
purely statistical  1.0
statistical approach  0.030303030303030304
to HMM  0.0013280212483399733
HMM parameter  0.3333333333333333
parameter  1.0
parameter estimation  1.0
estimation  1.0
estimation and  1.0
and instead  0.001445086705202312
instead optimize  0.14285714285714285
optimize  1.0
optimize some  1.0
some classification-related  0.012048192771084338
classification-related  1.0
classification-related measure  1.0
are maximum  0.004149377593360996
maximum mutual  0.16666666666666666
mutual  1.0
mutual information  1.0
information -LRB-  0.021739130434782608
-LRB- MMI  0.0027100271002710027
MMI  1.0
MMI -RRB-  1.0
, minimum  0.0005614823133071309
minimum  1.0
minimum classification  0.5
classification error  0.058823529411764705
error -LRB-  0.16666666666666666
-LRB- MCE  0.0027100271002710027
MCE  1.0
MCE -RRB-  1.0
and minimum  0.001445086705202312
minimum phone  0.5
phone error  0.25
-LRB- MPE  0.0027100271002710027
MPE  1.0
MPE -RRB-  1.0
Decoding of  0.5
what happens  0.03125
happens  1.0
happens when  1.0
presented with  0.16666666666666666
new utterance  0.041666666666666664
utterance and  0.3333333333333333
and must  0.001445086705202312
must compute  0.07142857142857142
compute the  0.5
likely source  0.0625
source sentence  0.08333333333333333
would probably  0.018867924528301886
probably use  0.25
use the  0.013888888888888888
best path  0.05555555555555555
path ,  0.5
and here  0.001445086705202312
here there  0.5
a choice  0.001226993865030675
choice between  0.125
between dynamically  0.02564102564102564
dynamically creating  0.5
combination hidden  0.2
includes both  0.14285714285714285
the acoustic  0.0006920415224913495
acoustic and  0.16666666666666666
language model  0.006756756756756757
model information  0.03333333333333333
and combining  0.001445086705202312
combining it  0.25
it statically  0.008547008547008548
statically  1.0
statically beforehand  1.0
beforehand  1.0
beforehand -LRB-  1.0
the finite  0.0006920415224913495
state transducer  0.14285714285714285
transducer  1.0
transducer ,  0.5
or FST  0.0045045045045045045
FST  1.0
FST ,  1.0
, approach  0.0005614823133071309
approach -RRB-  0.05714285714285714
A possible  0.02
possible improvement  0.041666666666666664
improvement to  0.25
to decoding  0.0013280212483399733
decoding  1.0
decoding is  1.0
keep a  0.3333333333333333
of good  0.00089126559714795
candidates instead  0.2
of just  0.00089126559714795
just keeping  0.1111111111111111
keeping the  0.5
best candidate  0.05555555555555555
candidate ,  0.3333333333333333
better scoring  0.1111111111111111
scoring function  0.5
function -LRB-  0.125
-LRB- rescoring  0.0027100271002710027
rescoring  1.0
rescoring -RRB-  1.0
rate these  0.09090909090909091
these good  0.023809523809523808
candidates so  0.2
we may  0.022222222222222223
may pick  0.019230769230769232
pick  1.0
pick the  1.0
best one  0.05555555555555555
one according  0.015384615384615385
this refined  0.01098901098901099
refined  1.0
refined score  1.0
The set  0.005208333333333333
of candidates  0.00089126559714795
candidates can  0.2
be kept  0.004219409282700422
kept  1.0
kept either  1.0
list -LRB-  0.09090909090909091
the N-best  0.0006920415224913495
N-best  1.0
N-best list  1.0
list approach  0.09090909090909091
the models  0.0006920415224913495
a lattice  0.001226993865030675
lattice  1.0
lattice -RRB-  1.0
Rescoring is  1.0
by trying  0.005714285714285714
to minimize  0.0013280212483399733
minimize  1.0
minimize the  1.0
the Bayes  0.0006920415224913495
Bayes risk  0.3333333333333333
risk -LRB-  0.5
approximation thereof  0.16666666666666666
thereof  1.0
thereof -RRB-  1.0
: Instead  0.00980392156862745
Instead  0.3333333333333333
of taking  0.00089126559714795
sentence with  0.020833333333333332
with maximal  0.00546448087431694
maximal  1.0
maximal probability  1.0
probability ,  0.14285714285714285
we try  0.022222222222222223
take the  0.2
sentence that  0.041666666666666664
that minimizes  0.0070921985815602835
minimizes  1.0
minimizes the  1.0
the expectancy  0.0006920415224913495
expectancy  1.0
expectancy of  1.0
given loss  0.041666666666666664
loss  1.0
loss function  1.0
function with  0.125
with regards  0.00546448087431694
regards  1.0
regards to  1.0
possible transcriptions  0.08333333333333333
transcriptions  1.0
transcriptions -LRB-  0.5
we take  0.022222222222222223
the average  0.0006920415224913495
average distance  0.5
distance to  0.3333333333333333
to other  0.0013280212483399733
other possible  0.014285714285714285
possible sentences  0.041666666666666664
sentences weighted  0.013157894736842105
weighted by  0.3333333333333333
by their  0.005714285714285714
their estimated  0.029411764705882353
estimated  1.0
estimated probability  1.0
The loss  0.005208333333333333
function is  0.125
usually the  0.03125
the Levenshtein  0.0006920415224913495
Levenshtein  1.0
Levenshtein distance  1.0
be different  0.004219409282700422
different distances  0.02040816326530612
distances  1.0
distances for  0.5
for specific  0.0036101083032490976
specific tasks  0.047619047619047616
tasks ;  0.03125
transcriptions is  0.5
course ,  0.3333333333333333
, pruned  0.0005614823133071309
pruned  1.0
pruned to  1.0
to maintain  0.0013280212483399733
maintain  1.0
maintain tractability  1.0
tractability  1.0
tractability .  1.0
Efficient algorithms  1.0
been devised  0.014705882352941176
devised to  0.5
to rescore  0.0013280212483399733
rescore  1.0
rescore lattices  1.0
lattices  1.0
lattices represented  1.0
as weighted  0.003484320557491289
weighted finite  0.3333333333333333
state transducers  0.07142857142857142
transducers  1.0
transducers with  1.0
with edit  0.00546448087431694
edit  1.0
edit distances  1.0
distances represented  0.5
represented themselves  0.16666666666666666
themselves as  0.25
a finite  0.00245398773006135
transducer verifying  0.5
verifying  1.0
verifying certain  1.0
certain assumptions  0.14285714285714285
assumptions .  0.2
Dynamic time  0.8
time warping  0.12121212121212122
warping  1.0
warping -LRB-  0.25
-LRB- DTW  0.0027100271002710027
DTW  0.6666666666666666
DTW -RRB-  0.3333333333333333
-RRB- -  0.0028169014084507044
based speech  0.018518518518518517
recognition Main  0.008264462809917356
: Dynamic  0.00980392156862745
Dynamic  0.4
warping Dynamic  0.25
warping is  0.5
was historically  0.012987012987012988
historically  1.0
historically used  0.5
recognition but  0.008264462809917356
but has  0.014705882352941176
has now  0.011904761904761904
now largely  0.07692307692307693
largely been  0.2
been displaced  0.014705882352941176
displaced  1.0
displaced by  1.0
successful HMM-based  0.1111111111111111
for measuring  0.0036101083032490976
measuring  1.0
measuring similarity  1.0
between two  0.05128205128205128
two sequences  0.034482758620689655
sequences that  0.1111111111111111
may vary  0.019230769230769232
in time  0.0018726591760299626
time or  0.030303030303030304
or speed  0.0045045045045045045
, similarities  0.0005614823133071309
similarities in  0.5
in walking  0.0018726591760299626
walking  1.0
walking patterns  0.3333333333333333
patterns would  0.2
be detected  0.004219409282700422
detected ,  0.5
if in  0.07142857142857142
one video  0.015384615384615385
video the  0.2
person was  0.05263157894736842
was walking  0.012987012987012988
walking slowly  0.3333333333333333
slowly  1.0
slowly and  0.5
and if  0.001445086705202312
another he  0.07692307692307693
he or  0.14285714285714285
or she  0.0045045045045045045
she  1.0
she were  1.0
were walking  0.024390243902439025
walking more  0.3333333333333333
more quickly  0.010526315789473684
quickly  1.0
quickly ,  1.0
were accelerations  0.024390243902439025
accelerations  1.0
accelerations and  1.0
and decelerations  0.001445086705202312
decelerations  1.0
decelerations during  1.0
the course  0.0006920415224913495
course of  0.3333333333333333
one observation  0.015384615384615385
observation  1.0
observation .  1.0
DTW has  0.3333333333333333
to video  0.0013280212483399733
video ,  0.2
and graphics  0.001445086705202312
graphics  1.0
graphics --  1.0
-- indeed  0.04
indeed ,  0.3333333333333333
, any  0.0005614823133071309
any data  0.03225806451612903
be turned  0.004219409282700422
linear representation  0.14285714285714285
representation can  0.05263157894736842
be analyzed  0.004219409282700422
analyzed with  0.2
with DTW  0.00546448087431694
DTW .  0.3333333333333333
A well-known  0.02
well-known  1.0
well-known application  1.0
application has  0.07142857142857142
been automatic  0.014705882352941176
to cope  0.0013280212483399733
cope  1.0
cope with  1.0
different speaking  0.02040816326530612
speaking speeds  0.125
speeds .  0.5
method that  0.0625
that allows  0.0035460992907801418
allows a  0.125
an optimal  0.007575757575757576
optimal  1.0
optimal match  1.0
match between  0.16666666666666666
two given  0.034482758620689655
given sequences  0.041666666666666664
sequences -LRB-  0.1111111111111111
, time  0.0005614823133071309
time series  0.030303030303030304
series -RRB-  0.125
with certain  0.00546448087431694
certain restrictions  0.14285714285714285
restrictions  1.0
restrictions .  1.0
the sequences  0.0006920415224913495
sequences are  0.1111111111111111
are ``  0.004149377593360996
`` warped  0.005291005291005291
warped  1.0
warped ''  1.0
'' non-linearly  0.005376344086021506
non-linearly  1.0
non-linearly to  1.0
match each  0.16666666666666666
This sequence  0.015873015873015872
sequence alignment  0.125
alignment  1.0
alignment method  0.5
method is  0.0625
often used  0.022727272727272728
of hidden  0.00089126559714795
models ...  0.038461538461538464
...  0.5
... .  0.5
Neural networks  0.75
networks Main  0.07142857142857142
: Neural  0.00980392156862745
Neural  0.5
networks Neural  0.07142857142857142
networks emerged  0.07142857142857142
emerged  1.0
emerged as  1.0
an attractive  0.007575757575757576
attractive acoustic  0.3333333333333333
modeling approach  0.14285714285714285
approach in  0.02857142857142857
in ASR  0.003745318352059925
1980s .  0.2222222222222222
Since then  0.2
, neural  0.0022459292532285235
networks have  0.07142857142857142
many aspects  0.019230769230769232
recognition such  0.008264462809917356
as phoneme  0.003484320557491289
phoneme classification  0.5
classification ,  0.058823529411764705
, isolated  0.0005614823133071309
isolated word  0.2
word recognition  0.016666666666666666
and speaker  0.001445086705202312
to HMMs  0.0013280212483399733
HMMs ,  0.125
networks make  0.07142857142857142
make no  0.05
no assumptions  0.07692307692307693
assumptions about  0.2
about feature  0.025
feature statistical  0.07692307692307693
statistical properties  0.030303030303030304
properties and  0.25
and have  0.001445086705202312
have several  0.009615384615384616
several qualities  0.045454545454545456
qualities making  0.5
making them  0.14285714285714285
them attractive  0.05263157894736842
attractive recognition  0.3333333333333333
recognition models  0.008264462809917356
When used  0.14285714285714285
speech feature  0.006578947368421052
feature segment  0.07692307692307693
segment ,  0.2222222222222222
networks allow  0.07142857142857142
allow discriminative  0.2
training in  0.03571428571428571
efficient manner  0.3333333333333333
Few assumptions  1.0
assumptions on  0.2
the statistics  0.0006920415224913495
statistics of  0.125
input features  0.024390243902439025
made with  0.0625
with neural  0.00546448087431694
in spite  0.0018726591760299626
spite  1.0
spite of  1.0
of their  0.0017825311942959
their effectiveness  0.029411764705882353
effectiveness in  0.3333333333333333
in classifying  0.0018726591760299626
classifying short-time  0.2
short-time units  0.5
units such  0.14285714285714285
as individual  0.003484320557491289
individual phones  0.08333333333333333
phones  1.0
phones and  0.5
and isolated  0.001445086705202312
isolated words  0.2
networks are  0.07142857142857142
are rarely  0.004149377593360996
rarely successful  0.3333333333333333
successful for  0.1111111111111111
for continuous  0.0036101083032490976
continuous recognition  0.16666666666666666
recognition tasks  0.01652892561983471
, largely  0.0005614823133071309
largely because  0.2
their lack  0.029411764705882353
lack  1.0
lack of  1.0
of ability  0.00089126559714795
to model  0.0013280212483399733
model temporal  0.03333333333333333
temporal dependencies  0.5
one alternative  0.015384615384615385
alternative approach  0.3333333333333333
use neural  0.013888888888888888
networks as  0.07142857142857142
a pre-processing  0.001226993865030675
pre-processing  1.0
pre-processing e.g.  1.0
e.g. feature  0.017857142857142856
feature transformation  0.07692307692307693
transformation  1.0
transformation ,  1.0
, dimensionality  0.0005614823133071309
dimensionality  1.0
dimensionality reduction  1.0
reduction ,  0.5
the HMM  0.0006920415224913495
HMM based  0.3333333333333333
based recognition  0.018518518518518517
Further information  0.3333333333333333
information Popular  0.021739130434782608
Popular  1.0
Popular speech  1.0
recognition conferences  0.008264462809917356
conferences  1.0
conferences held  1.0
held  1.0
held each  1.0
each year  0.022222222222222223
year or  0.16666666666666666
two include  0.034482758620689655
include SpeechTEK  0.037037037037037035
SpeechTEK  1.0
SpeechTEK and  0.5
and SpeechTEK  0.001445086705202312
SpeechTEK Europe  0.5
, ICASSP  0.0005614823133071309
ICASSP  1.0
ICASSP ,  1.0
, Eurospeech\/ICSLP  0.0005614823133071309
Eurospeech\/ICSLP  1.0
Eurospeech\/ICSLP -LRB-  1.0
now named  0.15384615384615385
named Interspeech  0.14285714285714285
Interspeech  1.0
Interspeech -RRB-  1.0
the IEEE  0.001384083044982699
IEEE  1.0
IEEE ASRU  0.3333333333333333
ASRU  1.0
ASRU .  1.0
Conferences in  0.5
of Natural  0.00089126559714795
as ACL  0.003484320557491289
ACL ,  0.5
, NAACL  0.0005614823133071309
NAACL  1.0
NAACL ,  1.0
, EMNLP  0.0005614823133071309
EMNLP  1.0
EMNLP ,  1.0
and HLT  0.001445086705202312
HLT  1.0
HLT ,  1.0
are beginning  0.004149377593360996
beginning to  0.5
include papers  0.037037037037037035
on speech  0.0047169811320754715
speech processing  0.006578947368421052
Important journals  1.0
journals include  0.5
IEEE Transactions  0.6666666666666666
Transactions  1.0
Transactions on  1.0
on Speech  0.0047169811320754715
and Audio  0.001445086705202312
Audio  1.0
Audio Processing  0.5
Processing  1.0
Processing -LRB-  0.75
named IEEE  0.14285714285714285
on Audio  0.0047169811320754715
Audio ,  0.5
Language Processing  0.25
Processing -RRB-  0.25
, Computer  0.0005614823133071309
Computer Speech  0.3333333333333333
Language ,  0.08333333333333333
and Speech  0.001445086705202312
Speech Communication  0.03225806451612903
Communication  1.0
Communication .  1.0
Books like  1.0
`` Fundamentals  0.010582010582010581
Fundamentals  1.0
Fundamentals of  1.0
of Speech  0.00089126559714795
Recognition ''  0.375
'' by  0.026881720430107527
by Lawrence  0.005714285714285714
Lawrence  1.0
Lawrence Rabiner  1.0
Rabiner  1.0
Rabiner can  1.0
to acquire  0.0013280212483399733
acquire  1.0
acquire basic  1.0
basic knowledge  0.07692307692307693
knowledge but  0.037037037037037035
fully up  0.16666666666666666
to date  0.0026560424966799467
-LRB- 1993  0.0027100271002710027
1993 -RRB-  0.3333333333333333
A very  0.02
very recent  0.024390243902439025
recent book  0.125
book -LRB-  0.25
-LRB- Dec.  0.0027100271002710027
Dec.  1.0
Dec. 2011  1.0
2011 -RRB-  0.5
of Speaker  0.00089126559714795
Speaker Recognition  0.16666666666666666
by Homayoon  0.005714285714285714
Homayoon  1.0
Homayoon Beigi  1.0
Beigi  1.0
Beigi covers  1.0
more recent  0.010526315789473684
recent developments  0.125
some detail  0.012048192771084338
detail .  0.5
the title  0.0006920415224913495
title  1.0
title concentrates  1.0
concentrates  1.0
concentrates on  1.0
on speaker  0.0047169811320754715
speaker recognition  0.05555555555555555
but a  0.014705882352941176
large portion  0.043478260869565216
book applies  0.125
applies directly  0.14285714285714285
lot of  0.3333333333333333
of valuable  0.00089126559714795
valuable detailed  0.5
detailed background  0.5
background material  0.3333333333333333
material .  0.5
Another good  0.07692307692307693
good source  0.07692307692307693
source can  0.041666666666666664
be ``  0.004219409282700422
`` Statistical  0.005291005291005291
Statistical Methods  0.1111111111111111
Methods  0.25
for Speech  0.0036101083032490976
by Frederick  0.005714285714285714
Frederick  1.0
Frederick Jelinek  1.0
Jelinek and  0.5
`` Spoken  0.005291005291005291
Spoken  1.0
Spoken Language  1.0
-LRB- 2001  0.0027100271002710027
2001 -RRB-  0.5
-RRB- ''  0.005633802816901409
by Xuedong  0.005714285714285714
Xuedong  1.0
Xuedong Huang  1.0
Huang  1.0
Huang etc.  1.0
More up  0.1111111111111111
date is  0.3333333333333333
`` Computer  0.005291005291005291
Speech ''  0.03225806451612903
by Manfred  0.005714285714285714
Manfred  1.0
Manfred R.  1.0
R. Schroeder  0.16666666666666666
Schroeder  1.0
Schroeder ,  1.0
, second  0.0005614823133071309
second edition  0.1
edition  1.0
edition published  1.0
published in  0.14285714285714285
in 2004  0.0018726591760299626
2004 .  0.3333333333333333
The recently  0.005208333333333333
recently updated  0.3333333333333333
updated  1.0
updated textbook  1.0
textbook of  0.5
`` Speech  0.005291005291005291
-LRB- 2008  0.0027100271002710027
2008  1.0
2008 -RRB-  1.0
by Jurafsky  0.005714285714285714
Jurafsky  1.0
Jurafsky and  1.0
and Martin  0.001445086705202312
Martin presents  0.5
presents  1.0
presents the  1.0
the basics  0.0006920415224913495
basics  1.0
basics and  1.0
art for  0.5
for ASR  0.0036101083032490976
ASR .  0.16666666666666666
A good  0.02
good insight  0.07692307692307693
insight  1.0
insight into  1.0
the techniques  0.0006920415224913495
techniques used  0.043478260869565216
best modern  0.05555555555555555
modern systems  0.2
be gained  0.004219409282700422
gained by  0.5
by paying  0.005714285714285714
paying  1.0
paying attention  1.0
attention to  0.5
to government  0.0013280212483399733
government sponsored  0.3333333333333333
sponsored evaluations  0.5
evaluations such  0.16666666666666666
those organised  0.045454545454545456
organised  1.0
organised by  1.0
by DARPA  0.005714285714285714
DARPA -LRB-  0.25
the largest  0.0006920415224913495
largest  1.0
largest speech  1.0
speech recognition-related  0.006578947368421052
recognition-related  1.0
recognition-related project  1.0
project ongoing  0.07692307692307693
ongoing as  0.5
2007 is  0.2
the GALE  0.001384083044982699
GALE  1.0
GALE project  1.0
which involves  0.007246376811594203
involves both  0.1
both speech  0.03225806451612903
translation components  0.013513513513513514
components -RRB-  0.2
In terms  0.009523809523809525
of freely  0.00089126559714795
freely  1.0
freely available  1.0
available resources  0.058823529411764705
resources ,  0.3333333333333333
, Carnegie  0.0005614823133071309
University 's  0.1111111111111111
's SPHINX  0.0196078431372549
SPHINX  1.0
SPHINX toolkit  1.0
toolkit  1.0
toolkit is  0.5
one place  0.015384615384615385
place to  0.25
to start  0.0026560424966799467
start to  0.14285714285714285
both learn  0.03225806451612903
learn about  0.07692307692307693
start experimenting  0.14285714285714285
experimenting  1.0
experimenting .  1.0
Another resource  0.07692307692307693
resource -LRB-  0.2
-LRB- free  0.0027100271002710027
free as  0.25
in free  0.003745318352059925
free beer  0.25
beer  1.0
beer ,  1.0
not as  0.008928571428571428
free speech  0.25
the HTK  0.0006920415224913495
HTK  1.0
HTK book  0.5
the accompanying  0.0006920415224913495
accompanying  1.0
accompanying HTK  1.0
HTK toolkit  0.5
toolkit -RRB-  0.5
The AT&T  0.005208333333333333
AT&T  1.0
AT&T libraries  1.0
libraries  1.0
libraries GRM  0.5
GRM  1.0
GRM library  1.0
library  1.0
library ,  0.5
and DCD  0.001445086705202312
DCD  1.0
DCD library  1.0
library are  0.5
also general  0.014492753623188406
general software  0.045454545454545456
software libraries  0.037037037037037035
libraries for  0.5
for large-vocabulary  0.0036101083032490976
more software  0.010526315789473684
software resources  0.037037037037037035
see List  0.05
software .  0.037037037037037035
A useful  0.02
useful review  0.07142857142857142
review of  0.3333333333333333
of robustness  0.00089126559714795
robustness  1.0
robustness in  0.25
ASR is  0.16666666666666666
is provided  0.0020325203252032522
by Junqua  0.005714285714285714
Junqua  1.0
Junqua and  1.0
and Haton  0.001445086705202312
Haton  1.0
Haton -LRB-  1.0
-LRB- 1995  0.0027100271002710027
1995  1.0
1995 -RRB-  1.0
People with  1.0
with disabilities  0.01092896174863388
disabilities  1.0
disabilities People  0.25
People  0.5
disabilities can  0.25
can benefit  0.011049723756906077
from speech  0.009615384615384616
recognition programs  0.008264462809917356
For individuals  0.01639344262295082
individuals  1.0
individuals that  1.0
are Deaf  0.004149377593360996
Deaf  1.0
Deaf or  1.0
or Hard  0.0045045045045045045
Hard of  0.5
of Hearing  0.00089126559714795
Hearing  1.0
Hearing ,  1.0
software is  0.07407407407407407
automatically generate  0.047619047619047616
a closed-captioning  0.001226993865030675
closed-captioning  1.0
closed-captioning of  1.0
of conversations  0.00089126559714795
conversations such  0.3333333333333333
as discussions  0.003484320557491289
discussions in  0.3333333333333333
in conference  0.0018726591760299626
conference rooms  0.5
rooms  1.0
rooms ,  1.0
, classroom  0.0005614823133071309
classroom  1.0
classroom lectures  1.0
lectures  1.0
lectures ,  1.0
, and\/or  0.0005614823133071309
and\/or religious  0.3333333333333333
religious  1.0
religious services  1.0
people who  0.125
who have  0.2
have difficulty  0.009615384615384616
difficulty using  0.14285714285714285
using their  0.01694915254237288
their hands  0.029411764705882353
hands  1.0
hands ,  1.0
from mild  0.009615384615384616
mild  1.0
mild repetitive  1.0
repetitive stress  0.5
stress injuries  0.5
injuries  1.0
injuries to  1.0
to involved  0.0013280212483399733
involved disabilities  0.16666666666666666
disabilities that  0.25
that preclude  0.0035460992907801418
preclude  1.0
preclude using  1.0
using conventional  0.01694915254237288
conventional  1.0
conventional computer  1.0
computer input  0.022727272727272728
, people  0.0005614823133071309
who used  0.1
used the  0.008849557522123894
the keyboard  0.0006920415224913495
keyboard a  0.3333333333333333
lot and  0.3333333333333333
developed RSI  0.038461538461538464
RSI  1.0
RSI became  1.0
became an  0.2
an urgent  0.007575757575757576
urgent  1.0
urgent early  1.0
early market  0.1
market for  0.3333333333333333
in deaf  0.0018726591760299626
deaf  1.0
deaf telephony  1.0
as voicemail  0.003484320557491289
voicemail  1.0
voicemail to  1.0
, relay  0.0005614823133071309
relay  1.0
relay services  1.0
services ,  0.3333333333333333
and captioned  0.001445086705202312
captioned  1.0
captioned telephone  1.0
telephone .  0.5
Individuals with  1.0
with learning  0.00546448087431694
learning disabilities  0.023255813953488372
disabilities who  0.25
have problems  0.009615384615384616
problems with  0.058823529411764705
with thought-to-paper  0.00546448087431694
thought-to-paper  1.0
thought-to-paper communication  1.0
-LRB- essentially  0.0027100271002710027
essentially they  0.125
they think  0.025
think of  0.3333333333333333
an idea  0.007575757575757576
idea but  0.14285714285714285
is processed  0.0020325203252032522
processed incorrectly  0.16666666666666666
incorrectly  1.0
incorrectly causing  1.0
causing  1.0
causing it  1.0
up differently  0.045454545454545456
differently  1.0
differently on  1.0
on paper  0.0047169811320754715
paper -RRB-  0.09090909090909091
software -LRB-  0.037037037037037035
-LRB- icon  0.0027100271002710027
icon  1.0
icon -RRB-  1.0
-RRB- This  0.0028169014084507044
requires expansion  0.0625
expansion .  0.3333333333333333
Current research  0.2
and funding  0.001445086705202312
funding Measuring  0.125
Measuring  1.0
Measuring progress  1.0
progress in  0.2857142857142857
recognition performance  0.03305785123966942
performance is  0.1111111111111111
difficult and  0.03571428571428571
and controversial  0.001445086705202312
controversial  1.0
controversial .  1.0
Some speech  0.047619047619047616
Word error  0.14285714285714285
rates on  0.125
some tasks  0.012048192771084338
are less  0.004149377593360996
than 1  0.022222222222222223
On others  0.16666666666666666
others they  0.08333333333333333
as high  0.003484320557491289
high as  0.05555555555555555
as 50  0.003484320557491289
Sometimes it  1.0
it even  0.008547008547008548
even appears  0.037037037037037035
appears that  0.2
that performance  0.010638297872340425
is going  0.0020325203252032522
going backward  0.25
backward  1.0
backward ,  1.0
as researchers  0.003484320557491289
researchers undertake  0.1
undertake  1.0
undertake harder  1.0
harder tasks  0.14285714285714285
tasks that  0.03125
have higher  0.009615384615384616
higher error  0.14285714285714285
rates .  0.125
Because progress  0.5
progress is  0.14285714285714285
is slow  0.0020325203252032522
slow  1.0
slow and  0.5
measure ,  0.09090909090909091
is some  0.0020325203252032522
some perception  0.012048192771084338
perception that  0.5
performance has  0.05555555555555555
has plateaued  0.011904761904761904
plateaued  1.0
plateaued and  1.0
that funding  0.0035460992907801418
funding has  0.125
has dried  0.011904761904761904
dried  1.0
dried up  1.0
up or  0.045454545454545456
or shifted  0.0045045045045045045
shifted  1.0
shifted priorities  1.0
priorities  1.0
priorities .  1.0
Such perceptions  0.125
perceptions  1.0
perceptions are  1.0
not new  0.008928571428571428
new .  0.041666666666666664
1969 ,  0.5
John Pierce  0.125
Pierce  1.0
Pierce wrote  1.0
open letter  0.25
letter that  0.16666666666666666
did cause  0.2
cause much  0.5
much funding  0.045454545454545456
funding to  0.125
to dry  0.0013280212483399733
dry  1.0
dry up  1.0
In 1993  0.009523809523809525
1993 there  0.3333333333333333
strong feeling  0.25
feeling  1.0
feeling that  1.0
performance had  0.05555555555555555
had plateaued  0.07142857142857142
were workshops  0.024390243902439025
workshops dedicated  0.5
1990s ,  0.3333333333333333
funding continued  0.125
continued more  0.1111111111111111
less uninterrupted  0.08333333333333333
uninterrupted  1.0
uninterrupted and  1.0
and performance  0.001445086705202312
performance continued  0.05555555555555555
continued ,  0.1111111111111111
, slowly  0.0005614823133071309
slowly but  0.5
but steadily  0.014705882352941176
steadily  1.0
steadily ,  1.0
improve .  0.07692307692307693
For the  0.01639344262295082
past thirty  0.3333333333333333
thirty  1.0
thirty years  1.0
recognition research  0.008264462809917356
been characterized  0.014705882352941176
steady accumulation  0.5
accumulation  1.0
accumulation of  1.0
of small  0.00089126559714795
small incremental  0.1111111111111111
incremental  1.0
incremental improvements  1.0
improvements .  0.5
a trend  0.001226993865030675
to change  0.0013280212483399733
change  1.0
change focus  1.0
focus to  0.14285714285714285
to more  0.0026560424966799467
difficult tasks  0.07142857142857142
tasks due  0.03125
to progress  0.0013280212483399733
the availability  0.0006920415224913495
availability  1.0
availability of  1.0
of faster  0.00089126559714795
faster computers  0.3333333333333333
this shifting  0.01098901098901099
shifting  1.0
shifting to  1.0
tasks has  0.03125
has characterized  0.011904761904761904
characterized DARPA  0.25
DARPA funding  0.25
funding of  0.125
recognition since  0.008264462809917356
since the  0.1
decade ,  0.3333333333333333
has continued  0.011904761904761904
continued with  0.1111111111111111
the EARS  0.0006920415224913495
EARS  1.0
EARS project  1.0
which undertook  0.007246376811594203
undertook  1.0
undertook recognition  1.0
of Mandarin  0.00089126559714795
Mandarin  1.0
Mandarin and  1.0
and Arabic  0.002890173410404624
Arabic in  0.25
to English  0.0013280212483399733
which focused  0.007246376811594203
focused solely  0.09090909090909091
solely  1.0
solely on  1.0
on Mandarin  0.0047169811320754715
Arabic and  0.25
and required  0.001445086705202312
required translation  0.14285714285714285
translation simultaneously  0.013513513513513514
simultaneously with  0.5
Commercial research  0.5
other academic  0.014285714285714285
academic  1.0
academic research  1.0
research also  0.023809523809523808
also continue  0.014492753623188406
continue  1.0
continue to  1.0
to focus  0.0013280212483399733
on increasingly  0.0047169811320754715
increasingly difficult  0.3333333333333333
One key  0.07692307692307693
key area  0.3333333333333333
improve robustness  0.07692307692307693
robustness of  0.25
not just  0.008928571428571428
just robustness  0.1111111111111111
robustness against  0.5
against noise  0.2
noise but  0.125
but robustness  0.014705882352941176
against any  0.2
any condition  0.03225806451612903
condition  1.0
condition that  1.0
that causes  0.0035460992907801418
causes  1.0
causes a  1.0
major degradation  0.08333333333333333
degradation  1.0
degradation in  1.0
in performance  0.0018726591760299626
Another key  0.07692307692307693
research is  0.047619047619047616
is focused  0.0020325203252032522
an opportunity  0.007575757575757576
opportunity rather  0.5
This research  0.015873015873015872
applications there  0.04
large quantity  0.043478260869565216
quantity of  0.3333333333333333
speech data  0.006578947368421052
, up  0.0005614823133071309
to millions  0.0013280212483399733
of hours  0.00089126559714795
hours .  0.5
is too  0.0020325203252032522
expensive to  0.14285714285714285
have humans  0.009615384615384616
humans transcribe  0.08333333333333333
transcribe  1.0
transcribe such  1.0
such large  0.008130081300813009
large quantities  0.08695652173913043
quantities of  0.6666666666666666
research focus  0.023809523809523808
focus is  0.14285714285714285
on developing  0.0047169811320754715
developing new  0.25
new methods  0.041666666666666664
learning that  0.023255813953488372
can effectively  0.0055248618784530384
effectively utilize  0.3333333333333333
utilize large  0.5
of unlabeled  0.00089126559714795
unlabeled  1.0
unlabeled data  1.0
Another area  0.07692307692307693
is better  0.0020325203252032522
better understanding  0.1111111111111111
human capabilities  0.021739130434782608
capabilities and  0.2
this understanding  0.01098901098901099
improve machine  0.07692307692307693
machine recognition  0.012658227848101266
of identifying  0.00089126559714795
boundaries between  0.18181818181818182
between words  0.05128205128205128
, syllables  0.0005614823133071309
syllables  1.0
syllables ,  0.5
phonemes in  0.16666666666666666
spoken natural  0.07142857142857142
term applies  0.1111111111111111
applies both  0.2857142857142857
the mental  0.0006920415224913495
mental processes  0.6666666666666666
processes used  0.4
by humans  0.011428571428571429
to artificial  0.0026560424966799467
artificial processes  0.18181818181818182
processes of  0.2
important subproblem  0.0625
subproblem  1.0
subproblem of  1.0
be adequately  0.004219409282700422
adequately  1.0
adequately solved  1.0
solved in  0.2
in isolation  0.0018726591760299626
isolation .  0.5
processing problems  0.018518518518518517
one must  0.015384615384615385
must take  0.07142857142857142
account context  0.3333333333333333
, grammar  0.0005614823133071309
even so  0.037037037037037035
often a  0.022727272727272728
a probabilistic  0.001226993865030675
probabilistic division  0.14285714285714285
division rather  0.5
a categorical  0.001226993865030675
categorical  1.0
categorical .  1.0
A comprehensive  0.02
comprehensive survey  0.2
survey  1.0
survey of  1.0
segmentation problems  0.06060606060606061
and techniques  0.001445086705202312
techniques can  0.043478260869565216
seen in  0.1
in .  0.0018726591760299626
Some writing  0.047619047619047616
writing systems  0.2222222222222222
systems indicate  0.008928571428571428
indicate speech  0.3333333333333333
segmentation between  0.030303030303030304
words by  0.009174311926605505
word divider  0.016666666666666666
divider  1.0
divider ,  1.0
the space  0.0020761245674740486
space .  0.2
is compounded  0.0020325203252032522
compounded  1.0
compounded by  1.0
of co-articulation  0.00089126559714795
co-articulation  1.0
co-articulation of  1.0
speech sounds  0.006578947368421052
where one  0.02857142857142857
one may  0.015384615384615385
be modified  0.004219409282700422
modified  1.0
modified in  1.0
ways by  0.125
the adjacent  0.0006920415224913495
adjacent sounds  0.16666666666666666
sounds :  0.06666666666666667
: it  0.00980392156862745
may blend  0.019230769230769232
blend smoothly  0.6666666666666666
smoothly  1.0
smoothly with  0.5
, fuse  0.0005614823133071309
fuse  1.0
fuse with  1.0
, split  0.0005614823133071309
split ,  0.25
even disappear  0.037037037037037035
disappear  1.0
disappear .  1.0
phenomenon may  0.2
may happen  0.019230769230769232
happen  1.0
happen between  1.0
between adjacent  0.05128205128205128
adjacent words  0.3333333333333333
words just  0.009174311926605505
easily as  0.1111111111111111
as within  0.003484320557491289
single word  0.07142857142857142
notion that  0.25
that speech  0.0035460992907801418
speech is  0.006578947368421052
produced like  0.1111111111111111
like writing  0.03571428571428571
of distinct  0.00089126559714795
distinct vowels  0.14285714285714285
vowels  1.0
vowels and  0.3333333333333333
and consonants  0.001445086705202312
consonants  1.0
consonants ,  0.3333333333333333
a relic  0.001226993865030675
relic  1.0
relic of  1.0
of our  0.00089126559714795
our alphabetic  0.2
alphabetic  1.0
alphabetic heritage  1.0
heritage  1.0
heritage -LRB-  1.0
way we  0.08333333333333333
we produce  0.044444444444444446
produce vowels  0.045454545454545456
vowels depends  0.3333333333333333
the surrounding  0.001384083044982699
surrounding consonants  0.2
consonants and  0.3333333333333333
produce consonants  0.045454545454545456
consonants depends  0.3333333333333333
surrounding vowels  0.2
vowels .  0.3333333333333333
when we  0.05714285714285714
we say  0.044444444444444446
say `  0.2857142857142857
` kit  0.125
kit  1.0
kit '  1.0
the -LRB-  0.0006920415224913495
-LRB- k  0.0027100271002710027
k  1.0
k -RRB-  1.0
is farther  0.0020325203252032522
farther  1.0
farther forward  1.0
forward  1.0
forward than  1.0
than when  0.022222222222222223
` caught  0.0625
caught  1.0
caught '  1.0
' .  0.10526315789473684
But also  0.16666666666666666
the vowel  0.001384083044982699
vowel  1.0
vowel in  1.0
in `  0.003745318352059925
` kick  0.0625
kick  1.0
kick '  1.0
' is  0.05263157894736842
is phonetically  0.0020325203252032522
phonetically  1.0
phonetically different  1.0
though we  0.1
we normally  0.022222222222222223
normally  1.0
normally do  0.5
not hear  0.008928571428571428
hear this  0.5
this .  0.01098901098901099
are language-specific  0.004149377593360996
language-specific  1.0
language-specific changes  1.0
changes  1.0
changes which  1.0
which occur  0.007246376811594203
occur on  0.2
on casual  0.0047169811320754715
casual  1.0
casual speech  1.0
speech which  0.006578947368421052
it quite  0.008547008547008548
from spelling  0.009615384615384616
spelling  1.0
spelling .  1.0
phrase `  0.1
` hit  0.0625
hit  1.0
hit you  1.0
you '  0.07692307692307693
' could  0.05263157894736842
could often  0.0625
often be  0.022727272727272728
more appropriately  0.010526315789473684
appropriately spelled  0.5
spelled  1.0
spelled `  1.0
` hitcha  0.0625
hitcha  1.0
hitcha '  1.0
even with  0.037037037037037035
best algorithms  0.05555555555555555
of phonetic  0.00089126559714795
phonetic  1.0
phonetic segmentation  0.5
segmentation will  0.030303030303030304
will usually  0.02857142857142857
usually be  0.03125
very distant  0.024390243902439025
distant  1.0
distant from  1.0
standard written  0.07142857142857142
written language  0.11538461538461539
the lexical  0.0006920415224913495
syntactic parsing  0.07692307692307693
spoken text  0.07142857142857142
text normally  0.006289308176100629
normally requires  0.5
requires specialized  0.0625
specialized algorithms  0.5
, distinct  0.0005614823133071309
for parsing  0.0036101083032490976
parsing written  0.03571428571428571
Statistical models  0.1111111111111111
models can  0.038461538461538464
to segment  0.00398406374501992
segment and  0.1111111111111111
and align  0.001445086705202312
align  1.0
align recorded  1.0
recorded speech  0.5
to words  0.0013280212483399733
or phones  0.0045045045045045045
phones .  0.5
Applications include  0.5
include automatic  0.037037037037037035
automatic lip-synch  0.043478260869565216
lip-synch  1.0
lip-synch timing  1.0
timing  1.0
timing for  1.0
for cartoon  0.0036101083032490976
cartoon  1.0
cartoon animation  1.0
animation  1.0
animation ,  1.0
, follow-the-bouncing-ball  0.0005614823133071309
follow-the-bouncing-ball  1.0
follow-the-bouncing-ball video  1.0
video sub-titling  0.2
sub-titling  1.0
sub-titling ,  1.0
and linguistic  0.001445086705202312
linguistic research  0.0625
Automatic segmentation  0.3333333333333333
and alignment  0.001445086705202312
alignment software  0.5
is commercially  0.0020325203252032522
Lexical segmentation  0.5
segmentation In  0.030303030303030304
In all  0.009523809523809525
all natural  0.023255813953488372
complex spoken  0.041666666666666664
spoken sentence  0.07142857142857142
sentence -LRB-  0.020833333333333332
which often  0.007246376811594203
been heard  0.014705882352941176
heard  1.0
heard or  1.0
or uttered  0.0045045045045045045
uttered before  0.3333333333333333
be understood  0.004219409282700422
understood  1.0
understood only  1.0
by decomposing  0.005714285714285714
decomposing  1.0
decomposing it  1.0
into smaller  0.01282051282051282
smaller lexical  0.14285714285714285
lexical segments  0.07692307692307693
segments -LRB-  0.2
-LRB- roughly  0.005420054200542005
roughly ,  0.6666666666666666
language -RRB-  0.013513513513513514
, associating  0.0005614823133071309
associating  1.0
associating a  1.0
each segment  0.044444444444444446
then combining  0.02857142857142857
combining those  0.25
those meanings  0.045454545454545456
meanings according  0.25
The recognition  0.005208333333333333
each lexical  0.022222222222222223
lexical segment  0.07692307692307693
segment in  0.1111111111111111
turn requires  0.16666666666666666
requires its  0.0625
its decomposition  0.02857142857142857
decomposition  1.0
decomposition into  1.0
of discrete  0.00089126559714795
discrete phonetic  0.3333333333333333
phonetic segments  0.5
segments and  0.2
and mapping  0.001445086705202312
mapping each  0.5
segment to  0.1111111111111111
one element  0.015384615384615385
element  1.0
element of  1.0
finite set  0.2
of elementary  0.00089126559714795
elementary  1.0
elementary sounds  1.0
phonemes of  0.16666666666666666
meaning then  0.043478260869565216
then can  0.02857142857142857
found by  0.07142857142857142
by standard  0.005714285714285714
standard table  0.07142857142857142
table lookup  0.14285714285714285
lookup  1.0
lookup algorithms  1.0
For most  0.01639344262295082
lexical units  0.07692307692307693
are surprisingly  0.004149377593360996
surprisingly difficult  0.3333333333333333
identify .  0.08333333333333333
One might  0.07692307692307693
might expect  0.038461538461538464
expect that  0.3333333333333333
the inter-word  0.0006920415224913495
inter-word  1.0
inter-word spaces  1.0
spaces used  0.2
by many  0.005714285714285714
many written  0.019230769230769232
English or  0.02702702702702703
or Spanish  0.0045045045045045045
Spanish ,  0.5
would correspond  0.018867924528301886
to pauses  0.0013280212483399733
pauses in  0.25
their spoken  0.029411764705882353
spoken version  0.07142857142857142
version ;  0.3333333333333333
is true  0.0020325203252032522
true only  0.5
very slow  0.024390243902439025
slow speech  0.5
speaker deliberately  0.05555555555555555
deliberately  1.0
deliberately inserts  1.0
inserts  1.0
inserts those  1.0
those pauses  0.045454545454545456
pauses .  0.25
In normal  0.009523809523809525
normal speech  0.5
one typically  0.015384615384615385
typically finds  0.05555555555555555
finds  1.0
finds many  1.0
many consecutive  0.019230769230769232
consecutive words  0.5
words being  0.009174311926605505
being said  0.05555555555555555
said  1.0
said with  1.0
no pauses  0.07692307692307693
final sounds  0.1111111111111111
sounds of  0.13333333333333333
word blend  0.016666666666666666
smoothly or  0.5
or fuse  0.0045045045045045045
the initial  0.0006920415224913495
initial sounds  0.3333333333333333
utterance can  0.3333333333333333
different meanings  0.02040816326530612
meanings depending  0.25
A popular  0.02
popular example  0.1111111111111111
often quoted  0.022727272727272728
quoted  1.0
quoted in  1.0
field ,  0.037037037037037035
phrase How  0.1
How to  0.2857142857142857
to wreck  0.0013280212483399733
wreck  1.0
wreck a  1.0
nice beach  0.25
beach  1.0
beach ,  1.0
which sounds  0.007246376811594203
sounds very  0.06666666666666667
to How  0.0013280212483399733
recognize speech  0.1111111111111111
As this  0.05555555555555555
this example  0.01098901098901099
example shows  0.012345679012345678
shows  1.0
shows ,  1.0
, proper  0.0005614823133071309
proper lexical  0.14285714285714285
lexical segmentation  0.07692307692307693
segmentation depends  0.030303030303030304
on context  0.0047169811320754715
which draws  0.007246376811594203
draws  1.0
draws on  1.0
the whole  0.0006920415224913495
whole of  0.1111111111111111
human knowledge  0.021739130434782608
and experience  0.001445086705202312
experience ,  0.5
and would  0.001445086705202312
would thus  0.018867924528301886
thus require  0.1
require advanced  0.045454545454545456
advanced pattern  0.2
and artificial  0.001445086705202312
intelligence technologies  0.125
technologies to  0.25
implemented on  0.2
problem overlaps  0.022727272727272728
overlaps to  0.5
some extent  0.012048192771084338
extent with  0.25
segmentation that  0.030303030303030304
that occurs  0.0035460992907801418
languages which  0.02
traditionally written  0.5
written without  0.038461538461538464
without inter-word  0.07692307692307693
spaces ,  0.2
Chinese and  0.14285714285714285
and Japanese  0.001445086705202312
Japanese .  0.125
even for  0.037037037037037035
often much  0.022727272727272728
much easier  0.045454545454545456
than speech  0.022222222222222223
segmentation ,  0.09090909090909091
the written  0.0006920415224913495
language usually  0.006756756756756757
usually has  0.03125
has little  0.011904761904761904
little interference  0.3333333333333333
interference  1.0
interference between  1.0
often contains  0.022727272727272728
contains additional  0.1
additional clues  0.16666666666666666
clues not  0.3333333333333333
of Chinese  0.00089126559714795
Chinese characters  0.14285714285714285
characters for  0.0625
for word  0.0036101083032490976
word stems  0.016666666666666666
stems in  0.5
in Japanese  0.0018726591760299626
Japanese -RRB-  0.125
Text segmentation  0.16666666666666666
of dividing  0.00267379679144385
dividing  1.0
dividing written  0.3333333333333333
meaningful units  0.125
units ,  0.14285714285714285
as words  0.003484320557491289
or topics  0.0045045045045045045
topics .  0.14285714285714285
to mental  0.0013280212483399733
humans when  0.08333333333333333
when reading  0.02857142857142857
reading text  0.125
processes implemented  0.2
implemented in  0.2
in computers  0.0018726591760299626
is non-trivial  0.0020325203252032522
non-trivial  1.0
non-trivial ,  0.5
because while  0.03333333333333333
while some  0.05
have explicit  0.009615384615384616
explicit word  0.2
word boundary  0.016666666666666666
boundary markers  0.16666666666666666
markers ,  0.3333333333333333
word spaces  0.016666666666666666
spaces of  0.2
written English  0.038461538461538464
English and  0.08108108108108109
the distinctive  0.0006920415224913495
distinctive initial  0.5
initial ,  0.3333333333333333
, medial  0.0005614823133071309
medial  1.0
medial and  1.0
and final  0.001445086705202312
final letter  0.1111111111111111
of Arabic  0.00089126559714795
Arabic ,  0.25
such signals  0.008130081300813009
are sometimes  0.004149377593360996
sometimes ambiguous  0.07692307692307693
all written  0.06976744186046512
Compare speech  1.0
dividing speech  0.3333333333333333
into linguistically  0.01282051282051282
linguistically  1.0
linguistically meaningful  1.0
meaningful portions  0.125
portions  1.0
portions .  1.0
In English  0.01904761904761905
and many  0.001445086705202312
languages using  0.02
the Latin  0.0006920415224913495
Latin alphabet  0.25
space is  0.2
good approximation  0.07692307692307693
approximation of  0.16666666666666666
word delimiter  0.016666666666666666
delimiter  1.0
delimiter .  1.0
-LRB- Some  0.0027100271002710027
Some examples  0.047619047619047616
examples where  0.041666666666666664
space character  0.2
character alone  0.045454545454545456
alone may  0.25
be sufficient  0.004219409282700422
sufficient include  0.2
include contractions  0.037037037037037035
contractions like  0.5
like ca  0.03571428571428571
ca  1.0
ca n't  1.0
n't for  0.25
for can  0.0036101083032490976
However the  0.02702702702702703
the equivalent  0.0006920415224913495
equivalent to  0.2
this character  0.01098901098901099
character is  0.09090909090909091
not found  0.008928571428571428
written scripts  0.038461538461538464
scripts ,  0.3333333333333333
without it  0.07692307692307693
it word  0.008547008547008548
word segmentation  0.05
a difficult  0.001226993865030675
difficult problem  0.03571428571428571
Languages which  0.3333333333333333
which do  0.007246376811594203
a trivial  0.001226993865030675
trivial word  0.25
segmentation process  0.030303030303030304
process include  0.027777777777777776
include Chinese  0.037037037037037035
Japanese ,  0.125
sentences but  0.02631578947368421
not words  0.026785714285714284
are delimited  0.012448132780082987
delimited  1.0
delimited ,  0.5
, Thai  0.0005614823133071309
Thai and  0.5
and Lao  0.001445086705202312
Lao  1.0
Lao ,  1.0
where phrases  0.02857142857142857
and Vietnamese  0.001445086705202312
Vietnamese  1.0
Vietnamese ,  1.0
where syllables  0.02857142857142857
syllables but  0.5
delimited .  0.25
some writing  0.012048192771084338
systems however  0.008928571428571428
the Ge'ez  0.0006920415224913495
Ge'ez  1.0
Ge'ez script  1.0
script used  0.25
for Amharic  0.0036101083032490976
Amharic  1.0
Amharic and  1.0
and Tigrinya  0.001445086705202312
Tigrinya  1.0
Tigrinya among  1.0
are explicitly  0.004149377593360996
explicitly delimited  0.25
delimited -LRB-  0.25
-LRB- at  0.0027100271002710027
least historically  0.2
historically -RRB-  0.5
a non-whitespace  0.001226993865030675
non-whitespace  1.0
non-whitespace character  1.0
character .  0.045454545454545456
The Unicode  0.005208333333333333
Unicode  1.0
Unicode Consortium  1.0
Consortium  1.0
Consortium has  1.0
has published  0.011904761904761904
a Standard  0.001226993865030675
Standard Annex  0.5
Annex  1.0
Annex on  1.0
on Text  0.0047169811320754715
Text Segmentation  0.16666666666666666
Segmentation  1.0
Segmentation ,  1.0
, exploring  0.0005614823133071309
exploring  1.0
exploring the  1.0
the issues  0.0006920415224913495
issues of  0.2
of segmentation  0.00089126559714795
segmentation in  0.030303030303030304
in multiscript  0.0018726591760299626
multiscript  1.0
multiscript texts  1.0
Word splitting  0.2857142857142857
splitting  1.0
splitting is  0.5
parsing concatenated  0.03571428571428571
concatenated  1.0
concatenated text  1.0
i.e. text  0.05263157894736842
contains no  0.1
no spaces  0.07692307692307693
spaces or  0.2
other word  0.014285714285714285
word separators  0.016666666666666666
separators  1.0
separators -RRB-  1.0
to infer  0.0013280212483399733
infer  1.0
infer where  1.0
where word  0.02857142857142857
word breaks  0.016666666666666666
breaks exist  0.5
exist  1.0
exist .  1.0
splitting may  0.5
may also  0.019230769230769232
also refer  0.014492753623188406
of hyphenation  0.00089126559714795
hyphenation  1.0
hyphenation .  1.0
Sentence segmentation  0.4
segmentation Sentence  0.030303030303030304
dividing a  0.3333333333333333
language into  0.006756756756756757
into its  0.01282051282051282
its component  0.02857142857142857
component sentences  0.2
using punctuation  0.01694915254237288
particularly the  0.2
full stop  0.4
stop  1.0
stop character  1.0
reasonable approximation  0.5
approximation .  0.16666666666666666
However even  0.02702702702702703
English this  0.02702702702702703
not trivial  0.008928571428571428
trivial due  0.25
character for  0.045454545454545456
for abbreviations  0.0036101083032490976
which may  0.007246376811594203
may or  0.019230769230769232
not also  0.008928571428571428
also terminate  0.014492753623188406
terminate  1.0
terminate a  1.0
example Mr.  0.012345679012345678
Mr.  1.0
Mr. is  0.5
not its  0.008928571428571428
own sentence  0.16666666666666666
`` Mr.  0.005291005291005291
Mr. Smith  0.5
Smith  1.0
Smith went  1.0
the shops  0.0006920415224913495
shops  1.0
shops in  1.0
in Jones  0.0018726591760299626
Jones  1.0
Jones Street  1.0
Street .  0.3333333333333333
When processing  0.14285714285714285
processing plain  0.018518518518518517
plain  1.0
plain text  1.0
, tables  0.0005614823133071309
tables of  0.3333333333333333
abbreviations that  0.2
contain periods  0.08333333333333333
periods can  0.3333333333333333
can help  0.0055248618784530384
help prevent  0.1111111111111111
prevent  1.0
prevent incorrect  1.0
incorrect assignment  0.3333333333333333
assignment of  0.5
of sentence  0.00089126559714795
As with  0.05555555555555555
languages contain  0.02
contain punctuation  0.08333333333333333
punctuation characters  0.14285714285714285
are useful  0.004149377593360996
for approximating  0.0036101083032490976
approximating  1.0
approximating sentence  1.0
Other segmentation  0.14285714285714285
problems Processes  0.058823529411764705
Processes  1.0
Processes may  1.0
be required  0.004219409282700422
required to  0.14285714285714285
segment text  0.2222222222222222
segments besides  0.2
besides  1.0
besides words  1.0
including morphemes  0.07142857142857142
morphemes -LRB-  0.3333333333333333
task usually  0.023809523809523808
usually called  0.03125
called morphological  0.05555555555555555
morphological analysis  0.3333333333333333
, paragraphs  0.0005614823133071309
paragraphs ,  0.25
, topics  0.0005614823133071309
topics or  0.14285714285714285
discourse turns  0.027777777777777776
turns .  0.3333333333333333
A document  0.02
contain multiple  0.08333333333333333
multiple topics  0.07692307692307693
topics ,  0.14285714285714285
computerized text  0.5
segmentation may  0.030303030303030304
to discover  0.0013280212483399733
discover  1.0
discover these  1.0
these topics  0.023809523809523808
topics automatically  0.14285714285714285
and segment  0.001445086705202312
segment the  0.1111111111111111
text accordingly  0.006289308176100629
accordingly  1.0
accordingly .  1.0
The topic  0.005208333333333333
topic boundaries  0.125
boundaries may  0.09090909090909091
be apparent  0.004219409282700422
apparent  1.0
apparent from  1.0
from section  0.009615384615384616
section titles  0.16666666666666666
titles and  0.5
and paragraphs  0.001445086705202312
paragraphs .  0.25
cases one  0.05555555555555555
one needs  0.015384615384615385
use techniques  0.013888888888888888
techniques similar  0.043478260869565216
different approaches  0.02040816326530612
tried .  0.3333333333333333
segmentation approaches  0.030303030303030304
approaches Automatic  0.03571428571428571
of implementing  0.00089126559714795
implementing  1.0
implementing a  1.0
computer process  0.022727272727272728
When punctuation  0.14285714285714285
and similar  0.001445086705202312
similar clues  0.037037037037037035
clues are  0.3333333333333333
consistently available  0.3333333333333333
the segmentation  0.0006920415224913495
segmentation task  0.030303030303030304
task often  0.023809523809523808
often requires  0.022727272727272728
requires fairly  0.0625
fairly non-trivial  0.25
non-trivial techniques  0.5
as statistical  0.003484320557491289
statistical decision-making  0.030303030303030304
decision-making  1.0
decision-making ,  1.0
, large  0.0005614823133071309
large dictionaries  0.043478260869565216
dictionaries  1.0
dictionaries ,  1.0
as consideration  0.003484320557491289
consideration of  0.3333333333333333
semantic constraints  0.047619047619047616
constraints .  0.25
Effective natural  1.0
systems and  0.008928571428571428
segmentation tools  0.06060606060606061
tools usually  0.16666666666666666
usually operate  0.03125
operate  1.0
operate on  1.0
on text  0.0047169811320754715
specific domains  0.047619047619047616
and sources  0.001445086705202312
, processing  0.0005614823133071309
processing text  0.018518518518518517
text used  0.006289308176100629
in medical  0.0018726591760299626
records is  0.25
different problem  0.02040816326530612
problem than  0.022727272727272728
than processing  0.022222222222222223
processing news  0.018518518518518517
or real  0.0045045045045045045
real estate  0.1111111111111111
estate  1.0
estate advertisements  1.0
of developing  0.00089126559714795
developing text  0.25
tools starts  0.16666666666666666
starts with  0.5
with collecting  0.00546448087431694
collecting  1.0
collecting a  1.0
large corpus  0.043478260869565216
application domain  0.07142857142857142
two general  0.034482758620689655
general approaches  0.045454545454545456
: Manual  0.00980392156862745
Manual  0.3333333333333333
Manual analysis  0.3333333333333333
and writing  0.001445086705202312
writing custom  0.1111111111111111
custom software  0.5
software Annotate  0.037037037037037035
Annotate  1.0
Annotate the  1.0
the sample  0.0006920415224913495
sample corpus  0.3333333333333333
corpus with  0.03225806451612903
with boundary  0.00546448087431694
boundary information  0.16666666666666666
information and  0.021739130434782608
use Machine  0.013888888888888888
Machine Learning  0.1111111111111111
Learning  1.0
Learning Some  1.0
Some text  0.047619047619047616
segmentation systems  0.030303030303030304
systems take  0.008928571428571428
any markup  0.03225806451612903
markup  1.0
markup like  1.0
like HTML  0.03571428571428571
HTML  1.0
HTML and  1.0
and know  0.001445086705202312
know document  0.5
document formats  0.027777777777777776
formats  1.0
formats like  1.0
like PDF  0.03571428571428571
PDF  1.0
PDF to  1.0
provide additional  0.16666666666666666
additional evidence  0.16666666666666666
evidence for  0.5
for sentence  0.0036101083032490976
sentence and  0.020833333333333332
and paragraph  0.001445086705202312
paragraph boundaries  0.3333333333333333
